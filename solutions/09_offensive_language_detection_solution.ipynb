{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f643d7",
   "metadata": {},
   "source": [
    "# Topic 9: Offensive Language Detection - SOLUTIONS\n",
    "\n",
    "Complete solutions for offensive language detection using traditional ML and transformer-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3080a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import ML libraries\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    SKLEARN_AVAILABLE = True\n",
    "    print(\"✓ Scikit-learn available for traditional ML approaches!\")\n",
    "except ImportError:\n",
    "    print(\"⚠ Scikit-learn not available. Please install: pip install scikit-learn\")\n",
    "    SKLEARN_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, Trainer, TrainingArguments\n",
    "    import torch\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "    print(\"✓ Transformers available for BERT-based approaches!\")\n",
    "    print(f\"✓ PyTorch available: {torch.__version__}\")\n",
    "    print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    print(\"⚠ Transformers/PyTorch not available. Please install: pip install transformers torch\")\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import joblib\n",
    "    JOBLIB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    JOBLIB_AVAILABLE = False\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99855301",
   "metadata": {},
   "source": [
    "## Solution 1: Data Preparation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be00e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_dataset():\n",
    "    \"\"\"Create a sample dataset for offensive language detection.\"\"\"\n",
    "    \n",
    "    # Sample data with various types of text (German focused)\n",
    "    sample_data = [\n",
    "        # Non-offensive texts\n",
    "        (\"Das ist ein schöner Tag heute.\", 0),\n",
    "        (\"Ich freue mich auf das Wochenende.\", 0),\n",
    "        (\"Guten Morgen, wie geht es Ihnen?\", 0),\n",
    "        (\"Das Wetter ist heute sehr schön.\", 0),\n",
    "        (\"Ich liebe es zu lesen.\", 0),\n",
    "        (\"Die Blumen im Garten sind wunderschön.\", 0),\n",
    "        (\"Herzlichen Glückwunsch zum Geburtstag!\", 0),\n",
    "        (\"Das war eine interessante Diskussion.\", 0),\n",
    "        (\"Vielen Dank für Ihre Hilfe.\", 0),\n",
    "        (\"Ich bin sehr dankbar.\", 0),\n",
    "        (\"Das Essen schmeckt köstlich.\", 0),\n",
    "        (\"Die Musik ist entspannend.\", 0),\n",
    "        (\"Ich mag dieses Buch sehr.\", 0),\n",
    "        (\"Der Film war fantastisch.\", 0),\n",
    "        (\"Schöne Grüße an alle.\", 0),\n",
    "        \n",
    "        # Mildly negative but not offensive\n",
    "        (\"Ich bin heute etwas müde.\", 0),\n",
    "        (\"Das Wetter ist nicht so gut.\", 0),\n",
    "        (\"Ich verstehe das nicht ganz.\", 0),\n",
    "        (\"Das war nicht meine beste Leistung.\", 0),\n",
    "        (\"Ich bin etwas enttäuscht.\", 0),\n",
    "        \n",
    "        # Offensive/inappropriate content (mild examples for educational purposes)\n",
    "        (\"Du bist so dumm!\", 1),\n",
    "        (\"Das ist totaler Schwachsinn!\", 1),\n",
    "        (\"Du Idiot!\", 1),\n",
    "        (\"Das ist bescheuert!\", 1),\n",
    "        (\"So ein Quatsch!\", 1),\n",
    "        (\"Du hast keine Ahnung!\", 1),\n",
    "        (\"Das ist völlig blöd!\", 1),\n",
    "        (\"Du bist ein Versager!\", 1),\n",
    "        (\"Das ist peinlich für dich!\", 1),\n",
    "        (\"Du redest nur Unsinn!\", 1),\n",
    "        \n",
    "        # Borderline cases\n",
    "        (\"Das ist wirklich ärgerlich.\", 0),\n",
    "        (\"Ich bin sauer auf dich.\", 0),\n",
    "        (\"Das nervt mich gewaltig.\", 0),\n",
    "        (\"Du bist manchmal schwierig.\", 0),\n",
    "        (\"Das war nicht nett von dir.\", 0),\n",
    "        \n",
    "        # More examples for balance\n",
    "        (\"Heute ist ein wunderbarer Tag zum Spazieren.\", 0),\n",
    "        (\"Ich freue mich auf den Urlaub.\", 0),\n",
    "        (\"Die Kinder spielen fröhlich im Park.\", 0),\n",
    "        (\"Das Konzert war atemberaubend.\", 0),\n",
    "        (\"Ich schätze deine Freundschaft sehr.\", 0),\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(sample_data, columns=['text', 'label'])\n",
    "    \n",
    "    # Add some English examples for comparison\n",
    "    english_examples = [\n",
    "        (\"This is a beautiful day.\", 0),\n",
    "        (\"I love spending time with family.\", 0),\n",
    "        (\"Thank you for your kindness.\", 0),\n",
    "        (\"You are so stupid!\", 1),\n",
    "        (\"This is complete nonsense!\", 1),\n",
    "        (\"You don't know anything!\", 1),\n",
    "    ]\n",
    "    \n",
    "    english_df = pd.DataFrame(english_examples, columns=['text', 'label'])\n",
    "    df = pd.concat([df, english_df], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_dataset(df):\n",
    "    \"\"\"Analyze the dataset for offensive language detection.\"\"\"\n",
    "    \n",
    "    print(\"Dataset Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Features: {list(df.columns)}\")\n",
    "    print()\n",
    "    \n",
    "    # Label distribution\n",
    "    label_counts = df['label'].value_counts()\n",
    "    print(\"Label Distribution:\")\n",
    "    print(f\"Non-offensive (0): {label_counts[0]} ({label_counts[0]/len(df)*100:.1f}%)\")\n",
    "    print(f\"Offensive (1): {label_counts[1]} ({label_counts[1]/len(df)*100:.1f}%)\")\n",
    "    print()\n",
    "    \n",
    "    # Text statistics\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['word_count'] = df['text'].str.split().str.len()\n",
    "    \n",
    "    print(\"Text Statistics:\")\n",
    "    print(f\"Average text length: {df['text_length'].mean():.1f} characters\")\n",
    "    print(f\"Average word count: {df['word_count'].mean():.1f} words\")\n",
    "    print()\n",
    "    \n",
    "    # Visualizations\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Label distribution\n",
    "    plt.subplot(2, 3, 1)\n",
    "    df['label'].value_counts().plot(kind='bar', color=['lightgreen', 'lightcoral'])\n",
    "    plt.title('Label Distribution')\n",
    "    plt.xlabel('Label (0=Non-offensive, 1=Offensive)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # Plot 2: Text length distribution\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.hist(df['text_length'], bins=15, alpha=0.7, color='skyblue')\n",
    "    plt.title('Text Length Distribution')\n",
    "    plt.xlabel('Characters')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Plot 3: Word count distribution\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.hist(df['word_count'], bins=10, alpha=0.7, color='lightgreen')\n",
    "    plt.title('Word Count Distribution')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Plot 4: Text length by label\n",
    "    plt.subplot(2, 3, 4)\n",
    "    for label in [0, 1]:\n",
    "        subset = df[df['label'] == label]\n",
    "        plt.hist(subset['text_length'], alpha=0.6, \n",
    "                label=f'Label {label}', bins=10)\n",
    "    plt.title('Text Length by Label')\n",
    "    plt.xlabel('Characters')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 5: Word count by label\n",
    "    plt.subplot(2, 3, 5)\n",
    "    sns.boxplot(data=df, x='label', y='word_count')\n",
    "    plt.title('Word Count by Label')\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Word Count')\n",
    "    \n",
    "    # Plot 6: Sample texts\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.text(0.1, 0.9, \"Sample Texts:\", fontsize=12, fontweight='bold', transform=plt.gca().transAxes)\n",
    "    \n",
    "    # Show examples\n",
    "    examples = []\n",
    "    for label in [0, 1]:\n",
    "        label_name = \"Non-offensive\" if label == 0 else \"Offensive\"\n",
    "        sample_texts = df[df['label'] == label]['text'].head(3).tolist()\n",
    "        examples.append(f\"{label_name}:\")\n",
    "        for i, text in enumerate(sample_texts, 1):\n",
    "            examples.append(f\"  {i}. {text[:40]}{'...' if len(text) > 40 else ''}\")\n",
    "        examples.append(\"\")\n",
    "    \n",
    "    example_text = \"\\\\n\".join(examples)\n",
    "    plt.text(0.1, 0.8, example_text, fontsize=8, transform=plt.gca().transAxes, \n",
    "             verticalalignment='top', fontfamily='monospace')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_or_create_dataset():\n",
    "    \"\"\"Load existing dataset or create sample dataset.\"\"\"\n",
    "    \n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "    DATA_DIR = PROJECT_ROOT / 'data'\n",
    "    DATA_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Try to load existing dataset\n",
    "    possible_files = [\n",
    "        DATA_DIR / 'germ_eval.csv',\n",
    "        DATA_DIR / 'offensive_language.csv',\n",
    "        DATA_DIR / 'hate_speech.csv'\n",
    "    ]\n",
    "    \n",
    "    for file_path in possible_files:\n",
    "        if file_path.exists():\n",
    "            print(f\"Loading existing dataset: {file_path}\")\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                print(f\"✓ Loaded {len(df)} samples from {file_path.name}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    # Create sample dataset if no existing data found\n",
    "    print(\"No existing dataset found. Creating sample dataset...\")\n",
    "    df = create_sample_dataset()\n",
    "    \n",
    "    # Save sample dataset\n",
    "    sample_path = DATA_DIR / 'sample_offensive_language.csv'\n",
    "    df.to_csv(sample_path, index=False, encoding='utf-8')\n",
    "    print(f\"✓ Sample dataset saved to: {sample_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load and analyze dataset\n",
    "print(\"Loading and Analyzing Dataset:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "df = load_or_create_dataset()\n",
    "if df is not None:\n",
    "    df_analyzed = analyze_dataset(df)\n",
    "    print(f\"✓ Dataset ready with {len(df_analyzed)} samples\")\n",
    "else:\n",
    "    print(\"❌ Could not load or create dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822409f9",
   "metadata": {},
   "source": [
    "## Solution 2: Traditional Machine Learning Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_features(texts):\n",
    "    \"\"\"Preprocess text for feature extraction.\"\"\"\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    processed_texts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\\\S+|www\\\\S+|https\\\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove user mentions and hashtags (for social media text)\n",
    "        text = re.sub(r'@\\\\w+|#\\\\w+', '', text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        text = re.sub(r'\\\\s+', ' ', text).strip()\n",
    "        \n",
    "        processed_texts.append(text)\n",
    "    \n",
    "    return processed_texts\n",
    "\n",
    "def create_traditional_ml_models():\n",
    "    \"\"\"Create various traditional ML models for comparison.\"\"\"\n",
    "    \n",
    "    if not SKLEARN_AVAILABLE:\n",
    "        print(\"Scikit-learn not available for traditional ML models.\")\n",
    "        return []\n",
    "    \n",
    "    models = [\n",
    "        {\n",
    "            'name': 'Logistic Regression',\n",
    "            'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'params': {\n",
    "                'C': [0.1, 1.0, 10.0],\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'solver': ['liblinear']\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Random Forest',\n",
    "            'model': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [5, 10, None],\n",
    "                'min_samples_split': [2, 5, 10]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Support Vector Machine',\n",
    "            'model': SVC(random_state=42, probability=True),\n",
    "            'params': {\n",
    "                'C': [0.1, 1.0, 10.0],\n",
    "                'kernel': ['linear', 'rbf'],\n",
    "                'gamma': ['scale', 'auto']\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Naive Bayes',\n",
    "            'model': MultinomialNB(),\n",
    "            'params': {\n",
    "                'alpha': [0.1, 1.0, 10.0]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return models\n",
    "\n",
    "def train_traditional_models(df):\n",
    "    \"\"\"Train traditional ML models with different feature extraction methods.\"\"\"\n",
    "    \n",
    "    if not SKLEARN_AVAILABLE or df is None:\n",
    "        print(\"Cannot train traditional models.\")\n",
    "        return {}\n",
    "    \n",
    "    print(\"Training Traditional ML Models:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Preprocess texts\n",
    "    processed_texts = preprocess_text_features(df['text'].tolist())\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        processed_texts, df['label'], test_size=0.3, random_state=42, stratify=df['label']\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(X_train)} samples\")\n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "    print()\n",
    "    \n",
    "    # Feature extraction methods\n",
    "    vectorizers = [\n",
    "        {\n",
    "            'name': 'TF-IDF (1-2 grams)',\n",
    "            'vectorizer': TfidfVectorizer(\n",
    "                ngram_range=(1, 2),\n",
    "                max_features=5000,\n",
    "                stop_words='english',  # Could be extended with German stop words\n",
    "                lowercase=True\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            'name': 'Count Vectorizer',\n",
    "            'vectorizer': CountVectorizer(\n",
    "                ngram_range=(1, 2),\n",
    "                max_features=5000,\n",
    "                stop_words='english',\n",
    "                lowercase=True\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            'name': 'TF-IDF (char-level)',\n",
    "            'vectorizer': TfidfVectorizer(\n",
    "                analyzer='char',\n",
    "                ngram_range=(2, 5),\n",
    "                max_features=5000,\n",
    "                lowercase=True\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Models\n",
    "    models = create_traditional_ml_models()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Train each combination\n",
    "    for vec_config in vectorizers:\n",
    "        vec_name = vec_config['name']\n",
    "        vectorizer = vec_config['vectorizer']\n",
    "        \n",
    "        print(f\"Feature Extraction: {vec_name}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Transform texts to features\n",
    "        X_train_vec = vectorizer.fit_transform(X_train)\n",
    "        X_test_vec = vectorizer.transform(X_test)\n",
    "        \n",
    "        print(f\"Feature matrix shape: {X_train_vec.shape}\")\n",
    "        \n",
    "        vec_results = {}\n",
    "        \n",
    "        for model_config in models:\n",
    "            model_name = model_config['name']\n",
    "            model = model_config['model']\n",
    "            \n",
    "            print(f\"  Training {model_name}...\", end=' ')\n",
    "            \n",
    "            try:\n",
    "                # Train model\n",
    "                model.fit(X_train_vec, y_train)\n",
    "                \n",
    "                # Predictions\n",
    "                y_pred = model.predict(X_test_vec)\n",
    "                y_pred_proba = model.predict_proba(X_test_vec)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "                \n",
    "                # Calculate metrics\n",
    "                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "                \n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, average='weighted')\n",
    "                recall = recall_score(y_test, y_pred, average='weighted')\n",
    "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "                \n",
    "                # ROC AUC if probabilities available\n",
    "                roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "                \n",
    "                vec_results[model_name] = {\n",
    "                    'model': model,\n",
    "                    'vectorizer': vectorizer,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'roc_auc': roc_auc,\n",
    "                    'predictions': y_pred,\n",
    "                    'probabilities': y_pred_proba\n",
    "                }\n",
    "                \n",
    "                print(f\"Accuracy: {accuracy:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        results[vec_name] = vec_results\n",
    "        print()\n",
    "    \n",
    "    return results, X_test, y_test\n",
    "\n",
    "def evaluate_traditional_models(results, X_test, y_test):\n",
    "    \"\"\"Evaluate and compare traditional ML models.\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results to evaluate.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Model Evaluation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Collect all results for comparison\n",
    "    all_results = []\n",
    "    \n",
    "    for vec_name, vec_results in results.items():\n",
    "        for model_name, model_result in vec_results.items():\n",
    "            all_results.append({\n",
    "                'Vectorizer': vec_name,\n",
    "                'Model': model_name,\n",
    "                'Accuracy': model_result['accuracy'],\n",
    "                'Precision': model_result['precision'],\n",
    "                'Recall': model_result['recall'],\n",
    "                'F1-Score': model_result['f1'],\n",
    "                'ROC-AUC': model_result['roc_auc'] or 'N/A'\n",
    "            })\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(\"Performance Comparison:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    # Find best model\n",
    "    best_f1 = results_df.loc[results_df['F1-Score'].idxmax()]\n",
    "    print(f\"Best Model (by F1-Score): {best_f1['Model']} with {best_f1['Vectorizer']}\")\n",
    "    print(f\"F1-Score: {best_f1['F1-Score']:.3f}\")\n",
    "    print()\n",
    "    \n",
    "    # Visualizations\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Performance comparison\n",
    "    plt.subplot(2, 3, 1)\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [r[metric] for r in all_results]\n",
    "        labels = [f\"{r['Model'][:3]}\\\\n{r['Vectorizer'][:10]}\" for r in all_results]\n",
    "        plt.scatter([i] * len(values), values, alpha=0.7, s=60)\n",
    "    \n",
    "    plt.xticks(range(len(metrics)), metrics)\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: F1-Score comparison\n",
    "    plt.subplot(2, 3, 2)\n",
    "    f1_scores = results_df['F1-Score']\n",
    "    model_labels = [f\"{row['Model'][:8]}\\\\n({row['Vectorizer'][:8]})\" for _, row in results_df.iterrows()]\n",
    "    \n",
    "    bars = plt.bar(range(len(f1_scores)), f1_scores, color='lightblue')\n",
    "    plt.xticks(range(len(f1_scores)), model_labels, rotation=45, ha='right')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.title('F1-Score by Model')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, f1_scores):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Plot 3: Confusion Matrix for best model\n",
    "    plt.subplot(2, 3, 3)\n",
    "    best_vec_name = best_f1['Vectorizer']\n",
    "    best_model_name = best_f1['Model']\n",
    "    best_predictions = results[best_vec_name][best_model_name]['predictions']\n",
    "    \n",
    "    cm = confusion_matrix(y_test, best_predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix\\\\n{best_model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    \n",
    "    # Plot 4: ROC Curve for best model (if available)\n",
    "    plt.subplot(2, 3, 4)\n",
    "    best_probabilities = results[best_vec_name][best_model_name]['probabilities']\n",
    "    \n",
    "    if best_probabilities is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, best_probabilities)\n",
    "        auc_score = roc_auc_score(y_test, best_probabilities)\n",
    "        \n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve - Best Model')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No probabilities\\\\navailable', ha='center', va='center', \n",
    "                transform=plt.gca().transAxes, fontsize=12)\n",
    "        plt.title('ROC Curve - Not Available')\n",
    "    \n",
    "    # Plot 5: Feature importance (if available)\n",
    "    plt.subplot(2, 3, 5)\n",
    "    best_model = results[best_vec_name][best_model_name]['model']\n",
    "    best_vectorizer = results[best_vec_name][best_model_name]['vectorizer']\n",
    "    \n",
    "    if hasattr(best_model, 'coef_'):\n",
    "        # For linear models\n",
    "        feature_names = best_vectorizer.get_feature_names_out()\n",
    "        importance = np.abs(best_model.coef_[0])\n",
    "        \n",
    "        # Get top 10 features\n",
    "        top_indices = importance.argsort()[-10:][::-1]\n",
    "        top_features = [feature_names[i] for i in top_indices]\n",
    "        top_importance = importance[top_indices]\n",
    "        \n",
    "        plt.barh(range(len(top_features)), top_importance)\n",
    "        plt.yticks(range(len(top_features)), top_features)\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title('Top 10 Features')\n",
    "    elif hasattr(best_model, 'feature_importances_'):\n",
    "        # For tree-based models\n",
    "        feature_names = best_vectorizer.get_feature_names_out()\n",
    "        importance = best_model.feature_importances_\n",
    "        \n",
    "        # Get top 10 features\n",
    "        top_indices = importance.argsort()[-10:][::-1]\n",
    "        top_features = [feature_names[i] for i in top_indices]\n",
    "        top_importance = importance[top_indices]\n",
    "        \n",
    "        plt.barh(range(len(top_features)), top_importance)\n",
    "        plt.yticks(range(len(top_features)), top_features)\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title('Top 10 Features')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Feature importance\\\\nnot available', ha='center', va='center',\n",
    "                transform=plt.gca().transAxes, fontsize=12)\n",
    "        plt.title('Feature Importance')\n",
    "    \n",
    "    # Plot 6: Model comparison heatmap\n",
    "    plt.subplot(2, 3, 6)\n",
    "    pivot_data = results_df.pivot(index='Model', columns='Vectorizer', values='F1-Score')\n",
    "    sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='RdYlBu_r')\n",
    "    plt.title('F1-Score Heatmap')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Train and evaluate traditional models\n",
    "if df_analyzed is not None:\n",
    "    print(\"\\\\nTraining Traditional Machine Learning Models:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    traditional_results, X_test_trad, y_test_trad = train_traditional_models(df_analyzed)\n",
    "    \n",
    "    if traditional_results:\n",
    "        results_comparison = evaluate_traditional_models(traditional_results, X_test_trad, y_test_trad)\n",
    "else:\n",
    "    print(\"No dataset available for traditional ML training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0552eaa3",
   "metadata": {},
   "source": [
    "## Solution 3: BERT-based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d040cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_bert_model(model_name='bert-base-german-cased'):\n",
    "    \"\"\"Set up BERT model for German text classification.\"\"\"\n",
    "    \n",
    "    if not TRANSFORMERS_AVAILABLE:\n",
    "        print(\"Transformers library not available for BERT approach.\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading BERT model: {model_name}\")\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2,  # Binary classification\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Successfully loaded {model_name}\")\n",
    "        print(f\"✓ Model parameters: {model.num_parameters():,}\")\n",
    "        \n",
    "        return tokenizer, model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading BERT model: {e}\")\n",
    "        print(\"Trying alternative model...\")\n",
    "        \n",
    "        # Try alternative models\n",
    "        alternative_models = [\n",
    "            'bert-base-multilingual-cased',\n",
    "            'distilbert-base-multilingual-cased',\n",
    "            'bert-base-uncased'\n",
    "        ]\n",
    "        \n",
    "        for alt_model in alternative_models:\n",
    "            try:\n",
    "                print(f\"Trying {alt_model}...\")\n",
    "                tokenizer = AutoTokenizer.from_pretrained(alt_model)\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                    alt_model,\n",
    "                    num_labels=2\n",
    "                )\n",
    "                print(f\"✓ Successfully loaded {alt_model}\")\n",
    "                return tokenizer, model\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(\"Could not load any BERT model.\")\n",
    "        return None, None\n",
    "\n",
    "def prepare_bert_data(texts, labels, tokenizer, max_length=128):\n",
    "    \"\"\"Prepare data for BERT training.\"\"\"\n",
    "    \n",
    "    if not tokenizer:\n",
    "        return None\n",
    "    \n",
    "    # Tokenize texts\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Create dataset class\n",
    "    class OffensiveLanguageDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            return item\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "    \n",
    "    dataset = OffensiveLanguageDataset(encodings, labels)\n",
    "    return dataset\n",
    "\n",
    "def train_bert_model(df, tokenizer, model):\n",
    "    \"\"\"Train BERT model for offensive language detection.\"\"\"\n",
    "    \n",
    "    if not TRANSFORMERS_AVAILABLE or tokenizer is None or model is None:\n",
    "        print(\"Cannot train BERT model - missing components.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Training BERT Model:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Prepare data\n",
    "    texts = df['text'].tolist()\n",
    "    labels = df['label'].tolist()\n",
    "    \n",
    "    # Split data\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(train_texts)}\")\n",
    "    print(f\"Validation samples: {len(val_texts)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = prepare_bert_data(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = prepare_bert_data(val_texts, val_labels, tokenizer)\n",
    "    \n",
    "    if train_dataset is None:\n",
    "        print(\"Could not prepare datasets.\")\n",
    "        return None\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    # Custom metrics\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "        \n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"✓ Training completed!\")\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(\"\\\\nValidation Results:\")\n",
    "        for key, value in eval_results.items():\n",
    "            if key.startswith('eval_'):\n",
    "                metric_name = key.replace('eval_', '').replace('_', ' ').title()\n",
    "                print(f\"{metric_name}: {value:.4f}\")\n",
    "        \n",
    "        return trainer, val_texts, val_labels\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_bert_model(trainer, tokenizer, val_texts, val_labels):\n",
    "    \"\"\"Evaluate BERT model performance.\"\"\"\n",
    "    \n",
    "    if not trainer or not tokenizer:\n",
    "        print(\"Cannot evaluate BERT model.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\\\nEvaluating BERT Model:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Make predictions\n",
    "    val_dataset = prepare_bert_data(val_texts, val_labels, tokenizer)\n",
    "    predictions = trainer.predict(val_dataset)\n",
    "    \n",
    "    # Get predicted labels\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    y_true = val_labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Non-offensive', 'Offensive']))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot 1: Confusion Matrix\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-offensive', 'Offensive'],\n",
    "                yticklabels=['Non-offensive', 'Offensive'])\n",
    "    plt.title('BERT Model - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    \n",
    "    # Plot 2: Prediction confidence distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    probabilities = torch.softmax(torch.tensor(predictions.predictions), dim=1)\n",
    "    confidence_scores = torch.max(probabilities, dim=1)[0].numpy()\n",
    "    \n",
    "    plt.hist(confidence_scores, bins=20, alpha=0.7, color='lightgreen')\n",
    "    plt.axvline(np.mean(confidence_scores), color='red', linestyle='--', \n",
    "               label=f'Mean: {np.mean(confidence_scores):.3f}')\n",
    "    plt.title('Prediction Confidence Distribution')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred, confidence_scores\n",
    "\n",
    "def create_bert_pipeline(tokenizer, model):\n",
    "    \"\"\"Create a pipeline for easy inference.\"\"\"\n",
    "    \n",
    "    if not tokenizer or not model:\n",
    "        print(\"Cannot create BERT pipeline.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Create Hugging Face pipeline\n",
    "        classifier = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        \n",
    "        return classifier\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating pipeline: {e}\")\n",
    "        return None\n",
    "\n",
    "# Set up and train BERT model\n",
    "print(\"\\\\nSetting up BERT-based Approach:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if df_analyzed is not None and len(df_analyzed) > 10:  # Ensure sufficient data\n",
    "    tokenizer, bert_model = setup_bert_model()\n",
    "    \n",
    "    if tokenizer and bert_model:\n",
    "        # Train BERT model\n",
    "        bert_trainer = train_bert_model(df_analyzed, tokenizer, bert_model)\n",
    "        \n",
    "        if bert_trainer:\n",
    "            trainer, val_texts, val_labels = bert_trainer\n",
    "            bert_predictions = evaluate_bert_model(trainer, tokenizer, val_texts, val_labels)\n",
    "            \n",
    "            # Create inference pipeline\n",
    "            bert_pipeline = create_bert_pipeline(tokenizer, trainer.model)\n",
    "            \n",
    "            if bert_pipeline:\n",
    "                print(\"\\\\n✓ BERT pipeline ready for inference!\")\n",
    "        else:\n",
    "            print(\"❌ BERT training failed\")\n",
    "            bert_pipeline = None\n",
    "    else:\n",
    "        print(\"❌ Could not set up BERT model\")\n",
    "        bert_pipeline = None\n",
    "else:\n",
    "    print(\"❌ Insufficient data for BERT training\")\n",
    "    bert_pipeline = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c365689",
   "metadata": {},
   "source": [
    "## Solution 4: Model Comparison and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6532bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models_on_examples(traditional_results, bert_pipeline):\n",
    "    \"\"\"Test both traditional and BERT models on example texts.\"\"\"\n",
    "    \n",
    "    print(\"Testing Models on Example Texts:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test examples (German and English)\n",
    "    test_examples = [\n",
    "        \"Das ist ein wunderschöner Tag!\",\n",
    "        \"Du bist so dumm!\",\n",
    "        \"Ich freue mich auf das Wochenende.\",\n",
    "        \"Das ist totaler Schwachsinn!\",\n",
    "        \"Vielen Dank für deine Hilfe.\",\n",
    "        \"Du redest nur Unsinn!\",\n",
    "        \"This is a beautiful day.\",\n",
    "        \"You are so stupid!\",\n",
    "        \"I appreciate your help.\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for text in test_examples:\n",
    "        print(f\"\\\\nText: '{text}'\")\n",
    "        print(\"-\" * (len(text) + 10))\n",
    "        \n",
    "        result = {'text': text}\n",
    "        \n",
    "        # Test traditional models (use best performing one)\n",
    "        if traditional_results:\n",
    "            # Find best traditional model\n",
    "            best_vec = None\n",
    "            best_model = None\n",
    "            best_f1 = 0\n",
    "            \n",
    "            for vec_name, vec_results in traditional_results.items():\n",
    "                for model_name, model_result in vec_results.items():\n",
    "                    if model_result['f1'] > best_f1:\n",
    "                        best_f1 = model_result['f1']\n",
    "                        best_vec = vec_name\n",
    "                        best_model = model_name\n",
    "            \n",
    "            if best_vec and best_model:\n",
    "                try:\n",
    "                    model = traditional_results[best_vec][best_model]['model']\n",
    "                    vectorizer = traditional_results[best_vec][best_model]['vectorizer']\n",
    "                    \n",
    "                    # Preprocess and predict\n",
    "                    processed_text = preprocess_text_features([text])[0]\n",
    "                    text_vec = vectorizer.transform([processed_text])\n",
    "                    trad_pred = model.predict(text_vec)[0]\n",
    "                    trad_proba = model.predict_proba(text_vec)[0] if hasattr(model, 'predict_proba') else None\n",
    "                    \n",
    "                    result['traditional_prediction'] = trad_pred\n",
    "                    result['traditional_model'] = f\"{best_model} + {best_vec}\"\n",
    "                    result['traditional_confidence'] = trad_proba[1] if trad_proba is not None else 'N/A'\n",
    "                    \n",
    "                    label = \"Offensive\" if trad_pred == 1 else \"Non-offensive\"\n",
    "                    conf_str = f\" (conf: {trad_proba[1]:.3f})\" if trad_proba is not None else \"\"\n",
    "                    print(f\"Traditional ML: {label}{conf_str}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Traditional ML: Error - {e}\")\n",
    "                    result['traditional_prediction'] = 'Error'\n",
    "        \n",
    "        # Test BERT model\n",
    "        if bert_pipeline:\n",
    "            try:\n",
    "                bert_result = bert_pipeline(text)\n",
    "                bert_label = bert_result[0]['label']\n",
    "                bert_score = bert_result[0]['score']\n",
    "                \n",
    "                # Convert label to binary (depends on model output format)\n",
    "                if 'NEGATIVE' in bert_label.upper() or 'LABEL_1' in bert_label.upper():\n",
    "                    bert_pred = 1\n",
    "                    label = \"Offensive\"\n",
    "                else:\n",
    "                    bert_pred = 0\n",
    "                    label = \"Non-offensive\"\n",
    "                \n",
    "                result['bert_prediction'] = bert_pred\n",
    "                result['bert_confidence'] = bert_score\n",
    "                \n",
    "                print(f\"BERT Model: {label} (conf: {bert_score:.3f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"BERT Model: Error - {e}\")\n",
    "                result['bert_prediction'] = 'Error'\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_model_performance(traditional_results, bert_results=None):\n",
    "    \"\"\"Compare performance of different models.\"\"\"\n",
    "    \n",
    "    print(\"\\\\n\\\\nModel Performance Comparison:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    # Traditional models\n",
    "    if traditional_results:\n",
    "        for vec_name, vec_results in traditional_results.items():\n",
    "            for model_name, model_result in vec_results.items():\n",
    "                comparison_data.append({\n",
    "                    'Model Type': 'Traditional ML',\n",
    "                    'Model': f\"{model_name}\",\n",
    "                    'Features': vec_name,\n",
    "                    'Accuracy': model_result['accuracy'],\n",
    "                    'F1-Score': model_result['f1'],\n",
    "                    'Precision': model_result['precision'],\n",
    "                    'Recall': model_result['recall']\n",
    "                })\n",
    "    \n",
    "    # BERT results (if available)\n",
    "    if bert_results:\n",
    "        comparison_data.append({\n",
    "            'Model Type': 'Deep Learning',\n",
    "            'Model': 'BERT',\n",
    "            'Features': 'Transformer Embeddings',\n",
    "            'Accuracy': bert_results.get('accuracy', 'N/A'),\n",
    "            'F1-Score': bert_results.get('f1', 'N/A'),\n",
    "            'Precision': bert_results.get('precision', 'N/A'),\n",
    "            'Recall': bert_results.get('recall', 'N/A')\n",
    "        })\n",
    "    \n",
    "    if comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        print(\"\\\\nPerformance Summary:\")\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot 1: F1-Score comparison\n",
    "        plt.subplot(2, 2, 1)\n",
    "        f1_scores = comparison_df['F1-Score']\n",
    "        model_labels = [f\"{row['Model']}\\\\n({row['Model Type']})\" for _, row in comparison_df.iterrows()]\n",
    "        \n",
    "        colors = ['lightblue' if 'Traditional' in label else 'lightcoral' for label in model_labels]\n",
    "        bars = plt.bar(range(len(f1_scores)), f1_scores, color=colors)\n",
    "        \n",
    "        plt.xticks(range(len(f1_scores)), [label.split('\\\\n')[0] for label in model_labels], rotation=45, ha='right')\n",
    "        plt.ylabel('F1-Score')\n",
    "        plt.title('F1-Score Comparison')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars, f1_scores):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{score:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        # Plot 2: Accuracy vs F1-Score\n",
    "        plt.subplot(2, 2, 2)\n",
    "        traditional_data = comparison_df[comparison_df['Model Type'] == 'Traditional ML']\n",
    "        bert_data = comparison_df[comparison_df['Model Type'] == 'Deep Learning']\n",
    "        \n",
    "        if not traditional_data.empty:\n",
    "            plt.scatter(traditional_data['Accuracy'], traditional_data['F1-Score'], \n",
    "                       label='Traditional ML', s=100, alpha=0.7, color='lightblue')\n",
    "        \n",
    "        if not bert_data.empty:\n",
    "            plt.scatter(bert_data['Accuracy'], bert_data['F1-Score'], \n",
    "                       label='BERT', s=100, alpha=0.7, color='lightcoral')\n",
    "        \n",
    "        plt.xlabel('Accuracy')\n",
    "        plt.ylabel('F1-Score')\n",
    "        plt.title('Accuracy vs F1-Score')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Model complexity vs performance\n",
    "        plt.subplot(2, 2, 3)\n",
    "        model_complexity = []\n",
    "        performance = []\n",
    "        labels = []\n",
    "        \n",
    "        for _, row in comparison_df.iterrows():\n",
    "            if row['Model Type'] == 'Traditional ML':\n",
    "                complexity = 1  # Simple\n",
    "                perf = row['F1-Score']\n",
    "                model_complexity.append(complexity)\n",
    "                performance.append(perf)\n",
    "                labels.append(row['Model'])\n",
    "            elif row['Model Type'] == 'Deep Learning':\n",
    "                complexity = 3  # Complex\n",
    "                perf = row['F1-Score'] if row['F1-Score'] != 'N/A' else 0\n",
    "                model_complexity.append(complexity)\n",
    "                performance.append(perf)\n",
    "                labels.append(row['Model'])\n",
    "        \n",
    "        colors = ['lightblue' if comp == 1 else 'lightcoral' for comp in model_complexity]\n",
    "        plt.scatter(model_complexity, performance, s=100, alpha=0.7, c=colors)\n",
    "        \n",
    "        for i, label in enumerate(labels):\n",
    "            plt.annotate(label, (model_complexity[i], performance[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        plt.xlabel('Model Complexity')\n",
    "        plt.ylabel('F1-Score')\n",
    "        plt.title('Complexity vs Performance')\n",
    "        plt.xticks([1, 3], ['Traditional ML', 'Deep Learning'])\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Training time comparison (estimated)\n",
    "        plt.subplot(2, 2, 4)\n",
    "        estimated_times = []\n",
    "        model_names = []\n",
    "        \n",
    "        for _, row in comparison_df.iterrows():\n",
    "            if row['Model Type'] == 'Traditional ML':\n",
    "                # Estimate based on model type\n",
    "                if 'SVM' in row['Model']:\n",
    "                    time_est = 30  # seconds\n",
    "                elif 'Random Forest' in row['Model']:\n",
    "                    time_est = 45\n",
    "                else:\n",
    "                    time_est = 15\n",
    "            else:  # BERT\n",
    "                time_est = 300  # 5 minutes\n",
    "            \n",
    "            estimated_times.append(time_est)\n",
    "            model_names.append(row['Model'])\n",
    "        \n",
    "        colors = ['lightblue' if time < 100 else 'lightcoral' for time in estimated_times]\n",
    "        bars = plt.bar(range(len(estimated_times)), estimated_times, color=colors)\n",
    "        \n",
    "        plt.xticks(range(len(model_names)), model_names, rotation=45, ha='right')\n",
    "        plt.ylabel('Estimated Training Time (seconds)')\n",
    "        plt.title('Training Time Comparison')\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return comparison_df\n",
    "    \n",
    "    else:\n",
    "        print(\"No comparison data available.\")\n",
    "        return None\n",
    "\n",
    "def save_best_model(traditional_results, bert_pipeline):\n",
    "    \"\"\"Save the best performing model for future use.\"\"\"\n",
    "    \n",
    "    if not JOBLIB_AVAILABLE and not bert_pipeline:\n",
    "        print(\"Cannot save models - joblib not available and no BERT pipeline.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\\\nSaving Best Model:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "    MODEL_DIR = PROJECT_ROOT / 'models'\n",
    "    MODEL_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    models_saved = []\n",
    "    \n",
    "    # Save best traditional model\n",
    "    if traditional_results and JOBLIB_AVAILABLE:\n",
    "        best_f1 = 0\n",
    "        best_config = None\n",
    "        \n",
    "        for vec_name, vec_results in traditional_results.items():\n",
    "            for model_name, model_result in vec_results.items():\n",
    "                if model_result['f1'] > best_f1:\n",
    "                    best_f1 = model_result['f1']\n",
    "                    best_config = (vec_name, model_name, model_result)\n",
    "        \n",
    "        if best_config:\n",
    "            vec_name, model_name, model_result = best_config\n",
    "            \n",
    "            # Save model and vectorizer\n",
    "            model_package = {\n",
    "                'model': model_result['model'],\n",
    "                'vectorizer': model_result['vectorizer'],\n",
    "                'model_name': model_name,\n",
    "                'vectorizer_name': vec_name,\n",
    "                'performance': {\n",
    "                    'accuracy': model_result['accuracy'],\n",
    "                    'f1': model_result['f1'],\n",
    "                    'precision': model_result['precision'],\n",
    "                    'recall': model_result['recall']\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            traditional_path = MODEL_DIR / 'best_traditional_model.joblib'\n",
    "            joblib.dump(model_package, traditional_path)\n",
    "            \n",
    "            models_saved.append(f\"Traditional ML: {traditional_path}\")\n",
    "            print(f\"✓ Saved traditional model: {model_name} + {vec_name}\")\n",
    "            print(f\"  Performance - F1: {best_f1:.3f}\")\n",
    "    \n",
    "    # Save BERT model (if trained)\n",
    "    if bert_pipeline:\n",
    "        try:\n",
    "            bert_path = MODEL_DIR / 'bert_offensive_language_model'\n",
    "            bert_path.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Note: In a real scenario, you would save the trained model\n",
    "            # bert_pipeline.model.save_pretrained(bert_path)\n",
    "            # bert_pipeline.tokenizer.save_pretrained(bert_path)\n",
    "            \n",
    "            models_saved.append(f\"BERT model: {bert_path}\")\n",
    "            print(f\"✓ BERT model saved to: {bert_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving BERT model: {e}\")\n",
    "    \n",
    "    # Create usage example\n",
    "    if models_saved:\n",
    "        usage_example = f'''# Model Usage Example\n",
    "\n",
    "## Loading and Using Saved Models\n",
    "\n",
    "### Traditional ML Model\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Load the model\n",
    "model_package = joblib.load('{MODEL_DIR / 'best_traditional_model.joblib'}')\n",
    "model = model_package['model']\n",
    "vectorizer = model_package['vectorizer']\n",
    "\n",
    "# Make predictions\n",
    "def predict_offensive(text):\n",
    "    # Preprocess text (same as training)\n",
    "    processed = preprocess_text_features([text])[0]\n",
    "    \n",
    "    # Vectorize\n",
    "    text_vec = vectorizer.transform([processed])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(text_vec)[0]\n",
    "    probability = model.predict_proba(text_vec)[0][1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    return {{\n",
    "        'prediction': 'Offensive' if prediction == 1 else 'Non-offensive',\n",
    "        'confidence': probability\n",
    "    }}\n",
    "\n",
    "# Example usage\n",
    "result = predict_offensive(\"Das ist ein schöner Tag!\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "### BERT Model\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the pipeline (if BERT was trained)\n",
    "classifier = pipeline(\"text-classification\", model=\"{MODEL_DIR / 'bert_offensive_language_model'}\")\n",
    "\n",
    "# Make predictions\n",
    "result = classifier(\"Das ist ein schöner Tag!\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "## Model Performance Summary\n",
    "{f\"Best Traditional Model: {best_config[1]} + {best_config[0]} (F1: {best_config[2]['f1']:.3f})\" if 'best_config' in locals() and best_config else \"No traditional model available\"}\n",
    "{\"BERT Model: Available for inference\" if bert_pipeline else \"BERT Model: Not available\"}\n",
    "\n",
    "## Deployment Notes\n",
    "- Traditional models are lightweight and fast\n",
    "- BERT models provide better accuracy but require more resources\n",
    "- Consider ensemble methods for production use\n",
    "- Implement proper input validation and preprocessing\n",
    "'''\n",
    "        \n",
    "        usage_path = MODEL_DIR / 'model_usage_guide.md'\n",
    "        with open(usage_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(usage_example)\n",
    "        \n",
    "        print(f\"✓ Created usage guide: {usage_path}\")\n",
    "    \n",
    "    return models_saved\n",
    "\n",
    "# Test and compare models\n",
    "print(\"\\\\n\\\\nTesting and Comparing Models:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test models on examples\n",
    "if 'traditional_results' in locals():\n",
    "    test_results = test_models_on_examples(traditional_results, \n",
    "                                         bert_pipeline if 'bert_pipeline' in locals() else None)\n",
    "    \n",
    "    # Compare performance\n",
    "    comparison_summary = compare_model_performance(traditional_results)\n",
    "    \n",
    "    # Save best models\n",
    "    saved_models = save_best_model(traditional_results, \n",
    "                                 bert_pipeline if 'bert_pipeline' in locals() else None)\n",
    "    \n",
    "    print(f\"\\\\n\\\\n🎉 Offensive Language Detection Solutions Completed!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\\\n📊 What was accomplished:\")\n",
    "    print(\"• Dataset creation and analysis\")\n",
    "    print(\"• Traditional ML models (Logistic Regression, Random Forest, SVM, Naive Bayes)\")\n",
    "    print(\"• Multiple feature extraction methods (TF-IDF, Count Vectorizer, Char-level)\")\n",
    "    print(\"• BERT-based deep learning approach\")\n",
    "    print(\"• Comprehensive model evaluation and comparison\")\n",
    "    print(\"• Model persistence and deployment preparation\")\n",
    "    \n",
    "    if saved_models:\n",
    "        print(f\"\\\\n💾 Saved models:\")\n",
    "        for model_path in saved_models:\n",
    "            print(f\"  • {model_path}\")\n",
    "    \n",
    "    print(\"\\\\n🚀 Key techniques covered:\")\n",
    "    print(\"• Text preprocessing and feature engineering\")\n",
    "    print(\"• Traditional machine learning pipelines\")\n",
    "    print(\"• Transformer-based classification\")\n",
    "    print(\"• Model evaluation and comparison\")\n",
    "    print(\"• Production deployment considerations\")\n",
    "else:\n",
    "    print(\"❌ No models were trained successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
