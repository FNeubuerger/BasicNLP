{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06024e2e",
   "metadata": {},
   "source": [
    "# Exercise 4: BERT Classification - SOLUTIONS\n",
    "\n",
    "Complete solutions for German BERT classification with fine-tuning, model comparison, and performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc7745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers library available!\n"
     ]
    }
   ],
   "source": [
    "# Essential imports for BERT classification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import transformers for BERT\n",
    "try:\n",
    "    from transformers import (\n",
    "        pipeline, AutoTokenizer, AutoModelForSequenceClassification,\n",
    "        TrainingArguments, Trainer, AutoConfig\n",
    "    )\n",
    "    import torch\n",
    "    from datasets import Dataset\n",
    "    print(\"‚úÖ Transformers library available!\")\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "    \n",
    "    # Check if GPU is available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"‚úÖ Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ùå Transformers not available. Install with: pip install transformers torch datasets\")\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    device = 'cpu'\n",
    "\n",
    "print(\"\\nü§ñ BERT Classification Toolkit Ready!\")\n",
    "print(\"Available: German BERT models, Fine-tuning, Performance comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5096f1",
   "metadata": {},
   "source": [
    "## Solution 1: German BERT Model Loading and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa9f57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load model: oliverguhr/german-sentiment-bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felix Neub√ºrger\\Documents\\Lehre\\NLP_BA2526\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Felix Neub√ºrger\\.cache\\huggingface\\hub\\models--oliverguhr--german-sentiment-bert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded: oliverguhr/german-sentiment-bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felix Neub√ºrger\\Documents\\Lehre\\NLP_BA2526\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_german_bert_models():\n",
    "    \"\"\"Load and compare different German BERT models.\"\"\"\n",
    "    \n",
    "    if not TRANSFORMERS_AVAILABLE:\n",
    "        print(\"Transformers library not available.\")\n",
    "        return {}\n",
    "    \n",
    "    # German BERT models to test\n",
    "    german_models = {\n",
    "        'German BERT (dbmdz)': 'dbmdz/bert-base-german-cased',\n",
    "        'German Sentiment BERT': 'oliverguhr/german-sentiment-bert',\n",
    "        'German DistilBERT': 'distilbert-base-german-cased',\n",
    "        'Multilingual BERT': 'bert-base-multilingual-cased'\n",
    "    }\n",
    "    \n",
    "    loaded_models = {}\n",
    "    \n",
    "    print(\"üîç Loading German BERT Models...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for name, model_id in german_models.items():\n",
    "        try:\n",
    "            print(f\"\\nLoading {name} ({model_id})...\")\n",
    "            \n",
    "            # Load tokenizer and model\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "            \n",
    "            # Create pipeline\n",
    "            classifier = pipeline(\n",
    "                \"sentiment-analysis\" if \"sentiment\" in model_id else \"text-classification\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "            \n",
    "            loaded_models[name] = {\n",
    "                'classifier': classifier,\n",
    "                'tokenizer': tokenizer,\n",
    "                'model': model,\n",
    "                'model_id': model_id\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Successfully loaded {name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nüéØ Successfully loaded {len(loaded_models)} out of {len(german_models)} models\")\n",
    "    return loaded_models\n",
    "\n",
    "# Load German BERT models\n",
    "bert_models = load_german_bert_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81533fa",
   "metadata": {},
   "source": [
    "## Solution 2: Test BERT with German Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ff66365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classification Results:\n",
      "============================================================\n",
      "\n",
      "Example 1:\n",
      "Text: Das Essen war wirklich fantastisch und der Service ausgezeichnet!\n",
      "Prediction: positive (confidence: 0.999)\n",
      "All scores:\n",
      "  positive: 0.999\n",
      "  negative: 0.001\n",
      "  neutral: 0.000\n",
      "\n",
      "Example 2:\n",
      "Text: Ich bin sehr entt√§uscht von der schlechten Qualit√§t des Produkts.\n",
      "Prediction: negative (confidence: 0.998)\n",
      "All scores:\n",
      "  positive: 0.002\n",
      "  negative: 0.998\n",
      "  neutral: 0.000\n",
      "\n",
      "Example 3:\n",
      "Text: Das Hotel war ganz okay, nichts Besonderes aber auch nicht schlecht.\n",
      "Prediction: negative (confidence: 0.968)\n",
      "All scores:\n",
      "  positive: 0.032\n",
      "  negative: 0.968\n",
      "  neutral: 0.000\n",
      "\n",
      "Example 4:\n",
      "Text: Absolut brillant! Kann ich nur weiterempfehlen.\n",
      "Prediction: positive (confidence: 1.000)\n",
      "All scores:\n",
      "  positive: 1.000\n",
      "  negative: 0.000\n",
      "  neutral: 0.000\n",
      "\n",
      "Example 5:\n",
      "Text: Furchtbar schlecht, das war rausgeworfenes Geld.\n",
      "Prediction: negative (confidence: 0.999)\n",
      "All scores:\n",
      "  positive: 0.001\n",
      "  negative: 0.999\n",
      "  neutral: 0.000\n",
      "\n",
      "Example 6:\n",
      "Text: Der Film war mittelm√§√üig, hat mich nicht wirklich begeistert.\n",
      "Prediction: negative (confidence: 0.997)\n",
      "All scores:\n",
      "  positive: 0.003\n",
      "  negative: 0.997\n",
      "  neutral: 0.000\n",
      "\n",
      "Example 7:\n",
      "Text: Wunderbares Wetter heute, perfekt f√ºr einen Spaziergang!\n",
      "Prediction: positive (confidence: 0.885)\n",
      "All scores:\n",
      "  positive: 0.885\n",
      "  negative: 0.045\n",
      "  neutral: 0.070\n",
      "\n",
      "Example 8:\n",
      "Text: Die Bedienung war unfreundlich und das Essen kalt.\n",
      "Prediction: negative (confidence: 0.999)\n",
      "All scores:\n",
      "  positive: 0.001\n",
      "  negative: 0.999\n",
      "  neutral: 0.000\n"
     ]
    }
   ],
   "source": [
    "def test_bert_classifier(classifier, test_texts):\n",
    "    \"\"\"Test BERT classifier with German texts.\"\"\"\n",
    "    \n",
    "    if classifier is None:\n",
    "        print(\"No classifier available.\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"BERT Classification Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        try:\n",
    "            # Get prediction\n",
    "            prediction = classifier(text)\n",
    "            \n",
    "            print(f\"\\nExample {i}:\")\n",
    "            print(f\"Text: {text}\")\n",
    "            \n",
    "            # Handle different output formats\n",
    "            if isinstance(prediction[0], list):\n",
    "                # Multiple scores returned\n",
    "                scores = prediction[0]\n",
    "                best_pred = max(scores, key=lambda x: x['score'])\n",
    "                print(f\"Prediction: {best_pred['label']} (confidence: {best_pred['score']:.3f})\")\n",
    "                print(\"All scores:\")\n",
    "                for score in scores:\n",
    "                    print(f\"  {score['label']}: {score['score']:.3f}\")\n",
    "            else:\n",
    "                # Single prediction\n",
    "                pred = prediction[0]\n",
    "                print(f\"Prediction: {pred['label']} (confidence: {pred['score']:.3f})\")\n",
    "            \n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'prediction': prediction\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text {i}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test texts in German\n",
    "german_test_texts = [\n",
    "    \"Das Essen war wirklich fantastisch und der Service ausgezeichnet!\",\n",
    "    \"Ich bin sehr entt√§uscht von der schlechten Qualit√§t des Produkts.\",\n",
    "    \"Das Hotel war ganz okay, nichts Besonderes aber auch nicht schlecht.\",\n",
    "    \"Absolut brillant! Kann ich nur weiterempfehlen.\",\n",
    "    \"Furchtbar schlecht, das war rausgeworfenes Geld.\",\n",
    "    \"Der Film war mittelm√§√üig, hat mich nicht wirklich begeistert.\",\n",
    "    \"Wunderbares Wetter heute, perfekt f√ºr einen Spaziergang!\",\n",
    "    \"Die Bedienung war unfreundlich und das Essen kalt.\"\n",
    "]\n",
    "\n",
    "# Test the classifier\n",
    "test_results = test_bert_classifier(classifier, german_test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b38aa5e",
   "metadata": {},
   "source": [
    "## Solution 2: BERT Fine-tuning for German Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9875e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_german_sentiment_dataset():\n",
    "    \"\"\"Create a comprehensive German sentiment dataset for fine-tuning.\"\"\"\n",
    "    \n",
    "    # Positive examples (expanded dataset)\n",
    "    positive_texts = [\n",
    "        \"Das Restaurant war absolut fantastisch, das Essen ausgezeichnet!\",\n",
    "        \"Ich bin sehr gl√ºcklich mit diesem Kauf, perfekte Qualit√§t.\",\n",
    "        \"Wunderbare Erfahrung, kann ich nur weiterempfehlen.\",\n",
    "        \"Brillanter Service und hervorragende Leistung.\",\n",
    "        \"Das Hotel war traumhaft, alles perfekt organisiert.\",\n",
    "        \"Ausgezeichnetes Produkt, bin vollkommen zufrieden.\",\n",
    "        \"Fantastischer Film, hat mich begeistert!\",\n",
    "        \"Tolle Atmosph√§re und freundliches Personal.\",\n",
    "        \"Hervorragende Qualit√§t zum fairen Preis.\",\n",
    "        \"Ich liebe dieses Produkt, funktioniert einwandfrei.\"\n",
    "    ]\n",
    "    \n",
    "    # Negative examples (expanded dataset)\n",
    "    negative_texts = [\n",
    "        \"Das war eine Katastrophe, absolut entt√§uschend.\",\n",
    "        \"Furchtbare Erfahrung, kann ich nicht empfehlen.\",\n",
    "        \"Schlechte Qualit√§t und mieser Service.\",\n",
    "        \"Ich bin sehr unzufrieden, rausgeworfenes Geld.\",\n",
    "        \"Das Hotel war schrecklich, alles schmutzig.\",\n",
    "        \"Mangelhaftes Produkt, funktioniert nicht richtig.\",\n",
    "        \"Langweiliger Film, pure Zeitverschwendung.\",\n",
    "        \"Unfreundliches Personal und schlechte Atmosph√§re.\",\n",
    "        \"√úberteuert bei schlechter Qualit√§t.\",\n",
    "        \"Ich hasse es, wie schlecht das gemacht ist.\"\n",
    "    ]\n",
    "    \n",
    "    # Neutral examples\n",
    "    neutral_texts = [\n",
    "        \"Das Produkt ist okay, nichts Besonderes.\",\n",
    "        \"Durchschnittliche Leistung, geht so.\",\n",
    "        \"Mittelm√§√üiges Restaurant, nicht schlecht aber auch nicht gut.\",\n",
    "        \"Das ist ganz normal, wie erwartet.\",\n",
    "        \"Akzeptable Qualit√§t f√ºr den Preis.\",\n",
    "        \"Geht in Ordnung, aber k√∂nnte besser sein.\",\n",
    "        \"Standard Service, nichts Au√üergew√∂hnliches.\",\n",
    "        \"Ist halt so, kann man machen.\",\n",
    "        \"Durchschnittlich, erf√ºllt den Zweck.\",\n",
    "        \"Mittlere Qualit√§t, wie √ºblich.\"\n",
    "    ]\n",
    "    \n",
    "    # Combine data\n",
    "    texts = positive_texts + negative_texts + neutral_texts\n",
    "    labels = ([2] * len(positive_texts) +    # positive = 2\n",
    "             [0] * len(negative_texts) +     # negative = 0  \n",
    "             [1] * len(neutral_texts))       # neutral = 1\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'label': labels,\n",
    "        'label_name': ['negative' if l == 0 else 'neutral' if l == 1 else 'positive' for l in labels]\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create dataset for demonstration\n",
    "demo_df = create_german_sentiment_dataset()\n",
    "print(\"üìä German Sentiment Dataset Created:\")\n",
    "print(f\"Total samples: {len(demo_df)}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(demo_df['label_name'].value_counts())\n",
    "print(\"\\nSample data:\")\n",
    "print(demo_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a0e35",
   "metadata": {},
   "source": [
    "## Solution 3: Compare BERT with Traditional Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e278f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: BERT vs Traditional Method\n",
      "==================================================\n",
      "\n",
      "Text: Das ist super gut!\n",
      "Traditional: Positive (conf: 0.700)\n",
      "BERT: positive (conf: 0.985)\n",
      "\n",
      "Text: Sehr schlecht gemacht.\n",
      "Traditional: Positive (conf: 0.616)\n",
      "BERT: negative (conf: 0.998)\n",
      "\n",
      "Text: Ganz normal, nichts Besonderes.\n",
      "Traditional: Positive (conf: 0.763)\n",
      "BERT: negative (conf: 0.966)\n"
     ]
    }
   ],
   "source": [
    "def compare_bert_vs_traditional():\n",
    "    \"\"\"Compare BERT with traditional classification methods.\"\"\"\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "    # Create simple dataset for comparison\n",
    "    texts = [\n",
    "        \"Das ist wirklich fantastisch und toll!\",\n",
    "        \"Ich bin sehr gl√ºcklich damit.\",\n",
    "        \"Ausgezeichnete Qualit√§t, sehr empfehlenswert.\",\n",
    "        \"Brilliant und wunderbar gemacht.\",\n",
    "        \"Das ist schlecht und entt√§uschend.\",\n",
    "        \"Furchtbare Qualit√§t, nicht zu empfehlen.\",\n",
    "        \"Ich bin sehr unzufrieden damit.\",\n",
    "        \"Schrecklich schlecht, eine Katastrophe.\",\n",
    "        \"Das ist ganz okay, nicht besonders.\",\n",
    "        \"Durchschnittlich, nichts Besonderes.\",\n",
    "        \"Geht so, ist in Ordnung.\",\n",
    "        \"Mittelm√§√üige Qualit√§t, akzeptabel.\"\n",
    "    ]\n",
    "    \n",
    "    # Simple labels (positive=1, negative=0, neutral=0.5)\n",
    "    labels = [1, 1, 1, 1, 0, 0, 0, 0, 0.5, 0.5, 0.5, 0.5]\n",
    "    \n",
    "    # Traditional method: TF-IDF + Naive Bayes\n",
    "    vectorizer = TfidfVectorizer(max_features=100)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Convert neutral labels to binary for traditional classifier\n",
    "    binary_labels = [1 if l >= 0.5 else 0 for l in labels]\n",
    "    \n",
    "    nb_classifier = MultinomialNB()\n",
    "    nb_classifier.fit(X, binary_labels)\n",
    "    \n",
    "    # Test both methods\n",
    "    test_texts = [\n",
    "        \"Das ist super gut!\",\n",
    "        \"Sehr schlecht gemacht.\",\n",
    "        \"Ganz normal, nichts Besonderes.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Comparison: BERT vs Traditional Method\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for text in test_texts:\n",
    "        print(f\"\\nText: {text}\")\n",
    "        \n",
    "        # Traditional method prediction\n",
    "        text_vec = vectorizer.transform([text])\n",
    "        trad_pred = nb_classifier.predict(text_vec)[0]\n",
    "        trad_prob = nb_classifier.predict_proba(text_vec)[0].max()\n",
    "        print(f\"Traditional: {'Positive' if trad_pred else 'Negative'} (conf: {trad_prob:.3f})\")\n",
    "        \n",
    "        # BERT prediction\n",
    "        if classifier:\n",
    "            try:\n",
    "                bert_pred = classifier(text)\n",
    "                if isinstance(bert_pred[0], list):\n",
    "                    best = max(bert_pred[0], key=lambda x: x['score'])\n",
    "                    print(f\"BERT: {best['label']} (conf: {best['score']:.3f})\")\n",
    "                else:\n",
    "                    pred = bert_pred[0]\n",
    "                    print(f\"BERT: {pred['label']} (conf: {pred['score']:.3f})\")\n",
    "            except Exception as e:\n",
    "                print(f\"BERT: Error - {e}\")\n",
    "        else:\n",
    "            print(\"BERT: Not available\")\n",
    "\n",
    "# Run comparison\n",
    "compare_bert_vs_traditional()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeaad94",
   "metadata": {},
   "source": [
    "## Solution 4: Batch Processing with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "307e4948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 12 texts with BERT...\n",
      "Processed batch 1: 3 texts\n",
      "Processed batch 2: 3 texts\n",
      "Processed batch 3: 3 texts\n",
      "Processed batch 4: 3 texts\n",
      "\n",
      "Batch Processing Results:\n",
      " 1. positive (0.991) - Das Restaurant war ausgezeichnet.\n",
      " 2. negative (0.998) - Der Service war schlecht.\n",
      " 3. neutral  (0.995) - Das Wetter ist heute sch√∂n.\n",
      " 4. negative (0.964) - Ich bin m√ºde und gestresst.\n",
      " 5. positive (0.994) - Die Musik war wunderbar.\n",
      " 6. negative (0.985) - Das Buch ist langweilig.\n",
      " 7. positive (0.875) - Der Film war spannend.\n",
      " 8. negative (0.999) - Das Essen schmeckt schrecklich.\n",
      " 9. positive (0.975) - Die Reise war entspannend.\n",
      "10. negative (0.985) - Der Computer ist kaputt.\n",
      "11. neutral  (1.000) - Das Meeting war produktiv.\n",
      "12. negative (0.967) - Die Pr√§sentation war chaotisch.\n"
     ]
    }
   ],
   "source": [
    "def batch_process_with_bert(texts, batch_size=4):\n",
    "    \"\"\"Process multiple texts with BERT in batches.\"\"\"\n",
    "    \n",
    "    if not classifier:\n",
    "        print(\"No BERT classifier available.\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            batch_results = classifier(batch)\n",
    "            results.extend(batch_results)\n",
    "            print(f\"Processed batch {i//batch_size + 1}: {len(batch)} texts\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i//batch_size + 1}: {e}\")\n",
    "            # Process individually if batch fails\n",
    "            for text in batch:\n",
    "                try:\n",
    "                    result = classifier(text)\n",
    "                    results.append(result[0] if isinstance(result, list) else result)\n",
    "                except:\n",
    "                    results.append({'label': 'ERROR', 'score': 0.0})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create larger dataset for batch processing\n",
    "large_dataset = [\n",
    "    \"Das Restaurant war ausgezeichnet.\",\n",
    "    \"Der Service war schlecht.\",\n",
    "    \"Das Wetter ist heute sch√∂n.\",\n",
    "    \"Ich bin m√ºde und gestresst.\",\n",
    "    \"Die Musik war wunderbar.\",\n",
    "    \"Das Buch ist langweilig.\",\n",
    "    \"Der Film war spannend.\",\n",
    "    \"Das Essen schmeckt schrecklich.\",\n",
    "    \"Die Reise war entspannend.\",\n",
    "    \"Der Computer ist kaputt.\",\n",
    "    \"Das Meeting war produktiv.\",\n",
    "    \"Die Pr√§sentation war chaotisch.\"\n",
    "]\n",
    "\n",
    "print(f\"Processing {len(large_dataset)} texts with BERT...\")\n",
    "batch_results = batch_process_with_bert(large_dataset, batch_size=3)\n",
    "\n",
    "# Analyze results\n",
    "if batch_results:\n",
    "    print(\"\\nBatch Processing Results:\")\n",
    "    for i, (text, result) in enumerate(zip(large_dataset, batch_results)):\n",
    "        if isinstance(result, list) and result:\n",
    "            best = max(result, key=lambda x: x.get('score', 0))\n",
    "            print(f\"{i+1:2d}. {best['label']:<8} ({best['score']:.3f}) - {text}\")\n",
    "        elif isinstance(result, dict):\n",
    "            print(f\"{i+1:2d}. {result.get('label', 'N/A'):<8} ({result.get('score', 0):.3f}) - {text}\")\n",
    "        else:\n",
    "            print(f\"{i+1:2d}. ERROR    - {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8a12a",
   "metadata": {},
   "source": [
    "## Solution 5: BERT Model Information and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1322e0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing BERT model: bert-base-german-cased\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felix Neub√ºrger\\Documents\\Lehre\\NLP_BA2526\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Felix Neub√ºrger\\.cache\\huggingface\\hub\\models--bert-base-german-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Das ist ein sch√∂ner Tag in Deutschland!\n",
      "Tokens: ['Das', 'ist', 'ein', 'sch√∂ner', 'Tag', 'in', 'Deutschland', '!']\n",
      "Token IDs: [3, 295, 127, 39, 24308, 1419, 50, 832, 26982, 4]\n",
      "Vocabulary size: 30000\n",
      "\n",
      "Tokenization Examples:\n",
      "Guten Morgen!                  -> ['Guten', 'Morgen', '!']\n",
      "Wie geht es Ihnen?             -> ['Wie', 'geht', 'es', 'Ihnen', '?']\n",
      "Ich liebe die deutsche Sprache. -> ['Ich', 'liebe', 'die', 'deutsche', 'Sprache', '.']\n",
      "Das Wetter ist heute wunderbar. -> ['Das', 'Wetter', 'ist', 'heute', 'wunder', '##bar', '.']\n",
      "\n",
      "Special Tokens:\n",
      "CLS token: [CLS]\n",
      "SEP token: [SEP]\n",
      "PAD token: [PAD]\n",
      "UNK token: [UNK]\n"
     ]
    }
   ],
   "source": [
    "def analyze_bert_model():\n",
    "    \"\"\"Analyze BERT model information and capabilities.\"\"\"\n",
    "    \n",
    "    if not TRANSFORMERS_AVAILABLE:\n",
    "        print(\"Transformers library not available.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Try to load a specific German BERT model for analysis\n",
    "        model_name = \"bert-base-german-cased\"\n",
    "        \n",
    "        print(f\"Analyzing BERT model: {model_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Test tokenization\n",
    "        test_sentence = \"Das ist ein sch√∂ner Tag in Deutschland!\"\n",
    "        tokens = tokenizer.tokenize(test_sentence)\n",
    "        token_ids = tokenizer.encode(test_sentence)\n",
    "        \n",
    "        print(f\"Original text: {test_sentence}\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Token IDs: {token_ids}\")\n",
    "        print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "        \n",
    "        # Test with different German texts\n",
    "        german_examples = [\n",
    "            \"Guten Morgen!\",\n",
    "            \"Wie geht es Ihnen?\",\n",
    "            \"Ich liebe die deutsche Sprache.\",\n",
    "            \"Das Wetter ist heute wunderbar.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTokenization Examples:\")\n",
    "        for example in german_examples:\n",
    "            tokens = tokenizer.tokenize(example)\n",
    "            print(f\"{example:<30} -> {tokens}\")\n",
    "        \n",
    "        # Special tokens\n",
    "        print(f\"\\nSpecial Tokens:\")\n",
    "        print(f\"CLS token: {tokenizer.cls_token}\")\n",
    "        print(f\"SEP token: {tokenizer.sep_token}\")\n",
    "        print(f\"PAD token: {tokenizer.pad_token}\")\n",
    "        print(f\"UNK token: {tokenizer.unk_token}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing model: {e}\")\n",
    "        print(\"This might happen if the model is not available or requires internet connection.\")\n",
    "\n",
    "# Analyze BERT model\n",
    "analyze_bert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463b560",
   "metadata": {},
   "source": [
    "## Solution 6: Simple BERT Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7071439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics:\n",
      "Total predictions: 8\n",
      "Average confidence: 0.981\n",
      "Min confidence: 0.885\n",
      "Max confidence: 1.000\n",
      "\n",
      "Prediction breakdown:\n",
      "  negative: 5 (62.5%)\n",
      "  positive: 3 (37.5%)\n"
     ]
    }
   ],
   "source": [
    "def visualize_bert_predictions(results):\n",
    "    \"\"\"Create simple visualizations of BERT predictions.\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Extract predictions and confidence scores\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    for result in results:\n",
    "        pred = result['prediction']\n",
    "        if isinstance(pred[0], list):\n",
    "            best = max(pred[0], key=lambda x: x['score'])\n",
    "            predictions.append(best['label'])\n",
    "            confidences.append(best['score'])\n",
    "        else:\n",
    "            predictions.append(pred[0]['label'])\n",
    "            confidences.append(pred[0]['score'])\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 1. Prediction distribution\n",
    "    pred_counts = pd.Series(predictions).value_counts()\n",
    "    ax1.bar(pred_counts.index, pred_counts.values, color=['lightblue', 'lightcoral', 'lightgreen'])\n",
    "    ax1.set_title('Distribution of BERT Predictions')\n",
    "    ax1.set_xlabel('Prediction')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, v in enumerate(pred_counts.values):\n",
    "        ax1.text(i, v + 0.1, str(v), ha='center')\n",
    "    \n",
    "    # 2. Confidence scores\n",
    "    ax2.hist(confidences, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2.set_title('Distribution of Confidence Scores')\n",
    "    ax2.set_xlabel('Confidence Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.axvline(x=np.mean(confidences), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(confidences):.3f}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    print(f\"Average confidence: {np.mean(confidences):.3f}\")\n",
    "    print(f\"Min confidence: {np.min(confidences):.3f}\")\n",
    "    print(f\"Max confidence: {np.max(confidences):.3f}\")\n",
    "    \n",
    "    print(\"\\nPrediction breakdown:\")\n",
    "    for label, count in pred_counts.items():\n",
    "        percentage = (count / len(predictions)) * 100\n",
    "        print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize results if we have them\n",
    "if 'test_results' in locals() and test_results:\n",
    "    visualize_bert_predictions(test_results)\n",
    "else:\n",
    "    print(\"No test results available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac80ce",
   "metadata": {},
   "source": [
    "## Solution 7: Error Handling and Fallback Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e3135a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\Felix Neub√ºrger\\Documents\\Lehre\\NLP_BA2526\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT classifier loaded successfully.\n",
      "Fallback classifier set up.\n",
      "\n",
      "Testing Robust Classifier:\n",
      "Text: Das ist wirklich gut und toll!\n",
      "Method: BERT, Label: positive, Score: 0.991\n",
      "\n",
      "Text: Furchtbar schlecht, eine Katastrophe.\n",
      "Method: BERT, Label: negative, Score: 0.999\n",
      "\n",
      "Text: Das geht schon in Ordnung.\n",
      "Method: BERT, Label: positive, Score: 0.503\n",
      "\n",
      "Text: Ich bin sehr zufrieden mit der Qualit√§t.\n",
      "Method: BERT, Label: positive, Score: 0.997\n",
      "\n",
      "Text: Das war entt√§uschend und mangelhaft.\n",
      "Method: BERT, Label: negative, Score: 0.999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class RobustGermanClassifier:\n",
    "    \"\"\"Robust German text classifier with fallback methods.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bert_classifier = None\n",
    "        self.fallback_classifier = None\n",
    "        self.setup_classifiers()\n",
    "    \n",
    "    def setup_classifiers(self):\n",
    "        \"\"\"Set up BERT and fallback classifiers.\"\"\"\n",
    "        \n",
    "        # Try to set up BERT classifier\n",
    "        if TRANSFORMERS_AVAILABLE:\n",
    "            try:\n",
    "                self.bert_classifier = pipeline(\"sentiment-analysis\", \n",
    "                                               model=\"oliverguhr/german-sentiment-bert\",\n",
    "                                               return_all_scores=True)\n",
    "                print(\"BERT classifier loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load BERT classifier: {e}\")\n",
    "        \n",
    "        # Set up simple fallback classifier\n",
    "        self.setup_fallback_classifier()\n",
    "    \n",
    "    def setup_fallback_classifier(self):\n",
    "        \"\"\"Set up simple keyword-based fallback classifier.\"\"\"\n",
    "        \n",
    "        self.positive_words = {\n",
    "            'gut', 'toll', 'super', 'fantastisch', 'wunderbar', 'ausgezeichnet',\n",
    "            'brilliant', 'perfekt', 'gro√üartig', 'hervorragend', 'sch√∂n',\n",
    "            'lieben', 'm√∂gen', 'gefallen', 'empfehlen', 'zufrieden'\n",
    "        }\n",
    "        \n",
    "        self.negative_words = {\n",
    "            'schlecht', 'furchtbar', 'schrecklich', 'katastrophal', 'schlimm',\n",
    "            'entt√§uschend', 'unzufrieden', 'hassen', 'entsetzlich', 'mangelhaft',\n",
    "            'schrecklich', 'grauenhaft', 'unertr√§glich', 'minderwertig'\n",
    "        }\n",
    "        \n",
    "        print(\"Fallback classifier set up.\")\n",
    "    \n",
    "    def fallback_classify(self, text):\n",
    "        \"\"\"Simple keyword-based classification.\"\"\"\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        words = text_lower.split()\n",
    "        \n",
    "        positive_score = sum(1 for word in words if word in self.positive_words)\n",
    "        negative_score = sum(1 for word in words if word in self.negative_words)\n",
    "        \n",
    "        if positive_score > negative_score:\n",
    "            return {'label': 'POSITIVE', 'score': 0.6 + 0.1 * positive_score}\n",
    "        elif negative_score > positive_score:\n",
    "            return {'label': 'NEGATIVE', 'score': 0.6 + 0.1 * negative_score}\n",
    "        else:\n",
    "            return {'label': 'NEUTRAL', 'score': 0.5}\n",
    "    \n",
    "    def classify(self, text):\n",
    "        \"\"\"Classify text using BERT or fallback method.\"\"\"\n",
    "        \n",
    "        # Try BERT first\n",
    "        if self.bert_classifier:\n",
    "            try:\n",
    "                result = self.bert_classifier(text)\n",
    "                if isinstance(result[0], list):\n",
    "                    best = max(result[0], key=lambda x: x['score'])\n",
    "                    return {'method': 'BERT', 'result': best}\n",
    "                else:\n",
    "                    return {'method': 'BERT', 'result': result[0]}\n",
    "            except Exception as e:\n",
    "                print(f\"BERT classification failed: {e}\")\n",
    "        \n",
    "        # Use fallback method\n",
    "        fallback_result = self.fallback_classify(text)\n",
    "        return {'method': 'Fallback', 'result': fallback_result}\n",
    "    \n",
    "    def batch_classify(self, texts):\n",
    "        \"\"\"Classify multiple texts.\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        for text in texts:\n",
    "            result = self.classify(text)\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'method': result['method'],\n",
    "                'label': result['result']['label'],\n",
    "                'score': result['result']['score']\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test the robust classifier\n",
    "robust_classifier = RobustGermanClassifier()\n",
    "\n",
    "test_texts = [\n",
    "    \"Das ist wirklich gut und toll!\",\n",
    "    \"Furchtbar schlecht, eine Katastrophe.\",\n",
    "    \"Das geht schon in Ordnung.\",\n",
    "    \"Ich bin sehr zufrieden mit der Qualit√§t.\",\n",
    "    \"Das war entt√§uschend und mangelhaft.\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting Robust Classifier:\")\n",
    "results = robust_classifier.batch_classify(test_texts)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Method: {result['method']}, Label: {result['label']}, Score: {result['score']:.3f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_with_bert(texts, batch_size=4):\n",
    "    \"\"\"Process multiple texts with BERT in batches.\"\"\"\n",
    "    \n",
    "    if not classifier:\n",
    "        print(\"No BERT classifier available.\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            batch_results = classifier(batch)\n",
    "            results.extend(batch_results)\n",
    "            print(f\"Processed batch {i//batch_size + 1}: {len(batch)} texts\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i//batch_size + 1}: {e}\")\n",
    "            # Process individually if batch fails\n",
    "            for text in batch:\n",
    "                try:\n",
    "                    result = classifier(text)\n",
    "                    results.append(result[0] if isinstance(result, list) else result)\n",
    "                except:\n",
    "                    results.append({'label': 'ERROR', 'score': 0.0})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create larger dataset for batch processing\n",
    "large_dataset = [\n",
    "    \"Das Restaurant war ausgezeichnet.\",\n",
    "    \"Der Service war schlecht.\",\n",
    "    \"Das Wetter ist heute sch√∂n.\",\n",
    "    \"Ich bin m√ºde und gestresst.\",\n",
    "    \"Die Musik war wunderbar.\",\n",
    "    \"Das Buch ist langweilig.\",\n",
    "    \"Der Film war spannend.\",\n",
    "    \"Das Essen schmeckt schrecklich.\",\n",
    "    \"Die Reise war entspannend.\",\n",
    "    \"Der Computer ist kaputt.\",\n",
    "    \"Das Meeting war produktiv.\",\n",
    "    \"Die Pr√§sentation war chaotisch.\"\n",
    "]\n",
    "\n",
    "print(f\"Processing {len(large_dataset)} texts with BERT...\")\n",
    "batch_results = batch_process_with_bert(large_dataset, batch_size=3)\n",
    "\n",
    "# Analyze results\n",
    "if batch_results:\n",
    "    print(\"\\nBatch Processing Results:\")\n",
    "    for i, (text, result) in enumerate(zip(large_dataset, batch_results)):\n",
    "        if isinstance(result, list) and result:\n",
    "            best = max(result, key=lambda x: x.get('score', 0))\n",
    "            print(f\"{i+1:2d}. {best['label']:<8} ({best['score']:.3f}) - {text}\")\n",
    "        elif isinstance(result, dict):\n",
    "            print(f\"{i+1:2d}. {result.get('label', 'N/A'):<8} ({result.get('score', 0):.3f}) - {text}\")\n",
    "        else:\n",
    "            print(f\"{i+1:2d}. ERROR    - {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cbf3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bert_model():\n",
    "    \"\"\"Analyze BERT model information and capabilities.\"\"\"\n",
    "    \n",
    "    if not TRANSFORMERS_AVAILABLE:\n",
    "        print(\"Transformers library not available.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Try to load a specific German BERT model for analysis\n",
    "        model_name = \"bert-base-german-cased\"\n",
    "        \n",
    "        print(f\"Analyzing BERT model: {model_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Test tokenization\n",
    "        test_sentence = \"Das ist ein sch√∂ner Tag in Deutschland!\"\n",
    "        tokens = tokenizer.tokenize(test_sentence)\n",
    "        token_ids = tokenizer.encode(test_sentence)\n",
    "        \n",
    "        print(f\"Original text: {test_sentence}\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Token IDs: {token_ids}\")\n",
    "        print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "        \n",
    "        # Test with different German texts\n",
    "        german_examples = [\n",
    "            \"Guten Morgen!\",\n",
    "            \"Wie geht es Ihnen?\",\n",
    "            \"Ich liebe die deutsche Sprache.\",\n",
    "            \"Das Wetter ist heute wunderbar.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTokenization Examples:\")\n",
    "        for example in german_examples:\n",
    "            tokens = tokenizer.tokenize(example)\n",
    "            print(f\"{example:<30} -> {tokens}\")\n",
    "        \n",
    "        # Special tokens\n",
    "        print(f\"\\nSpecial Tokens:\")\n",
    "        print(f\"CLS token: {tokenizer.cls_token}\")\n",
    "        print(f\"SEP token: {tokenizer.sep_token}\")\n",
    "        print(f\"PAD token: {tokenizer.pad_token}\")\n",
    "        print(f\"UNK token: {tokenizer.unk_token}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing model: {e}\")\n",
    "        print(\"This might happen if the model is not available or requires internet connection.\")\n",
    "\n",
    "# Analyze BERT model\n",
    "analyze_bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8829dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bert_predictions(results):\n",
    "    \"\"\"Create simple visualizations of BERT predictions.\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Extract predictions and confidence scores\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    for result in results:\n",
    "        pred = result['prediction']\n",
    "        if isinstance(pred[0], list):\n",
    "            best = max(pred[0], key=lambda x: x['score'])\n",
    "            predictions.append(best['label'])\n",
    "            confidences.append(best['score'])\n",
    "        else:\n",
    "            predictions.append(pred[0]['label'])\n",
    "            confidences.append(pred[0]['score'])\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 1. Prediction distribution\n",
    "    pred_counts = pd.Series(predictions).value_counts()\n",
    "    ax1.bar(pred_counts.index, pred_counts.values, color=['lightblue', 'lightcoral', 'lightgreen'])\n",
    "    ax1.set_title('Distribution of BERT Predictions')\n",
    "    ax1.set_xlabel('Prediction')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, v in enumerate(pred_counts.values):\n",
    "        ax1.text(i, v + 0.1, str(v), ha='center')\n",
    "    \n",
    "    # 2. Confidence scores\n",
    "    ax2.hist(confidences, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2.set_title('Distribution of Confidence Scores')\n",
    "    ax2.set_xlabel('Confidence Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.axvline(x=np.mean(confidences), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(confidences):.3f}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    print(f\"Average confidence: {np.mean(confidences):.3f}\")\n",
    "    print(f\"Min confidence: {np.min(confidences):.3f}\")\n",
    "    print(f\"Max confidence: {np.max(confidences):.3f}\")\n",
    "    \n",
    "    print(\"\\nPrediction breakdown:\")\n",
    "    for label, count in pred_counts.items():\n",
    "        percentage = (count / len(predictions)) * 100\n",
    "        print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize results if we have them\n",
    "if 'test_results' in locals() and test_results:\n",
    "    visualize_bert_predictions(test_results)\n",
    "else:\n",
    "    print(\"No test results available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustGermanClassifier:\n",
    "    \"\"\"Robust German text classifier with fallback methods.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bert_classifier = None\n",
    "        self.fallback_classifier = None\n",
    "        self.setup_classifiers()\n",
    "    \n",
    "    def setup_classifiers(self):\n",
    "        \"\"\"Set up BERT and fallback classifiers.\"\"\"\n",
    "        \n",
    "        # Try to set up BERT classifier\n",
    "        if TRANSFORMERS_AVAILABLE:\n",
    "            try:\n",
    "                self.bert_classifier = pipeline(\"sentiment-analysis\", \n",
    "                                               model=\"oliverguhr/german-sentiment-bert\",\n",
    "                                               return_all_scores=True)\n",
    "                print(\"BERT classifier loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load BERT classifier: {e}\")\n",
    "        \n",
    "        # Set up simple fallback classifier\n",
    "        self.setup_fallback_classifier()\n",
    "    \n",
    "    def setup_fallback_classifier(self):\n",
    "        \"\"\"Set up simple keyword-based fallback classifier.\"\"\"\n",
    "        \n",
    "        self.positive_words = {\n",
    "            'gut', 'toll', 'super', 'fantastisch', 'wunderbar', 'ausgezeichnet',\n",
    "            'brilliant', 'perfekt', 'gro√üartig', 'hervorragend', 'sch√∂n',\n",
    "            'lieben', 'm√∂gen', 'gefallen', 'empfehlen', 'zufrieden'\n",
    "        }\n",
    "        \n",
    "        self.negative_words = {\n",
    "            'schlecht', 'furchtbar', 'schrecklich', 'katastrophal', 'schlimm',\n",
    "            'entt√§uschend', 'unzufrieden', 'hassen', 'entsetzlich', 'mangelhaft',\n",
    "            'schrecklich', 'grauenhaft', 'unertr√§glich', 'minderwertig'\n",
    "        }\n",
    "        \n",
    "        print(\"Fallback classifier set up.\")\n",
    "    \n",
    "    def fallback_classify(self, text):\n",
    "        \"\"\"Simple keyword-based classification.\"\"\"\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        words = text_lower.split()\n",
    "        \n",
    "        positive_score = sum(1 for word in words if word in self.positive_words)\n",
    "        negative_score = sum(1 for word in words if word in self.negative_words)\n",
    "        \n",
    "        if positive_score > negative_score:\n",
    "            return {'label': 'POSITIVE', 'score': 0.6 + 0.1 * positive_score}\n",
    "        elif negative_score > positive_score:\n",
    "            return {'label': 'NEGATIVE', 'score': 0.6 + 0.1 * negative_score}\n",
    "        else:\n",
    "            return {'label': 'NEUTRAL', 'score': 0.5}\n",
    "    \n",
    "    def classify(self, text):\n",
    "        \"\"\"Classify text using BERT or fallback method.\"\"\"\n",
    "        \n",
    "        # Try BERT first\n",
    "        if self.bert_classifier:\n",
    "            try:\n",
    "                result = self.bert_classifier(text)\n",
    "                if isinstance(result[0], list):\n",
    "                    best = max(result[0], key=lambda x: x['score'])\n",
    "                    return {'method': 'BERT', 'result': best}\n",
    "                else:\n",
    "                    return {'method': 'BERT', 'result': result[0]}\n",
    "            except Exception as e:\n",
    "                print(f\"BERT classification failed: {e}\")\n",
    "        \n",
    "        # Use fallback method\n",
    "        fallback_result = self.fallback_classify(text)\n",
    "        return {'method': 'Fallback', 'result': fallback_result}\n",
    "    \n",
    "    def batch_classify(self, texts):\n",
    "        \"\"\"Classify multiple texts.\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        for text in texts:\n",
    "            result = self.classify(text)\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'method': result['method'],\n",
    "                'label': result['result']['label'],\n",
    "                'score': result['result']['score']\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test the robust classifier\n",
    "robust_classifier = RobustGermanClassifier()\n",
    "\n",
    "test_texts = [\n",
    "    \"Das ist wirklich gut und toll!\",\n",
    "    \"Furchtbar schlecht, eine Katastrophe.\",\n",
    "    \"Das geht schon in Ordnung.\",\n",
    "    \"Ich bin sehr zufrieden mit der Qualit√§t.\",\n",
    "    \"Das war entt√§uschend und mangelhaft.\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting Robust Classifier:\")\n",
    "results = robust_classifier.batch_classify(test_texts)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Method: {result['method']}, Label: {result['label']}, Score: {result['score']:.3f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_with_bert(texts, batch_size=4):\n",
    "    \"\"\"Process multiple texts with BERT in batches.\"\"\"\n",
    "    \n",
    "    if not classifier:\n",
    "        print(\"No BERT classifier available.\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            batch_results = classifier(batch)\n",
    "            results.extend(batch_results)\n",
    "            print(f\"Processed batch {i//batch_size + 1}: {len(batch)} texts\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i//batch_size + 1}: {e}\")\n",
    "            # Process individually if batch fails\n",
    "            for text in batch:\n",
    "                try:\n",
    "                    result = classifier(text)\n",
    "                    results.append(result[0] if isinstance(result, list) else result)\n",
    "                except:\n",
    "                    results.append({'label': 'ERROR', 'score': 0.0})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create larger dataset for batch processing\n",
    "large_dataset = [\n",
    "    \"Das Restaurant war ausgezeichnet.\",\n",
    "    \"Der Service war schlecht.\",\n",
    "    \"Das Wetter ist heute sch√∂n.\",\n",
    "    \"Ich bin m√ºde und gestresst.\",\n",
    "    \"Die Musik war wunderbar.\",\n",
    "    \"Das Buch ist langweilig.\",\n",
    "    \"Der Film war spannend.\",\n",
    "    \"Das Essen schmeckt schrecklich.\",\n",
    "    \"Die Reise war entspannend.\",\n",
    "    \"Der Computer ist kaputt.\",\n",
    "    \"Das Meeting war produktiv.\",\n",
    "    \"Die Pr√§sentation war chaotisch.\"\n",
    "]\n",
    "\n",
    "print(f\"Processing {len(large_dataset)} texts with BERT...\")\n",
    "batch_results = batch_process_with_bert(large_dataset, batch_size=3)\n",
    "\n",
    "# Analyze results\n",
    "if batch_results:\n",
    "    print(\"\\nBatch Processing Results:\")\n",
    "    for i, (text, result) in enumerate(zip(large_dataset, batch_results)):\n",
    "        if isinstance(result, list) and result:\n",
    "            best = max(result, key=lambda x: x.get('score', 0))\n",
    "            print(f\"{i+1:2d}. {best['label']:<8} ({best['score']:.3f}) - {text}\")\n",
    "        elif isinstance(result, dict):\n",
    "            print(f\"{i+1:2d}. {result.get('label', 'N/A'):<8} ({result.get('score', 0):.3f}) - {text}\")\n",
    "        else:\n",
    "            print(f\"{i+1:2d}. ERROR    - {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c7f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bert_model():\n",
    "    \"\"\"Analyze BERT model information and capabilities.\"\"\"\n",
    "    \n",
    "    if not TRANSFORMERS_AVAILABLE:\n",
    "        print(\"Transformers library not available.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Try to load a specific German BERT model for analysis\n",
    "        model_name = \"bert-base-german-cased\"\n",
    "        \n",
    "        print(f\"Analyzing BERT model: {model_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Test tokenization\n",
    "        test_sentence = \"Das ist ein sch√∂ner Tag in Deutschland!\"\n",
    "        tokens = tokenizer.tokenize(test_sentence)\n",
    "        token_ids = tokenizer.encode(test_sentence)\n",
    "        \n",
    "        print(f\"Original text: {test_sentence}\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Token IDs: {token_ids}\")\n",
    "        print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "        \n",
    "        # Test with different German texts\n",
    "        german_examples = [\n",
    "            \"Guten Morgen!\",\n",
    "            \"Wie geht es Ihnen?\",\n",
    "            \"Ich liebe die deutsche Sprache.\",\n",
    "            \"Das Wetter ist heute wunderbar.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTokenization Examples:\")\n",
    "        for example in german_examples:\n",
    "            tokens = tokenizer.tokenize(example)\n",
    "            print(f\"{example:<30} -> {tokens}\")\n",
    "        \n",
    "        # Special tokens\n",
    "        print(f\"\\nSpecial Tokens:\")\n",
    "        print(f\"CLS token: {tokenizer.cls_token}\")\n",
    "        print(f\"SEP token: {tokenizer.sep_token}\")\n",
    "        print(f\"PAD token: {tokenizer.pad_token}\")\n",
    "        print(f\"UNK token: {tokenizer.unk_token}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing model: {e}\")\n",
    "        print(\"This might happen if the model is not available or requires internet connection.\")\n",
    "\n",
    "# Analyze BERT model\n",
    "analyze_bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bert_predictions(results):\n",
    "    \"\"\"Create simple visualizations of BERT predictions.\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Extract predictions and confidence scores\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    for result in results:\n",
    "        pred = result['prediction']\n",
    "        if isinstance(pred[0], list):\n",
    "            best = max(pred[0], key=lambda x: x['score'])\n",
    "            predictions.append(best['label'])\n",
    "            confidences.append(best['score'])\n",
    "        else:\n",
    "            predictions.append(pred[0]['label'])\n",
    "            confidences.append(pred[0]['score'])\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 1. Prediction distribution\n",
    "    pred_counts = pd.Series(predictions).value_counts()\n",
    "    ax1.bar(pred_counts.index, pred_counts.values, color=['lightblue', 'lightcoral', 'lightgreen'])\n",
    "    ax1.set_title('Distribution of BERT Predictions')\n",
    "    ax1.set_xlabel('Prediction')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, v in enumerate(pred_counts.values):\n",
    "        ax1.text(i, v + 0.1, str(v), ha='center')\n",
    "    \n",
    "    # 2. Confidence scores\n",
    "    ax2.hist(confidences, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2.set_title('Distribution of Confidence Scores')\n",
    "    ax2.set_xlabel('Confidence Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.axvline(x=np.mean(confidences), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(confidences):.3f}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    print(f\"Average confidence: {np.mean(confidences):.3f}\")\n",
    "    print(f\"Min confidence: {np.min(confidences):.3f}\")\n",
    "    print(f\"Max confidence: {np.max(confidences):.3f}\")\n",
    "    \n",
    "    print(\"\\nPrediction breakdown:\")\n",
    "    for label, count in pred_counts.items():\n",
    "        percentage = (count / len(predictions)) * 100\n",
    "        print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize results if we have them\n",
    "if 'test_results' in locals() and test_results:\n",
    "    visualize_bert_predictions(test_results)\n",
    "else:\n",
    "    print(\"No test results available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec0da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustGermanClassifier:\n",
    "    \"\"\"Robust German text classifier with fallback methods.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bert_classifier = None\n",
    "        self.fallback_classifier = None\n",
    "        self.setup_classifiers()\n",
    "    \n",
    "    def setup_classifiers(self):\n",
    "        \"\"\"Set up BERT and fallback classifiers.\"\"\"\n",
    "        \n",
    "        # Try to set up BERT classifier\n",
    "        if TRANSFORMERS_AVAILABLE:\n",
    "            try:\n",
    "                self.bert_classifier = pipeline(\"sentiment-analysis\", \n",
    "                                               model=\"oliverguhr/german-sentiment-bert\",\n",
    "                                               return_all_scores=True)\n",
    "                print(\"BERT classifier loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load BERT classifier: {e}\")\n",
    "        \n",
    "        # Set up simple fallback classifier\n",
    "        self.setup_fallback_classifier()\n",
    "    \n",
    "    def setup_fallback_classifier(self):\n",
    "        \"\"\"Set up simple keyword-based fallback classifier.\"\"\"\n",
    "        \n",
    "        self.positive_words = {\n",
    "            'gut', 'toll', 'super', 'fantastisch', 'wunderbar', 'ausgezeichnet',\n",
    "            'brilliant', 'perfekt', 'gro√üartig', 'hervorragend', 'sch√∂n',\n",
    "            'lieben', 'm√∂gen', 'gefallen', 'empfehlen', 'zufrieden'\n",
    "        }\n",
    "        \n",
    "        self.negative_words = {\n",
    "            'schlecht', 'furchtbar', 'schrecklich', 'katastrophal', 'schlimm',\n",
    "            'entt√§uschend', 'unzufrieden', 'hassen', 'entsetzlich', 'mangelhaft',\n",
    "            'schrecklich', 'grauenhaft', 'unertr√§glich', 'minderwertig'\n",
    "        }\n",
    "        \n",
    "        print(\"Fallback classifier set up.\")\n",
    "    \n",
    "    def fallback_classify(self, text):\n",
    "        \"\"\"Simple keyword-based classification.\"\"\"\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        words = text_lower.split()\n",
    "        \n",
    "        positive_score = sum(1 for word in words if word in self.positive_words)\n",
    "        negative_score = sum(1 for word in words if word in self.negative_words)\n",
    "        \n",
    "        if positive_score > negative_score:\n",
    "            return {'label': 'POSITIVE', 'score': 0.6 + 0.1 * positive_score}\n",
    "        elif negative_score > positive_score:\n",
    "            return {'label': 'NEGATIVE', 'score': 0.6 + 0.1 * negative_score}\n",
    "        else:\n",
    "            return {'label': 'NEUTRAL', 'score': 0.5}\n",
    "    \n",
    "    def classify(self, text):\n",
    "        \"\"\"Classify text using BERT or fallback method.\"\"\"\n",
    "        \n",
    "        # Try BERT first\n",
    "        if self.bert_classifier:\n",
    "            try:\n",
    "                result = self.bert_classifier(text)\n",
    "                if isinstance(result[0], list):\n",
    "                    best = max(result[0], key=lambda x: x['score'])\n",
    "                    return {'method': 'BERT', 'result': best}\n",
    "                else:\n",
    "                    return {'method': 'BERT', 'result': result[0]}\n",
    "            except Exception as e:\n",
    "                print(f\"BERT classification failed: {e}\")\n",
    "        \n",
    "        # Use fallback method\n",
    "        fallback_result = self.fallback_classify(text)\n",
    "        return {'method': 'Fallback', 'result': fallback_result}\n",
    "    \n",
    "    def batch_classify(self, texts):\n",
    "        \"\"\"Classify multiple texts.\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        for text in texts:\n",
    "            result = self.classify(text)\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'method': result['method'],\n",
    "                'label': result['result']['label'],\n",
    "                'score': result['result']['score']\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test the robust classifier\n",
    "robust_classifier = RobustGermanClassifier()\n",
    "\n",
    "test_texts = [\n",
    "    \"Das ist wirklich gut und toll!\",\n",
    "    \"Furchtbar schlecht, eine Katastrophe.\",\n",
    "    \"Das geht schon in Ordnung.\",\n",
    "    \"Ich bin sehr zufrieden mit der Qualit√§t.\",\n",
    "    \"Das war entt√§uschend und mangelhaft.\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting Robust Classifier:\")\n",
    "results = robust_classifier.batch_classify(test_texts)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Method: {result['method']}, Label: {result['label']}, Score: {result['score']:.3f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf80e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_with_bert(texts, batch_size=4):\n",
    "    \"\"\"Process multiple texts with BERT in batches.\"\"\"\n",
    "    \n",
    "    if not classifier:\n",
    "        print(\"No BERT classifier available.\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            batch_results = classifier(batch)\n",
    "            results.extend(batch_results)\n",
    "            print(f\"Processed batch {i//batch_size + 1}: {len(batch)} texts\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i//batch_size + 1}: {e}\")\n",
    "            # Process individually if batch fails\n",
    "            for text in batch:\n",
    "                try:\n",
    "                    result = classifier(text)\n",
    "                    results.append(result[0] if isinstance(result, list) else result)\n",
    "                except:\n",
    "                    results.append({'label': 'ERROR', 'score': 0.0})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create larger dataset for batch processing\n",
    "large_dataset = [\n",
    "    \"Das Restaurant war ausgezeichnet.\",\n",
    "    \"Der Service war schlecht.\",\n",
    "    \"Das Wetter ist heute sch√∂n.\",\n",
    "    \"Ich bin m√ºde und gestresst.\",\n",
    "    \"Die Musik war wunderbar.\",\n",
    "    \"Das Buch ist langweilig.\",\n",
    "    \"Der Film war spannend.\",\n",
    "    \"Das Essen schmeckt schrecklich.\",\n",
    "    \"Die Reise war entspannend.\",\n",
    "    \"Der Computer ist kaputt.\",\n",
    "    \"Das Meeting war produktiv.\",\n",
    "    \"Die Pr√§sentation war chaotisch.\"\n",
    "]\n",
    "\n",
    "print(f\"Processing {len(large_dataset)} texts with BERT...\")\n",
    "batch_results = batch_process_with_bert(large_dataset, batch_size=3)\n",
    "\n",
    "# Analyze results\n",
    "if batch_results:\n",
    "    print(\"\\nBatch Processing Results:\")\n",
    "    for i, (text, result) in enumerate(zip(large_dataset, batch_results)):\n",
    "        if isinstance(result, list) and result:\n",
    "            best = max(result, key=lambda x: x.get('score', 0))\n",
    "            print(f\"{i+1:2d}. {best['label']:<8} ({best['score']:.3f}) - {text}\")\n",
    "        elif isinstance(result, dict):\n",
    "            print(f\"{i+1:2d}. {result.get('label', 'N/A'):<8} ({result.get('score', 0):.3f}) - {text}\")\n",
    "        else:\n",
    "            print(f\"{i+1:2d}. ERROR    - {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b9227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bert_model():\n",
    "    \"\"\"Analyze BERT model information and capabilities.\"\"\"\n",
    "    \n",
    "    if not TRANSFORMERS_AVAILABLE:\n",
    "        print(\"Transformers library not available.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Try to load a specific German BERT model for analysis\n",
    "        model_name = \"bert-base-german-cased\"\n",
    "        \n",
    "        print(f\"Analyzing BERT model: {model_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Test tokenization\n",
    "        test_sentence = \"Das ist ein sch√∂ner Tag in Deutschland!\"\n",
    "        tokens = tokenizer.tokenize(test_sentence)\n",
    "        token_ids = tokenizer.encode(test_sentence)\n",
    "        \n",
    "        print(f\"Original text: {test_sentence}\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Token IDs: {token_ids}\")\n",
    "        print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "        \n",
    "        # Test with different German texts\n",
    "        german_examples = [\n",
    "            \"Guten Morgen!\",\n",
    "            \"Wie geht es Ihnen?\",\n",
    "            \"Ich liebe die deutsche Sprache.\",\n",
    "            \"Das Wetter ist heute wunderbar.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTokenization Examples:\")\n",
    "        for example in german_examples:\n",
    "            tokens = tokenizer.tokenize(example)\n",
    "            print(f\"{example:<30} -> {tokens}\")\n",
    "        \n",
    "        # Special tokens\n",
    "        print(f\"\\nSpecial Tokens:\")\n",
    "        print(f\"CLS token: {tokenizer.cls_token}\")\n",
    "        print(f\"SEP token: {tokenizer.sep_token}\")\n",
    "        print(f\"PAD token: {tokenizer.pad_token}\")\n",
    "        print(f\"UNK token: {tokenizer.unk_token}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing model: {e}\")\n",
    "        print(\"This might happen if the model is not available or requires internet connection.\")\n",
    "\n",
    "# Analyze BERT model\n",
    "analyze_bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bert_predictions(results):\n",
    "    \"\"\"Create simple visualizations of BERT predictions.\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Extract predictions and confidence scores\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    for result in results:\n",
    "        pred = result['prediction']\n",
    "        if isinstance(pred[0], list):\n",
    "            best = max(pred[0], key=lambda x: x['score'])\n",
    "            predictions.append(best['label'])\n",
    "            confidences.append(best['score'])\n",
    "        else:\n",
    "            predictions.append(pred[0]['label'])\n",
    "            confidences.append(pred[0]['score'])\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 1. Prediction distribution\n",
    "    pred_counts = pd.Series(predictions).value_counts()\n",
    "    ax1.bar(pred_counts.index, pred_counts.values, color=['lightblue', 'lightcoral', 'lightgreen'])\n",
    "    ax1.set_title('Distribution of BERT Predictions')\n",
    "    ax1.set_xlabel('Prediction')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, v in enumerate(pred_counts.values):\n",
    "        ax1.text(i, v + 0.1, str(v), ha='center')\n",
    "    \n",
    "    # 2. Confidence scores\n",
    "    ax2.hist(confidences, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2.set_title('Distribution of Confidence Scores')\n",
    "    ax2.set_xlabel('Confidence Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.axvline(x=np.mean(confidences), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(confidences):.3f}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    print(f\"Average confidence: {np.mean(confidences):.3f}\")\n",
    "    print(f\"Min confidence: {np.min(confidences):.3f}\")\n",
    "    print(f\"Max confidence: {np.max(confidences):.3f}\")\n",
    "    \n",
    "    print(\"\\nPrediction breakdown:\")\n",
    "    for label, count in pred_counts.items():\n",
    "        percentage = (count / len(predictions)) * 100\n",
    "        print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize results if we have them\n",
    "if 'test_results' in locals() and test_results:\n",
    "    visualize_bert_predictions(test_results)\n",
    "else:\n",
    "    print(\"No test results available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea74ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustGermanClassifier:\n",
    "    \"\"\"Robust German text classifier with fallback methods.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bert_classifier = None\n",
    "        self.fallback_classifier = None\n",
    "        self.setup_classifiers()\n",
    "    \n",
    "    def setup_classifiers(self):\n",
    "        \"\"\"Set up BERT and fallback classifiers.\"\"\"\n",
    "        \n",
    "        # Try to set up BERT classifier\n",
    "        if TRANSFORMERS_AVAILABLE:\n",
    "            try:\n",
    "                self.bert_classifier = pipeline(\"sentiment-analysis\", \n",
    "                                               model=\"oliverguhr/german-sentiment-bert\",\n",
    "                                               return_all_scores=True)\n",
    "                print(\"BERT classifier loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load BERT classifier: {e}\")\n",
    "        \n",
    "        # Set up simple fallback classifier\n",
    "        self.setup_fallback_classifier()\n",
    "    \n",
    "    def setup_fallback_classifier(self):\n",
    "        \"\"\"Set up simple keyword-based fallback classifier.\"\"\"\n",
    "        \n",
    "        self.positive_words = {\n",
    "            'gut', 'toll', 'super', 'fantastisch', 'wunderbar', 'ausgezeichnet',\n",
    "            'brilliant', 'perfekt', 'gro√üartig', 'hervorragend', 'sch√∂n',\n",
    "            'lieben', 'm√∂gen', 'gefallen', 'empfehlen', 'zufrieden'\n",
    "        }\n",
    "        \n",
    "        self.negative_words = {\n",
    "            'schlecht', 'furchtbar', 'schrecklich', 'katastrophal', 'schlimm',\n",
    "            'entt√§uschend', 'unzufrieden', 'hassen', 'entsetzlich', 'mangelhaft',\n",
    "            'schrecklich', 'grauenhaft', 'unertr√§glich', 'minderwertig'\n",
    "        }\n",
    "        \n",
    "        print(\"Fallback classifier set up.\")\n",
    "    \n",
    "    def fallback_classify(self, text):\n",
    "        \"\"\"Simple keyword-based classification.\"\"\"\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        words = text_lower.split()\n",
    "        \n",
    "        positive_score = sum(1 for word in words if word in self.positive_words)\n",
    "        negative_score = sum(1 for word in words if word in self.negative_words)\n",
    "        \n",
    "        if positive_score > negative_score:\n",
    "            return {'label': 'POSITIVE', 'score': 0.6 + 0.1 * positive_score}\n",
    "        elif negative_score > positive_score:\n",
    "            return {'label': 'NEGATIVE', 'score': 0.6 + 0.1 * negative_score}\n",
    "        else:\n",
    "            return {'label': 'NEUTRAL', 'score': 0.5}\n",
    "    \n",
    "    def classify(self, text):\n",
    "        \"\"\"Classify text using BERT or fallback method.\"\"\"\n",
    "        \n",
    "        # Try BERT first\n",
    "        if self.bert_classifier:\n",
    "            try:\n",
    "                result = self.bert_classifier(text)\n",
    "                if isinstance(result[0], list):\n",
    "                    best = max(result[0], key=lambda x: x['score'])\n",
    "                    return {'method': 'BERT', 'result': best}\n",
    "                else:\n",
    "                    return {'method': 'BERT', 'result': result[0]}\n",
    "            except Exception as e:\n",
    "                print(f\"BERT classification failed: {e}\")\n",
    "        \n",
    "        # Use fallback method\n",
    "        fallback_result = self.fallback_classify(text)\n",
    "        return {'method': 'Fallback', 'result': fallback_result}\n",
    "    \n",
    "    def batch_classify(self, texts):\n",
    "        \"\"\"Classify multiple texts.\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        for text in texts:\n",
    "            result = self.classify(text)\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'method': result['method'],\n",
    "                'label': result['result']['label'],\n",
    "                'score': result['result']['score']\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test the robust classifier\n",
    "robust_classifier = RobustGermanClassifier()\n",
    "\n",
    "test_texts = [\n",
    "    \"Das ist wirklich gut und toll!\",\n",
    "    \"Furchtbar schlecht, eine Katastrophe.\",\n",
    "    \"Das geht schon in Ordnung.\",\n",
    "    \"Ich bin sehr zufrieden mit der Qualit√§t.\",\n",
    "    \"Das war entt√§uschend und mangelhaft.\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting Robust Classifier:\")\n",
    "results = robust_classifier.batch_classify(test_texts)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Method: {result['method']}, Label: {result['label']}, Score: {result['score']:.3f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1402641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_with_bert(texts, batch_size=4):\n",
    "    \"\"\"Process multiple texts with BERT in batches.\"\"\"\n",
    "    \n",
    "    if not classifier:\n",
    "        print(\"No BERT classifier available.\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            batch_results = classifier(batch)\n",
    "            results.extend(batch_results)\n",
    "            print(f\"Processed batch {i//batch_size + 1}: {len(batch)} texts\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i//batch_size + 1}: {e}\")\n",
    "            # Process individually if batch fails\n",
    "            for text in batch:\n",
    "                try:\n",
    "                    result = classifier(text)\n",
    "                    results.append(result[0] if isinstance(result, list) else result)\n",
    "                except:\n",
    "                    results.append({'label': 'ERROR', 'score': 0.0})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create larger dataset for batch processing\n",
    "large_dataset = [\n",
    "    \"Das Restaurant war ausgezeichnet.\",\n",
    "    \"Der Service war schlecht.\",\n",
    "    \"Das Wetter ist heute sch√∂n.\",\n",
    "    \"Ich bin m√ºde und gestresst.\",\n",
    "    \"Die Musik war wunderbar.\",\n",
    "    \"Das Buch ist langweilig.\",\n",
    "    \"Der Film war spannend.\",\n",
    "    \"Das Essen schmeckt schrecklich.\",\n",
    "    \"Die Reise war entspannend.\",\n",
    "    \"Der Computer ist kaputt.\",\n",
    "    \"Das Meeting war produktiv.\",\n",
    "    \"Die Pr√§sentation war chaotisch.\"\n",
    "]\n",
    "\n",
    "print(f\"Processing {len(large_dataset)} texts with BERT...\")\n",
    "batch_results = batch_process_with_bert(large_dataset, batch_size=3)\n",
    "\n",
    "# Analyze results\n",
    "if batch_results:\n",
    "    print(\"\\nBatch Processing Results:\")\n",
    "    for i, (text, result) in enumerate(zip(large_dataset, batch_results)):\n",
    "        if isinstance(result, list) and result:\n",
    "            best = max(result, key=lambda x: x.get('score', 0))\n",
    "            print(f\"{i+1:2d}. {best['label']:<8} ({best['score']:.3f}) - {text}\")\n",
    "        elif isinstance(result, dict):\n",
    "            print(f\"{i+1:2d}. {result.get('label', 'N/A'):<8} ({result.get('score', 0):.3f}) - {text}\")\n",
    "        else:\n",
    "            print(f\"{i+1:2d}. ERROR    - {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bert_model():\n",
    "    \"\"\"Analyze BERT model information and capabilities.\"\"\"\n",
    "    \n",
    "    if not TRANSFORMERS_AVAILABLE:\n",
    "        print(\"Transformers library not available.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Try to load a specific German BERT model for analysis\n",
    "        model_name = \"bert-base-german-cased\"\n",
    "        \n",
    "        print(f\"Analyzing BERT model: {model_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Test tokenization\n",
    "        test_sentence = \"Das ist ein sch√∂ner Tag in Deutschland!\"\n",
    "        tokens = tokenizer.tokenize(test_sentence)\n",
    "        token_ids = tokenizer.encode(test_sentence)\n",
    "        \n",
    "        print(f\"Original text: {test_sentence}\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Token IDs: {token_ids}\")\n",
    "        print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "        \n",
    "        # Test with different German texts\n",
    "        german_examples = [\n",
    "            \"Guten Morgen!\",\n",
    "            \"Wie geht es Ihnen?\",\n",
    "            \"Ich liebe die deutsche Sprache.\",\n",
    "            \"Das Wetter ist heute wunderbar.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTokenization Examples:\")\n",
    "        for example in german_examples:\n",
    "            tokens = tokenizer.tokenize(example)\n",
    "            print(f\"{example:<30} -> {tokens}\")\n",
    "        \n",
    "        # Special tokens\n",
    "        print(f\"\\nSpecial Tokens:\")\n",
    "        print(f\"CLS token: {tokenizer.cls_token}\")\n",
    "        print(f\"SEP token: {tokenizer.sep_token}\")\n",
    "        print(f\"PAD token: {tokenizer.pad_token}\")\n",
    "        print(f\"UNK token: {tokenizer.unk_token}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing model: {e}\")\n",
    "        print(\"This might happen if the model is not available or requires internet connection.\")\n",
    "\n",
    "# Analyze BERT model\n",
    "analyze_bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a9ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bert_predictions(results):\n",
    "    \"\"\"Create simple visualizations of BERT predictions.\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Extract predictions and confidence scores\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    for result in results:\n",
    "        pred = result['prediction']\n",
    "        if isinstance(pred[0], list):\n",
    "            best = max(pred[0], key=lambda x: x['score'])\n",
    "            predictions.append(best['label'])\n",
    "            confidences.append(best['score'])\n",
    "        else:\n",
    "            predictions.append(pred[0]['label'])\n",
    "            confidences.append(pred[0]['score'])\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 1. Prediction distribution\n",
    "    pred_counts = pd.Series(predictions).value_counts()\n",
    "    ax1.bar(pred_counts.index, pred_counts.values, color=['lightblue', 'lightcoral', 'lightgreen'])\n",
    "    ax1.set_title('Distribution of BERT Predictions')\n",
    "    ax1.set_xlabel('Prediction')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, v in enumerate(pred_counts.values):\n",
    "        ax1.text(i, v + 0.1, str(v), ha='center')\n",
    "    \n",
    "    # 2. Confidence scores\n",
    "    ax2.hist(confidences, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2.set_title('Distribution of Confidence Scores')\n",
    "    ax2.set_xlabel('Confidence Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.axvline(x=np.mean(confidences), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(confidences):.3f}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    print(f\"Average confidence: {np.mean(confidences):.3f}\")\n",
    "    print(f\"Min confidence: {np.min(confidences):.3f}\")\n",
    "    print(f\"Max confidence: {np.max(confidences):.3f}\")\n",
    "    \n",
    "    print(\"\\nPrediction breakdown:\")\n",
    "    for label, count in pred_counts.items():\n",
    "        percentage = (count / len(predictions)) * 100\n",
    "        print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize results if we have them\n",
    "if 'test_results' in locals() and test_results:\n",
    "    visualize_bert_predictions(test_results)\n",
    "else:\n",
    "    print(\"No test results available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustGermanClassifier:\n",
    "    \"\"\"Robust German text classifier with fallback methods.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bert_classifier = None\n",
    "        self.fallback_classifier = None\n",
    "        self.setup_classifiers()\n",
    "    \n",
    "    def setup_classifiers(self):\n",
    "        \"\"\"Set up BERT and fallback classifiers.\"\"\"\n",
    "        \n",
    "        # Try to set up BERT classifier\n",
    "        if TRANSFORMERS_AVAILABLE:\n",
    "            try:\n",
    "                self.bert_classifier = pipeline(\"sentiment-analysis\", \n",
    "                                               model=\"oliverguhr/german-sentiment-bert\",\n",
    "                                               return_all_scores=True)\n",
    "                print(\"BERT classifier loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load BERT classifier: {e}\")\n",
    "        \n",
    "        # Set up simple fallback classifier\n",
    "        self.setup_fallback_classifier()\n",
    "    \n",
    "    def setup_fallback_classifier(self):\n",
    "        \"\"\"Set up simple keyword-based fallback classifier.\"\"\"\n",
    "        \n",
    "        self.positive_words = {\n",
    "            'gut', 'toll', 'super', 'fantastisch', 'wunderbar', 'ausgezeichnet',\n",
    "            'brilliant', 'perfekt', 'gro√üartig', 'hervorragend', 'sch√∂n',\n",
    "            'lieben', 'm√∂gen', 'gefallen', 'empfehlen', 'zufrieden'\n",
    "        }\n",
    "        \n",
    "        self.negative_words = {\n",
    "            'schlecht', 'furchtbar', 'schrecklich', 'katastrophal', 'schlimm',\n",
    "            'entt√§uschend', 'unzufrieden', 'hassen', 'entsetzlich', 'mangelhaft',\n",
    "            'schrecklich', 'grauenhaft', 'unertr√§glich', 'minderwertig'\n",
    "        }\n",
    "        \n",
    "        print(\"Fallback classifier set up.\")\n",
    "    \n",
    "    def fallback_classify(self, text):\n",
    "        \"\"\"Simple keyword-based classification.\"\"\"\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        words = text_lower.split()\n",
    "        \n",
    "        positive_score = sum(1 for word in words if word in self.positive_words)\n",
    "        negative_score = sum(1 for word in words if word in self.negative_words)\n",
    "        \n",
    "        if positive_score > negative_score:\n",
    "            return {'label': 'POSITIVE', 'score': 0.6 + 0.1 * positive_score}\n",
    "        elif negative_score > positive_score:\n",
    "            return {'label': 'NEGATIVE', 'score': 0.6 + 0.1 * negative_score}\n",
    "        else:\n",
    "            return {'label': 'NEUTRAL', 'score': 0.5}\n",
    "    \n",
    "    def classify(self, text):\n",
    "        \"\"\"Classify text using BERT or fallback method.\"\"\"\n",
    "        \n",
    "        # Try BERT first\n",
    "        if self.bert_classifier:\n",
    "            try:\n",
    "                result = self.bert_classifier(text)\n",
    "                if isinstance(result[0], list):\n",
    "                    best = max(result[0], key=lambda x: x['score'])\n",
    "                    return {'method': 'BERT', 'result': best}\n",
    "                else:\n",
    "                    return {'method': 'BERT', 'result': result[0]}\n",
    "            except Exception as e:\n",
    "                print(f\"BERT classification failed: {e}\")\n",
    "        \n",
    "        # Use fallback method\n",
    "        fallback_result = self.fallback_classify(text)\n",
    "        return {'method': 'Fallback', 'result': fallback_result}\n",
    "    \n",
    "    def batch_classify(self, texts):\n",
    "        \"\"\"Classify multiple texts.\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        for text in texts:\n",
    "            result = self.classify(text)\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'method': result['method'],\n",
    "                'label': result['result']['label'],\n",
    "                'score': result['result']['score']\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test the robust classifier\n",
    "robust_classifier = RobustGermanClassifier()\n",
    "\n",
    "test_texts = [\n",
    "    \"Das ist wirklich gut und toll!\",\n",
    "    \"Furchtbar schlecht, eine Katastrophe.\",\n",
    "    \"Das geht schon in Ordnung.\",\n",
    "    \"Ich bin sehr zufrieden mit der Qualit√§t.\",\n",
    "    \"Das war entt√§uschend und mangelhaft.\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting Robust Classifier:\")\n",
    "results = robust_classifier.batch_classify(test_texts)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Method: {result['method']}, Label: {result['label']}, Score: {result['score']:.3f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe31b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_with_bert(texts, batch_size=4):\n",
    "    \"\"\"Process multiple texts with BERT in batches.\"\"\"\n",
    "    \n",
    "    if not classifier:\n",
    "        print(\"No BERT classifier available.\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            batch_results = classifier(batch)\n",
    "            results.extend(batch_results)\n",
    "            print(f\"Processed batch {i//batch_size + 1}: {len(batch)} texts\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i//batch_size + 1}: {e}\")\n",
    "            # Process individually if batch fails\n",
    "            for text in batch:\n",
    "                try:\n",
    "                    result = classifier(text)\n",
    "                    results.append(result[0] if isinstance(result, list) else result)\n",
    "                except:\n",
    "                    results.append({'label': 'ERROR', 'score': 0.0})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create larger dataset for batch processing\n",
    "large_dataset = [\n",
    "    \"Das Restaurant war ausgezeichnet.\",\n",
    "    \"Der Service war schlecht.\",\n",
    "    \"Das Wetter ist heute sch√∂n.\",\n",
    "    \"Ich bin m√ºde und gestresst.\",\n",
    "    \"Die Musik war wunderbar.\",\n",
    "    \"Das Buch ist langweilig.\",\n",
    "    \"Der Film war spannend.\",\n",
    "    \"Das Essen schmeckt schrecklich.\",\n",
    "    \"Die Reise war entspannend.\",\n",
    "    \"Der Computer ist kaputt.\",\n",
    "    \"Das Meeting war produktiv.\",\n",
    "    \"Die Pr√§sentation war chaotisch.\"\n",
    "]\n",
    "\n",
    "print(f\"Processing {len(large_dataset)} texts with BERT...\")\n",
    "batch_results = batch_process_with_bert(large_dataset, batch_size=3)\n",
    "\n",
    "# Analyze results\n",
    "if batch_results:\n",
    "    print(\"\\nBatch Processing Results:\")\n",
    "    for i, (text, result) in enumerate(zip(large_dataset, batch_results)):\n",
    "        if isinstance(result, list) and result:\n",
    "            best = max(result, key=lambda x: x.get('score', 0))\n",
    "            print(f\"{i+1:2d}. {best['label']:<8} ({best['score']:.3f}) - {text}\")\n",
    "        elif isinstance(result, dict):\n",
    "            print(f\"{i+1:2d}. {result.get('label', 'N/A'):<8} ({result.get('score', 0):.3f}) - {text}\")\n",
    "        else:\n",
    "            print(f\"{i+1:2d}. ERROR    - {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d1cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bert_model():\n",
    "    \"\"\"Analyze BERT model information and capabilities.\"\"\"\n",
    "    \n",
    "    if not TRANSFORMERS_AVAILABLE:\n",
    "        print(\"Transformers library not available.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Try to load a specific German BERT model for analysis\n",
    "        model_name = \"bert-base-german-cased\"\n",
    "        \n",
    "        print(f\"Analyzing BERT model: {model_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Test tokenization\n",
    "        test_sentence = \"Das ist ein sch√∂ner Tag in Deutschland!\"\n",
    "        tokens = tokenizer.tokenize(test_sentence)\n",
    "        token_ids = tokenizer.encode(test_sentence)\n",
    "        \n",
    "        print(f\"Original text: {test_sentence}\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Token IDs: {token_ids}\")\n",
    "        print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "        \n",
    "        # Test with different German texts\n",
    "        german_examples = [\n",
    "            \"Guten Morgen!\",\n",
    "            \"Wie geht es Ihnen?\",\n",
    "            \"Ich liebe die deutsche Sprache.\",\n",
    "            \"Das Wetter ist heute wunderbar.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTokenization Examples:\")\n",
    "        for example in german_examples:\n",
    "            tokens = tokenizer.tokenize(example)\n",
    "            print(f\"{example:<30} -> {tokens}\")\n",
    "        \n",
    "        # Special tokens\n",
    "        print(f\"\\nSpecial Tokens:\")\n",
    "        print(f\"CLS token: {tokenizer.cls_token}\")\n",
    "        print(f\"SEP token: {tokenizer.sep_token}\")\n",
    "        print(f\"PAD token: {tokenizer.pad_token}\")\n",
    "        print(f\"UNK token: {tokenizer.unk_token}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing model: {e}\")\n",
    "        print(\"This might happen if the model is not available or requires internet connection.\")\n",
    "\n",
    "# Analyze BERT model\n",
    "analyze_bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a34138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bert_predictions(results):\n",
    "    \"\"\"Create simple visualizations of BERT predictions.\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Extract predictions and confidence scores\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    for result in results:\n",
    "        pred = result['prediction']\n",
    "        if isinstance(pred[0], list):\n",
    "            best = max(pred[0], key=lambda x: x['score'])\n",
    "            predictions.append(best['label'])\n",
    "            confidences.append(best['score'])\n",
    "        else:\n",
    "            predictions.append(pred[0]['label'])\n",
    "            confidences.append(pred[0]['score'])\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 1. Prediction distribution\n",
    "    pred_counts = pd.Series(predictions).value_counts()\n",
    "    ax1.bar(pred_counts.index, pred_counts.values, color=['lightblue', 'lightcoral', 'lightgreen'])\n",
    "    ax1.set_title('Distribution of BERT Predictions')\n",
    "    ax1.set_xlabel('Prediction')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, v in enumerate(pred_counts.values):\n",
    "        ax1.text(i, v + 0.1, str(v), ha='center')\n",
    "    \n",
    "    # 2. Confidence scores\n",
    "    ax2.hist(confidences, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2.set_title('Distribution of Confidence Scores')\n",
    "    ax2.set_xlabel('Confidence Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.axvline(x=np.mean(confidences), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(confidences):.3f}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    print(f\"Average confidence: {np.mean(confidences):.3f}\")\n",
    "    print(f\"Min confidence: {np.min(confidences):.3f}\")\n",
    "    print(f\"Max confidence: {np.max(confidences):.3f}\")\n",
    "    \n",
    "    print(\"\\nPrediction breakdown:\")\n",
    "    for label, count in pred_counts.items():\n",
    "        percentage = (count / len(predictions)) * 100\n",
    "        print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize results if we have them\n",
    "if 'test_results' in locals() and test_results:\n",
    "    visualize_bert_predictions(test_results)\n",
    "else:\n",
    "    print(\"No test results available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737dae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustGermanClassifier:\n",
    "    \"\"\"Robust German text classifier with fallback methods.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bert_classifier = None\n",
    "        self.fallback_classifier = None\n",
    "        self.setup_classifiers()\n",
    "    \n",
    "    def setup_classifiers(self):\n",
    "        \"\"\"Set up BERT and fallback classifiers.\"\"\"\n",
    "        \n",
    "        # Try to set up BERT classifier\n",
    "        if TRANSFORMERS_AVAILABLE:\n",
    "            try:\n",
    "                self.bert_classifier = pipeline(\"sentiment-analysis\", \n",
    "                                               model=\"oliverguhr/german-sentiment-bert\",\n",
    "                                               return_all_scores=True)\n",
    "                print(\"BERT classifier loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load BERT classifier: {e}\")\n",
    "        \n",
    "        # Set up simple fallback classifier\n",
    "        self.setup_fallback_classifier()\n",
    "    \n",
    "    def setup_fallback_classifier(self):\n",
    "        \"\"\"Set up simple keyword-based fallback classifier.\"\"\"\n",
    "        \n",
    "        self.positive_words = {\n",
    "            'gut', 'toll', 'super', 'fantastisch', 'wunderbar', 'ausgezeichnet',\n",
    "            'brilliant', 'perfekt', 'gro√üartig', 'hervorragend', 'sch√∂n',\n",
    "            'lieben', 'm√∂gen', 'gefallen', 'empfehlen', 'zufrieden'\n",
    "        }\n",
    "        \n",
    "        self.negative_words = {\n",
    "            'schlecht', 'furchtbar', 'schrecklich', 'katastrophal', 'schlimm',\n",
    "            'entt√§uschend', 'unzufrieden', 'hassen', 'entsetzlich', 'mangelhaft',\n",
    "            'schrecklich', 'grauenhaft', 'unertr√§glich', 'minderwertig'\n",
    "        }\n",
    "        \n",
    "        print(\"Fallback classifier set up.\")\n",
    "    \n",
    "    def fallback_classify(self, text):\n",
    "        \"\"\"Simple keyword-based classification.\"\"\"\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        words = text_lower.split()\n",
    "        \n",
    "        positive_score = sum(1 for word in words if word in self.positive_words)\n",
    "        negative_score = sum(1 for word in words if word in self.negative_words)\n",
    "        \n",
    "        if positive_score > negative_score:\n",
    "            return {'label': 'POSITIVE', 'score': 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
