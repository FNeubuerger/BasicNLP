{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615077fb",
   "metadata": {},
   "source": [
    "# Topic 7: Speech Recognition - SOLUTIONS\n",
    "\n",
    "Complete solutions for Speech Recognition exercises using various approaches and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import speech processing libraries\n",
    "try:\n",
    "    import soundfile as sf\n",
    "    import librosa\n",
    "    AUDIO_LIBS_AVAILABLE = True\n",
    "    print(\"Audio processing libraries available!\")\n",
    "except ImportError:\n",
    "    print(\"Audio libraries not available. Please install: pip install soundfile librosa\")\n",
    "    AUDIO_LIBS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    import torch\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "    print(\"Transformers library available for ASR!\")\n",
    "except ImportError:\n",
    "    print(\"Transformers not available. Please install: pip install transformers torch\")\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import speech_recognition as sr\n",
    "    SPEECH_RECOGNITION_AVAILABLE = True\n",
    "    print(\"SpeechRecognition library available!\")\n",
    "except ImportError:\n",
    "    print(\"SpeechRecognition not available. Please install: pip install SpeechRecognition\")\n",
    "    SPEECH_RECOGNITION_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca526b",
   "metadata": {},
   "source": [
    "## Solution 1: Audio File Setup and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26177ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_audio_environment():\n",
    "    \"\"\"Set up directories and create sample audio files.\"\"\"\n",
    "    \n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "    AUDIO_DIR = PROJECT_ROOT / 'data' / 'audio'\n",
    "    AUDIO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"Setting up audio environment...\")\n",
    "    print(f\"Audio directory: {AUDIO_DIR}\")\n",
    "    \n",
    "    return AUDIO_DIR\n",
    "\n",
    "def create_sample_audio(audio_dir, filename=\"example_de.wav\", duration=2.0):\n",
    "    \"\"\"Create a sample audio file with synthetic speech-like sounds.\"\"\"\n",
    "    \n",
    "    if not AUDIO_LIBS_AVAILABLE:\n",
    "        print(\"Audio libraries not available for creating sample audio.\")\n",
    "        return None\n",
    "    \n",
    "    sample_path = audio_dir / filename\n",
    "    \n",
    "    if sample_path.exists():\n",
    "        print(f\"Audio file already exists: {sample_path}\")\n",
    "        return sample_path\n",
    "    \n",
    "    # Create synthetic audio that resembles speech patterns\n",
    "    sr = 16000  # Standard sample rate for speech\n",
    "    t = np.linspace(0, duration, int(sr * duration), False)\n",
    "    \n",
    "    # Create a more complex waveform that resembles speech\n",
    "    # Mix of different frequencies to simulate formants\n",
    "    f1, f2, f3 = 800, 1200, 2400  # Typical formant frequencies\n",
    "    \n",
    "    signal = (0.3 * np.sin(2 * np.pi * f1 * t) * np.exp(-2 * t) +\n",
    "              0.2 * np.sin(2 * np.pi * f2 * t) * np.exp(-1.5 * t) +\n",
    "              0.1 * np.sin(2 * np.pi * f3 * t) * np.exp(-1 * t))\n",
    "    \n",
    "    # Add some noise to make it more realistic\n",
    "    noise = 0.05 * np.random.normal(0, 1, len(signal))\n",
    "    signal += noise\n",
    "    \n",
    "    # Normalize\n",
    "    signal = signal / np.max(np.abs(signal)) * 0.8\n",
    "    \n",
    "    # Save audio file\n",
    "    sf.write(str(sample_path), signal, sr)\n",
    "    print(f\"Created sample audio: {sample_path}\")\n",
    "    \n",
    "    return sample_path\n",
    "\n",
    "def analyze_audio_file(audio_path):\n",
    "    \"\"\"Analyze an audio file and display its properties.\"\"\"\n",
    "    \n",
    "    if not AUDIO_LIBS_AVAILABLE or not audio_path or not audio_path.exists():\n",
    "        print(\"Cannot analyze audio file.\")\n",
    "        return None\n",
    "    \n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(str(audio_path), sr=None)\n",
    "    \n",
    "    print(f\"Audio Analysis for: {audio_path.name}\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Duration: {len(audio) / sr:.2f} seconds\")\n",
    "    print(f\"Sample rate: {sr} Hz\")\n",
    "    print(f\"Channels: 1 (mono)\")\n",
    "    print(f\"Total samples: {len(audio)}\")\n",
    "    print(f\"Audio range: [{audio.min():.3f}, {audio.max():.3f}]\")\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot 1: Waveform\n",
    "    plt.subplot(3, 1, 1)\n",
    "    time = np.linspace(0, len(audio) / sr, len(audio))\n",
    "    plt.plot(time, audio, linewidth=0.5)\n",
    "    plt.title('Audio Waveform')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Spectrogram\n",
    "    plt.subplot(3, 1, 2)\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz')\n",
    "    plt.title('Spectrogram')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    \n",
    "    # Plot 3: MFCC features\n",
    "    plt.subplot(3, 1, 3)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    librosa.display.specshow(mfccs, sr=sr, x_axis='time')\n",
    "    plt.title('MFCC Features')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'audio': audio,\n",
    "        'sample_rate': sr,\n",
    "        'duration': len(audio) / sr,\n",
    "        'mfccs': mfccs\n",
    "    }\n",
    "\n",
    "# Set up audio environment\n",
    "audio_dir = setup_audio_environment()\n",
    "sample_audio = create_sample_audio(audio_dir)\n",
    "\n",
    "if sample_audio:\n",
    "    audio_analysis = analyze_audio_file(sample_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360251c8",
   "metadata": {},
   "source": [
    "## Solution 2: Speech Recognition with Transformers (Wav2Vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6635a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_wav2vec2_model(language='german'):\n",
    "    \"\"\"Set up Wav2Vec2 model for speech recognition.\"\"\"\n",
    "    \n",
    "    if not TRANSFORMERS_AVAILABLE:\n",
    "        print(\"Transformers library not available.\")\n",
    "        return None\n",
    "    \n",
    "    # German Wav2Vec2 models\n",
    "    german_models = {\n",
    "        'wav2vec2-large-xlsr-53-german': 'jonatasgrosman/wav2vec2-large-xlsr-53-german',\n",
    "        'wav2vec2-base-german': 'maxidl/wav2vec2-large-xlsr-german'\n",
    "    }\n",
    "    \n",
    "    # English models as fallback\n",
    "    english_models = {\n",
    "        'wav2vec2-base-960h': 'facebook/wav2vec2-base-960h',\n",
    "        'wav2vec2-large-960h': 'facebook/wav2vec2-large-960h-lv60-self'\n",
    "    }\n",
    "    \n",
    "    model_name = german_models['wav2vec2-large-xlsr-53-german']\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading {language} ASR model: {model_name}\")\n",
    "        asr_pipeline = pipeline(\n",
    "            'automatic-speech-recognition',\n",
    "            model=model_name,\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        print(\"‚úì Model loaded successfully!\")\n",
    "        return asr_pipeline\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        print(\"Trying fallback English model...\")\n",
    "        \n",
    "        try:\n",
    "            model_name = english_models['wav2vec2-base-960h']\n",
    "            asr_pipeline = pipeline(\n",
    "                'automatic-speech-recognition',\n",
    "                model=model_name,\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "            print(\"‚úì Fallback English model loaded!\")\n",
    "            return asr_pipeline\n",
    "        except Exception as e2:\n",
    "            print(f\"Could not load any model: {e2}\")\n",
    "            return None\n",
    "\n",
    "def transcribe_audio_transformers(asr_pipeline, audio_path):\n",
    "    \"\"\"Transcribe audio using Transformers pipeline.\"\"\"\n",
    "    \n",
    "    if not asr_pipeline or not audio_path or not audio_path.exists():\n",
    "        print(\"Cannot transcribe: missing model or audio file.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"Transcribing: {audio_path.name}\")\n",
    "        \n",
    "        # Load and preprocess audio\n",
    "        audio, sr = librosa.load(str(audio_path), sr=16000)  # Wav2Vec2 expects 16kHz\n",
    "        \n",
    "        # Transcribe\n",
    "        result = asr_pipeline(audio)\n",
    "        \n",
    "        transcription = result['text'] if isinstance(result, dict) else str(result)\n",
    "        confidence = result.get('score', 'N/A') if isinstance(result, dict) else 'N/A'\n",
    "        \n",
    "        print(\"Transcription Results:\")\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"Text: {transcription}\")\n",
    "        print(f\"Confidence: {confidence}\")\n",
    "        \n",
    "        return {\n",
    "            'text': transcription,\n",
    "            'confidence': confidence,\n",
    "            'model': 'Wav2Vec2'\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Transcription error: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_test_speech_samples(audio_dir):\n",
    "    \"\"\"Create different types of test audio samples.\"\"\"\n",
    "    \n",
    "    if not AUDIO_LIBS_AVAILABLE:\n",
    "        print(\"Cannot create test samples without audio libraries.\")\n",
    "        return []\n",
    "    \n",
    "    samples = []\n",
    "    sr = 16000\n",
    "    \n",
    "    # Sample 1: Simple tone sequence (numbers)\n",
    "    print(\"Creating test sample 1: Number sequence\")\n",
    "    t1 = np.linspace(0, 3, sr * 3, False)\n",
    "    # Simulate counting \"eins, zwei, drei\" with different tones\n",
    "    signal1 = (0.5 * np.sin(2 * np.pi * 440 * t1[:sr]) +      # \"eins\"\n",
    "               0.5 * np.sin(2 * np.pi * 523 * t1[sr:2*sr]) +   # \"zwei\"  \n",
    "               0.5 * np.sin(2 * np.pi * 659 * t1[2*sr:]))      # \"drei\"\n",
    "    \n",
    "    path1 = audio_dir / \"test_numbers.wav\"\n",
    "    sf.write(str(path1), signal1 * 0.7, sr)\n",
    "    samples.append(path1)\n",
    "    \n",
    "    # Sample 2: White noise (should not transcribe well)\n",
    "    print(\"Creating test sample 2: White noise\")\n",
    "    noise = np.random.normal(0, 0.1, sr * 2)\n",
    "    path2 = audio_dir / \"test_noise.wav\"\n",
    "    sf.write(str(path2), noise, sr)\n",
    "    samples.append(path2)\n",
    "    \n",
    "    # Sample 3: Mixed frequency (simulated greeting)\n",
    "    print(\"Creating test sample 3: Greeting simulation\")\n",
    "    t3 = np.linspace(0, 2, sr * 2, False)\n",
    "    greeting = (0.3 * np.sin(2 * np.pi * 200 * t3) * np.exp(-0.5 * t3) +\n",
    "                0.2 * np.sin(2 * np.pi * 800 * t3) * (1 + 0.5 * np.sin(2 * np.pi * 3 * t3)))\n",
    "    \n",
    "    path3 = audio_dir / \"test_greeting.wav\"\n",
    "    sf.write(str(path3), greeting * 0.6, sr)\n",
    "    samples.append(path3)\n",
    "    \n",
    "    print(f\"Created {len(samples)} test samples\")\n",
    "    return samples\n",
    "\n",
    "# Set up Wav2Vec2 model\n",
    "print(\"Setting up Speech Recognition with Transformers:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "asr_model = setup_wav2vec2_model()\n",
    "\n",
    "if asr_model and sample_audio:\n",
    "    # Transcribe the original sample\n",
    "    result1 = transcribe_audio_transformers(asr_model, sample_audio)\n",
    "    \n",
    "    # Create and test additional samples\n",
    "    test_samples = create_test_speech_samples(audio_dir)\n",
    "    \n",
    "    print(\"\\nTesting multiple audio samples:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for i, test_file in enumerate(test_samples, 1):\n",
    "        print(f\"\\nTest {i}: {test_file.name}\")\n",
    "        result = transcribe_audio_transformers(asr_model, test_file)\n",
    "else:\n",
    "    print(\"Wav2Vec2 transcription not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2096dba3",
   "metadata": {},
   "source": [
    "## Solution 3: Speech Recognition with SpeechRecognition Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afddb5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_speech_recognition():\n",
    "    \"\"\"Set up SpeechRecognition with different engines.\"\"\"\n",
    "    \n",
    "    if not SPEECH_RECOGNITION_AVAILABLE:\n",
    "        print(\"SpeechRecognition library not available.\")\n",
    "        return None\n",
    "    \n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    # Test microphone availability\n",
    "    try:\n",
    "        mic_list = sr.Microphone.list_microphone_names()\n",
    "        print(f\"Available microphones: {len(mic_list)}\")\n",
    "        for i, name in enumerate(mic_list[:3]):  # Show first 3\n",
    "            print(f\"  {i}: {name}\")\n",
    "    except:\n",
    "        print(\"No microphones detected or microphone access unavailable.\")\n",
    "    \n",
    "    # Configure recognizer\n",
    "    recognizer.energy_threshold = 300\n",
    "    recognizer.dynamic_energy_threshold = True\n",
    "    recognizer.pause_threshold = 0.8\n",
    "    \n",
    "    print(\"SpeechRecognition setup complete!\")\n",
    "    return recognizer\n",
    "\n",
    "def transcribe_with_google(recognizer, audio_path, language='de-DE'):\n",
    "    \"\"\"Transcribe audio using Google Speech Recognition.\"\"\"\n",
    "    \n",
    "    if not recognizer or not audio_path.exists():\n",
    "        print(\"Cannot transcribe with Google API.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load audio file\n",
    "        with sr.AudioFile(str(audio_path)) as source:\n",
    "            # Adjust for ambient noise\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            # Record the audio\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        print(f\"Transcribing with Google API (language: {language})...\")\n",
    "        \n",
    "        # Try German first\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data, language=language)\n",
    "            print(f\"‚úì German transcription: {text}\")\n",
    "            return {'text': text, 'language': language, 'engine': 'Google'}\n",
    "        except:\n",
    "            # Fallback to English\n",
    "            text = recognizer.recognize_google(audio_data, language='en-US')\n",
    "            print(f\"‚úì English fallback transcription: {text}\")\n",
    "            return {'text': text, 'language': 'en-US', 'engine': 'Google'}\n",
    "    \n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand the audio\")\n",
    "        return {'text': '[UNRECOGNIZED]', 'language': language, 'engine': 'Google'}\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Google transcription: {e}\")\n",
    "        return None\n",
    "\n",
    "def transcribe_with_sphinx(recognizer, audio_path):\n",
    "    \"\"\"Transcribe audio using CMU Sphinx (offline).\"\"\"\n",
    "    \n",
    "    if not recognizer or not audio_path.exists():\n",
    "        print(\"Cannot transcribe with Sphinx.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with sr.AudioFile(str(audio_path)) as source:\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            audio_data = recognizer.record(source)\n",
    "        \n",
    "        print(\"Transcribing with CMU Sphinx (offline)...\")\n",
    "        text = recognizer.recognize_sphinx(audio_data)\n",
    "        print(f\"‚úì Sphinx transcription: {text}\")\n",
    "        return {'text': text, 'language': 'en-US', 'engine': 'Sphinx'}\n",
    "    \n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Sphinx could not understand the audio\")\n",
    "        return {'text': '[UNRECOGNIZED]', 'language': 'en-US', 'engine': 'Sphinx'}\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Sphinx error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Sphinx transcription: {e}\")\n",
    "        return None\n",
    "\n",
    "def compare_recognition_engines(recognizer, audio_files):\n",
    "    \"\"\"Compare different speech recognition engines.\"\"\"\n",
    "    \n",
    "    if not recognizer or not audio_files:\n",
    "        print(\"Cannot compare engines.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Comparing Speech Recognition Engines:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for audio_file in audio_files:\n",
    "        if not audio_file.exists():\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nTesting: {audio_file.name}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        file_results = {'file': audio_file.name, 'results': []}\n",
    "        \n",
    "        # Google API (German)\n",
    "        google_de = transcribe_with_google(recognizer, audio_file, 'de-DE')\n",
    "        if google_de:\n",
    "            file_results['results'].append(google_de)\n",
    "        \n",
    "        # Google API (English)\n",
    "        google_en = transcribe_with_google(recognizer, audio_file, 'en-US')\n",
    "        if google_en:\n",
    "            file_results['results'].append(google_en)\n",
    "        \n",
    "        # Sphinx (offline)\n",
    "        sphinx_result = transcribe_with_sphinx(recognizer, audio_file)\n",
    "        if sphinx_result:\n",
    "            file_results['results'].append(sphinx_result)\n",
    "        \n",
    "        results.append(file_results)\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"COMPARISON SUMMARY:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for file_result in results:\n",
    "        print(f\"\\nFile: {file_result['file']}\")\n",
    "        for result in file_result['results']:\n",
    "            engine = result['engine']\n",
    "            lang = result.get('language', 'N/A')\n",
    "            text = result['text'][:50] + '...' if len(result['text']) > 50 else result['text']\n",
    "            print(f\"  {engine} ({lang}): {text}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Set up SpeechRecognition\n",
    "print(\"Setting up SpeechRecognition Library:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sr_recognizer = setup_speech_recognition()\n",
    "\n",
    "if sr_recognizer and sample_audio:\n",
    "    # Test with original sample\n",
    "    print(\"\\nTesting original sample:\")\n",
    "    google_result = transcribe_with_google(sr_recognizer, sample_audio)\n",
    "    sphinx_result = transcribe_with_sphinx(sr_recognizer, sample_audio)\n",
    "    \n",
    "    # Compare engines on all test files\n",
    "    all_files = [sample_audio]\n",
    "    if 'test_samples' in locals():\n",
    "        all_files.extend(test_samples)\n",
    "    \n",
    "    comparison_results = compare_recognition_engines(sr_recognizer, all_files)\n",
    "else:\n",
    "    print(\"SpeechRecognition testing not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be06157",
   "metadata": {},
   "source": [
    "## Solution 4: Advanced Audio Processing and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ffbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_advanced_features(audio_path):\n",
    "    \"\"\"Extract advanced audio features for speech analysis.\"\"\"\n",
    "    \n",
    "    if not AUDIO_LIBS_AVAILABLE or not audio_path.exists():\n",
    "        print(\"Cannot extract features.\")\n",
    "        return None\n",
    "    \n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(str(audio_path), sr=16000)\n",
    "    \n",
    "    print(f\"Extracting features from: {audio_path.name}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # 1. MFCC (Mel-frequency cepstral coefficients)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    features['mfcc'] = {\n",
    "        'values': mfccs,\n",
    "        'mean': np.mean(mfccs, axis=1),\n",
    "        'std': np.std(mfccs, axis=1)\n",
    "    }\n",
    "    \n",
    "    # 2. Spectral features\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)[0]\n",
    "    \n",
    "    features['spectral'] = {\n",
    "        'centroid_mean': np.mean(spectral_centroids),\n",
    "        'centroid_std': np.std(spectral_centroids),\n",
    "        'rolloff_mean': np.mean(spectral_rolloff),\n",
    "        'rolloff_std': np.std(spectral_rolloff),\n",
    "        'bandwidth_mean': np.mean(spectral_bandwidth),\n",
    "        'bandwidth_std': np.std(spectral_bandwidth)\n",
    "    }\n",
    "    \n",
    "    # 3. Zero crossing rate\n",
    "    zcr = librosa.feature.zero_crossing_rate(audio)[0]\n",
    "    features['zcr'] = {\n",
    "        'mean': np.mean(zcr),\n",
    "        'std': np.std(zcr)\n",
    "    }\n",
    "    \n",
    "    # 4. Chroma features\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "    features['chroma'] = {\n",
    "        'values': chroma,\n",
    "        'mean': np.mean(chroma, axis=1)\n",
    "    }\n",
    "    \n",
    "    # 5. Tempo and beat\n",
    "    try:\n",
    "        tempo, beats = librosa.beat.beat_track(y=audio, sr=sr)\n",
    "        features['tempo'] = {\n",
    "            'bpm': tempo,\n",
    "            'n_beats': len(beats)\n",
    "        }\n",
    "    except:\n",
    "        features['tempo'] = {'bpm': 0, 'n_beats': 0}\n",
    "    \n",
    "    # 6. Energy and RMS\n",
    "    rms = librosa.feature.rms(y=audio)[0]\n",
    "    features['energy'] = {\n",
    "        'rms_mean': np.mean(rms),\n",
    "        'rms_std': np.std(rms),\n",
    "        'total_energy': np.sum(audio ** 2)\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"MFCC coefficients: {mfccs.shape}\")\n",
    "    print(f\"Spectral centroid (avg): {features['spectral']['centroid_mean']:.2f} Hz\")\n",
    "    print(f\"Zero crossing rate (avg): {features['zcr']['mean']:.4f}\")\n",
    "    print(f\"Tempo: {features['tempo']['bpm']:.1f} BPM\")\n",
    "    print(f\"RMS energy (avg): {features['energy']['rms_mean']:.4f}\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "def visualize_speech_features(audio_path, features):\n",
    "    \"\"\"Create comprehensive visualization of speech features.\"\"\"\n",
    "    \n",
    "    if not features or not AUDIO_LIBS_AVAILABLE:\n",
    "        print(\"Cannot create visualizations.\")\n",
    "        return\n",
    "    \n",
    "    # Load audio for time axis\n",
    "    audio, sr = librosa.load(str(audio_path), sr=16000)\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Waveform and RMS Energy\n",
    "    plt.subplot(4, 2, 1)\n",
    "    time = np.linspace(0, len(audio) / sr, len(audio))\n",
    "    plt.plot(time, audio, alpha=0.6, label='Waveform')\n",
    "    rms = librosa.feature.rms(y=audio)[0]\n",
    "    times_rms = librosa.frames_to_time(range(len(rms)), sr=sr)\n",
    "    plt.plot(times_rms, rms * 5, 'r-', linewidth=2, label='RMS Energy (x5)')\n",
    "    plt.title('Waveform and RMS Energy')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: MFCC\n",
    "    plt.subplot(4, 2, 2)\n",
    "    librosa.display.specshow(features['mfcc']['values'], sr=sr, x_axis='time')\n",
    "    plt.title('MFCC Features')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Plot 3: Spectral Centroid and Rolloff\n",
    "    plt.subplot(4, 2, 3)\n",
    "    cent = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]\n",
    "    times = librosa.frames_to_time(range(len(cent)), sr=sr)\n",
    "    plt.plot(times, cent, label='Spectral Centroid')\n",
    "    plt.plot(times, rolloff, label='Spectral Rolloff', alpha=0.7)\n",
    "    plt.title('Spectral Features')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Hz')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Zero Crossing Rate\n",
    "    plt.subplot(4, 2, 4)\n",
    "    zcr = librosa.feature.zero_crossing_rate(audio)[0]\n",
    "    times_zcr = librosa.frames_to_time(range(len(zcr)), sr=sr)\n",
    "    plt.plot(times_zcr, zcr)\n",
    "    plt.title('Zero Crossing Rate')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('ZCR')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Chroma Features\n",
    "    plt.subplot(4, 2, 5)\n",
    "    librosa.display.specshow(features['chroma']['values'], sr=sr, x_axis='time', y_axis='chroma')\n",
    "    plt.title('Chroma Features')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Plot 6: Mel Spectrogram\n",
    "    plt.subplot(4, 2, 6)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    librosa.display.specshow(mel_spec_db, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.title('Mel Spectrogram')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    \n",
    "    # Plot 7: Feature Summary (bar chart)\n",
    "    plt.subplot(4, 2, 7)\n",
    "    feature_names = ['MFCC_mean', 'Spectral_Cent', 'ZCR_mean', 'RMS_mean', 'Tempo']\n",
    "    feature_values = [\n",
    "        np.mean(features['mfcc']['mean']),\n",
    "        features['spectral']['centroid_mean'] / 1000,  # Scale to kHz\n",
    "        features['zcr']['mean'] * 100,  # Scale for visibility\n",
    "        features['energy']['rms_mean'] * 100,  # Scale for visibility\n",
    "        features['tempo']['bpm'] / 100  # Scale for visibility\n",
    "    ]\n",
    "    \n",
    "    bars = plt.bar(feature_names, feature_values, color='skyblue')\n",
    "    plt.title('Feature Summary (Scaled)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Scaled Values')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, feature_values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{val:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 8: MFCC Statistics\n",
    "    plt.subplot(4, 2, 8)\n",
    "    mfcc_mean = features['mfcc']['mean']\n",
    "    mfcc_std = features['mfcc']['std']\n",
    "    coeffs = range(len(mfcc_mean))\n",
    "    \n",
    "    plt.errorbar(coeffs, mfcc_mean, yerr=mfcc_std, marker='o', capsize=5)\n",
    "    plt.title('MFCC Coefficients (Mean ¬± Std)')\n",
    "    plt.xlabel('MFCC Coefficient')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def speech_quality_assessment(features):\n",
    "    \"\"\"Assess speech quality based on extracted features.\"\"\"\n",
    "    \n",
    "    print(\"\\nSpeech Quality Assessment:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Simple quality metrics based on features\n",
    "    quality_score = 0\n",
    "    assessments = []\n",
    "    \n",
    "    # 1. Energy level assessment\n",
    "    rms_mean = features['energy']['rms_mean']\n",
    "    if rms_mean > 0.01:\n",
    "        quality_score += 20\n",
    "        assessments.append(\"‚úì Good energy level\")\n",
    "    else:\n",
    "        assessments.append(\"‚ö† Low energy level\")\n",
    "    \n",
    "    # 2. Spectral richness\n",
    "    bandwidth_mean = features['spectral']['bandwidth_mean']\n",
    "    if bandwidth_mean > 1000:\n",
    "        quality_score += 20\n",
    "        assessments.append(\"‚úì Good spectral bandwidth\")\n",
    "    else:\n",
    "        assessments.append(\"‚ö† Limited spectral content\")\n",
    "    \n",
    "    # 3. Voice activity (based on ZCR)\n",
    "    zcr_mean = features['zcr']['mean']\n",
    "    if 0.01 < zcr_mean < 0.3:\n",
    "        quality_score += 20\n",
    "        assessments.append(\"‚úì Appropriate voice activity\")\n",
    "    else:\n",
    "        assessments.append(\"‚ö† Unusual voice activity pattern\")\n",
    "    \n",
    "    # 4. Consistency (based on standard deviations)\n",
    "    mfcc_consistency = np.mean(features['mfcc']['std'])\n",
    "    if mfcc_consistency < 2.0:\n",
    "        quality_score += 20\n",
    "        assessments.append(\"‚úì Consistent spectral features\")\n",
    "    else:\n",
    "        assessments.append(\"‚ö† High spectral variability\")\n",
    "    \n",
    "    # 5. Tempo assessment\n",
    "    tempo = features['tempo']['bpm']\n",
    "    if 60 < tempo < 180:\n",
    "        quality_score += 20\n",
    "        assessments.append(\"‚úì Reasonable speech tempo\")\n",
    "    else:\n",
    "        assessments.append(\"‚ö† Unusual tempo detected\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    print(f\"Overall Quality Score: {quality_score}/100\")\n",
    "    print()\n",
    "    \n",
    "    for assessment in assessments:\n",
    "        print(assessment)\n",
    "    \n",
    "    if quality_score >= 80:\n",
    "        print(\"\\nüéâ High quality speech signal!\")\n",
    "    elif quality_score >= 60:\n",
    "        print(\"\\nüëç Moderate quality speech signal\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Low quality speech signal - consider re-recording\")\n",
    "    \n",
    "    return quality_score, assessments\n",
    "\n",
    "# Extract and analyze features for all available audio files\n",
    "print(\"Advanced Audio Feature Extraction:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if sample_audio and AUDIO_LIBS_AVAILABLE:\n",
    "    # Extract features from main sample\n",
    "    main_features = extract_advanced_features(sample_audio)\n",
    "    \n",
    "    if main_features:\n",
    "        visualize_speech_features(sample_audio, main_features)\n",
    "        quality_score, assessments = speech_quality_assessment(main_features)\n",
    "        \n",
    "        # Test additional samples if available\n",
    "        if 'test_samples' in locals() and test_samples:\n",
    "            print(f\"\\nTesting {len(test_samples)} additional samples:\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            for i, test_file in enumerate(test_samples[:2], 1):  # Limit to first 2\n",
    "                print(f\"\\nSample {i}: {test_file.name}\")\n",
    "                features = extract_advanced_features(test_file)\n",
    "                if features:\n",
    "                    score, _ = speech_quality_assessment(features)\n",
    "else:\n",
    "    print(\"Feature extraction not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06026c",
   "metadata": {},
   "source": [
    "## Solution 5: Real-time Speech Recognition (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e145f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_realtime_recognition():\n",
    "    \"\"\"Set up real-time speech recognition from microphone.\"\"\"\n",
    "    \n",
    "    if not SPEECH_RECOGNITION_AVAILABLE:\n",
    "        print(\"SpeechRecognition library required for real-time recognition.\")\n",
    "        return None\n",
    "    \n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    # Configure for real-time\n",
    "    recognizer.energy_threshold = 4000\n",
    "    recognizer.dynamic_energy_threshold = True\n",
    "    recognizer.pause_threshold = 0.5\n",
    "    recognizer.phrase_threshold = 0.3\n",
    "    \n",
    "    return recognizer\n",
    "\n",
    "def test_microphone_setup(recognizer):\n",
    "    \"\"\"Test microphone setup and ambient noise calibration.\"\"\"\n",
    "    \n",
    "    if not recognizer:\n",
    "        print(\"No recognizer available.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(\"Testing microphone setup...\")\n",
    "        \n",
    "        # List available microphones\n",
    "        mic_list = sr.Microphone.list_microphone_names()\n",
    "        print(f\"Found {len(mic_list)} microphone(s):\")\n",
    "        for i, name in enumerate(mic_list[:5]):  # Show first 5\n",
    "            print(f\"  {i}: {name}\")\n",
    "        \n",
    "        # Test with default microphone\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"\\nCalibrating for ambient noise... (please be quiet)\")\n",
    "            recognizer.adjust_for_ambient_noise(source, duration=2)\n",
    "            print(f\"‚úì Energy threshold set to: {recognizer.energy_threshold}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Microphone test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def simulate_realtime_recognition(recognizer, duration=10):\n",
    "    \"\"\"Simulate real-time recognition with example code.\"\"\"\n",
    "    \n",
    "    if not recognizer:\n",
    "        print(\"No recognizer available for real-time simulation.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Real-time Speech Recognition Simulation\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Note: This is a simulation. For actual real-time recognition:\")\n",
    "    print(\"1. Ensure microphone permissions are granted\")\n",
    "    print(\"2. Run in a proper Python environment (not in notebook)\")\n",
    "    print(\"3. Have stable internet connection for Google API\")\n",
    "    print()\n",
    "    \n",
    "    # Show example code for real-time recognition\n",
    "    realtime_code = '''\n",
    "# Real-time speech recognition example code:\n",
    "import speech_recognition as sr\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "def realtime_recognition():\n",
    "    recognizer = sr.Recognizer()\n",
    "    microphone = sr.Microphone()\n",
    "    \n",
    "    # Adjust for ambient noise\n",
    "    with microphone as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "    \n",
    "    print(\"Listening... (speak now)\")\n",
    "    \n",
    "    def callback(recognizer, audio):\n",
    "        try:\n",
    "            # Use threading to avoid blocking\n",
    "            text = recognizer.recognize_google(audio, language='de-DE')\n",
    "            print(f\"Recognized: {text}\")\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    # Start listening in the background\n",
    "    stop_listening = recognizer.listen_in_background(microphone, callback)\n",
    "    \n",
    "    # Keep program running\n",
    "    import time\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(0.1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping...\")\n",
    "        stop_listening(wait_for_stop=False)\n",
    "\n",
    "# Run the real-time recognition\n",
    "# realtime_recognition()\n",
    "'''\n",
    "    \n",
    "    print(\"Example Real-time Recognition Code:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(realtime_code)\n",
    "    \n",
    "    # Demonstrate with audio file instead\n",
    "    if sample_audio and sample_audio.exists():\n",
    "        print(\"\\nDemonstrating with audio file instead:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            with sr.AudioFile(str(sample_audio)) as source:\n",
    "                audio = recognizer.record(source)\n",
    "            \n",
    "            # Simulate real-time processing\n",
    "            import time\n",
    "            print(\"Processing audio... \", end=\"\", flush=True)\n",
    "            for i in range(3):\n",
    "                time.sleep(0.5)\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "            print(\" Done!\")\n",
    "            \n",
    "            # Transcribe\n",
    "            try:\n",
    "                text = recognizer.recognize_google(audio, language='de-DE')\n",
    "                print(f\"Simulated real-time result: {text}\")\n",
    "            except:\n",
    "                try:\n",
    "                    text = recognizer.recognize_google(audio, language='en-US')\n",
    "                    print(f\"Simulated real-time result (EN): {text}\")\n",
    "                except:\n",
    "                    print(\"No speech recognized in simulation\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Simulation error: {e}\")\n",
    "\n",
    "# Set up and test real-time recognition\n",
    "print(\"Real-time Speech Recognition Setup:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "rt_recognizer = setup_realtime_recognition()\n",
    "\n",
    "if rt_recognizer:\n",
    "    mic_test = test_microphone_setup(rt_recognizer)\n",
    "    simulate_realtime_recognition(rt_recognizer)\n",
    "else:\n",
    "    print(\"Real-time recognition not available.\")\n",
    "\n",
    "print(\"\\n\\nüéâ All speech recognition solutions completed!\")\n",
    "print(\"Key techniques covered:\")\n",
    "print(\"‚Ä¢ Audio file processing and analysis\")\n",
    "print(\"‚Ä¢ Wav2Vec2 transformer-based ASR\")\n",
    "print(\"‚Ä¢ Multiple recognition engines comparison\")\n",
    "print(\"‚Ä¢ Advanced audio feature extraction\")\n",
    "print(\"‚Ä¢ Speech quality assessment\")\n",
    "print(\"‚Ä¢ Real-time recognition concepts\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
