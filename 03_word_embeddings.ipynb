{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14e4fec",
   "metadata": {},
   "source": [
    "# Exercise 3: Word Embeddings\n",
    "\n",
    "Welcome to the fascinating world of word embeddings! You'll learn how to represent words as vectors and discover semantic relationships between them.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this exercise, you will be able to:\n",
    "1. **Vector Representation**: Convert words into numerical vectors that capture semantic meaning\n",
    "2. **Similarity Analysis**: Calculate and interpret word similarities using cosine similarity\n",
    "3. **Word2Vec Training**: Train your own word embeddings on German text\n",
    "4. **Embedding Visualization**: Create 2D visualizations of high-dimensional word vectors\n",
    "5. **Analogy Tasks**: Solve word analogies using vector arithmetic (king - man + woman = queen)\n",
    "6. **German Language Processing**: Handle German-specific embeddings and compound words\n",
    "\n",
    "## What You'll Build\n",
    "- German word similarity analyzer\n",
    "- Custom Word2Vec model trained on German text\n",
    "- Interactive word embedding visualizations\n",
    "- Word analogy solver\n",
    "- Semantic clustering system\n",
    "\n",
    "## Applications\n",
    "- **Search Systems**: Find semantically similar documents\n",
    "- **Recommendation Engines**: Suggest related products based on descriptions\n",
    "- **Translation**: Bridge languages through shared embedding spaces\n",
    "- **Content Analysis**: Group similar concepts automatically\n",
    "\n",
    "**Ready to unlock the hidden meanings in words?** üî§‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9055eb28",
   "metadata": {},
   "source": [
    "## Exercise 1: Exploring Pre-trained Word Embeddings\n",
    "\n",
    "**Goal**: Explore semantic relationships using pre-trained German word embeddings.\n",
    "\n",
    "**Your Tasks**: \n",
    "1. Load and explore pre-trained German embeddings\n",
    "2. Calculate word similarities and find nearest neighbors\n",
    "3. Visualize word relationships in 2D space\n",
    "4. Solve word analogies using vector arithmetic\n",
    "\n",
    "**Hints**:\n",
    "- Use spaCy's German models for pre-trained embeddings\n",
    "- Cosine similarity measures the angle between word vectors\n",
    "- Similar words cluster together in embedding space\n",
    "- Vector arithmetic can reveal semantic relationships\n",
    "\n",
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aa5b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for word embeddings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import for word embeddings\n",
    "try:\n",
    "    from gensim.models import Word2Vec, KeyedVectors\n",
    "    from gensim.utils import simple_preprocess\n",
    "    print(\"‚úÖ Gensim imported successfully!\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Please install gensim: pip install gensim\")\n",
    "\n",
    "# Try to load German spaCy model with word vectors\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"de_core_news_md\")  # Medium model with word vectors\n",
    "    print(\"‚úÖ German spaCy model (medium) loaded successfully!\")\n",
    "    print(f\"   Model has {len(nlp.vocab)} vocabulary entries\")\n",
    "    print(f\"   Vector dimensions: {nlp.vocab.vectors.shape[1] if nlp.vocab.vectors.shape else 'No vectors'}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Please install spaCy: pip install spacy\")\n",
    "    nlp = None\n",
    "except IOError:\n",
    "    try:\n",
    "        nlp = spacy.load(\"de_core_news_sm\")  # Small model fallback\n",
    "        print(\"‚ö†Ô∏è  German spaCy model (small) loaded. Limited word vectors available.\")\n",
    "    except IOError:\n",
    "        print(\"‚ùå Please install German spaCy model: python -m spacy download de_core_news_md\")\n",
    "        nlp = None\n",
    "\n",
    "print(\"\\nü§ñ Word Embedding Toolkit Ready!\")\n",
    "print(\"Available tools: Pre-trained embeddings, Word2Vec training, Similarity analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1692c466",
   "metadata": {},
   "source": [
    "### Step 1: Exploring Pre-trained spaCy Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f1ef0",
   "metadata": {},
   "source": [
    "### Step 1: Basic Word Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465811d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_word_similarities(nlp_model, word, top_n=10):\n",
    "    \"\"\"\n",
    "    Find the most similar words to a given word using pre-trained embeddings.\n",
    "    \n",
    "    Args:\n",
    "        nlp_model: Loaded spaCy model\n",
    "        word (str): Target word to find similarities for\n",
    "        top_n (int): Number of similar words to return\n",
    "    \n",
    "    Returns:\n",
    "        list: List of (word, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # TODO: Implement word similarity analysis:\n",
    "    # 1. Get the word vector for the target word\n",
    "    # 2. Calculate similarities with other words in vocabulary\n",
    "    # 3. Return top N most similar words with scores\n",
    "    \n",
    "    if nlp_model is None:\n",
    "        print(\"No language model loaded!\")\n",
    "        return []\n",
    "    \n",
    "    # Get the target word's vector\n",
    "    target_doc = nlp_model(word)\n",
    "    if not target_doc[0].has_vector:\n",
    "        print(f\"No vector available for word: {word}\")\n",
    "        return []\n",
    "    \n",
    "    target_vector = target_doc[0].vector\n",
    "    \n",
    "    # Find similar words by comparing with vocabulary\n",
    "    similarities = []\n",
    "    \n",
    "    # Sample from vocabulary (full vocab is very large)\n",
    "    vocab_sample = list(nlp_model.vocab)[:10000]  # Sample first 10k words\n",
    "    \n",
    "    for token in vocab_sample:\n",
    "        if token.has_vector and token.is_alpha and not token.is_stop:\n",
    "            similarity = target_doc[0].similarity(nlp_model(token.text)[0])\n",
    "            if similarity > 0.3:  # Filter out very dissimilar words\n",
    "                similarities.append((token.text, similarity))\n",
    "    \n",
    "    # Sort by similarity and return top N\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Test word similarity analysis\n",
    "if nlp:\n",
    "    test_words = [\"Hund\", \"Auto\", \"sch√∂n\", \"Berlin\", \"essen\"]\n",
    "    \n",
    "    print(\"üîç Word Similarity Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for word in test_words:\n",
    "        print(f\"\\nWords similar to '{word}':\")\n",
    "        similar_words = explore_word_similarities(nlp, word, top_n=5)\n",
    "        \n",
    "        if similar_words:\n",
    "            for sim_word, score in similar_words:\n",
    "                print(f\"  {sim_word}: {score:.3f}\")\n",
    "        else:\n",
    "            print(\"  No similar words found or word not in vocabulary\")\n",
    "else:\n",
    "    print(\"Please load a spaCy model with word vectors to run this analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1598f4bb",
   "metadata": {},
   "source": [
    "### Step 2: Training Custom German Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ba78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_german_word2vec(texts, vector_size=100, window=5, min_count=2):\n",
    "    \"\"\"\n",
    "    Train a custom Word2Vec model on German text.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of German texts\n",
    "        vector_size (int): Dimensionality of word vectors\n",
    "        window (int): Context window size\n",
    "        min_count (int): Minimum word frequency to include\n",
    "    \n",
    "    Returns:\n",
    "        Word2Vec: Trained model\n",
    "    \"\"\"\n",
    "    # TODO: Implement Word2Vec training:\n",
    "    # 1. Preprocess texts (tokenization, cleaning)\n",
    "    # 2. Create Word2Vec model with appropriate parameters\n",
    "    # 3. Train the model on your corpus\n",
    "    # 4. Save the model for later use\n",
    "    \n",
    "    if 'Word2Vec' not in globals():\n",
    "        print(\"Please install gensim: pip install gensim\")\n",
    "        return None\n",
    "    \n",
    "    # Preprocess texts for training\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        # Simple tokenization and cleaning\n",
    "        words = simple_preprocess(text, deacc=True, min_len=2, max_len=15)\n",
    "        processed_texts.append(words)\n",
    "    \n",
    "    print(f\"Training Word2Vec on {len(processed_texts)} documents...\")\n",
    "    print(f\"Parameters: vector_size={vector_size}, window={window}, min_count={min_count}\")\n",
    "    \n",
    "    # Create and train Word2Vec model\n",
    "    model = Word2Vec(\n",
    "        sentences=processed_texts,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=4,\n",
    "        sg=1,  # Skip-gram model\n",
    "        epochs=10\n",
    "    )\n",
    "    \n",
    "    print(f\"Model trained! Vocabulary size: {len(model.wv.key_to_index)}\")\n",
    "    return model\n",
    "\n",
    "# Create sample German texts for training\n",
    "sample_german_texts = [\n",
    "    \"Berlin ist die Hauptstadt von Deutschland und eine wundersch√∂ne Stadt.\",\n",
    "    \"M√ºnchen ist bekannt f√ºr das Oktoberfest und liegt in Bayern.\",\n",
    "    \"Hamburg hat einen gro√üen Hafen und ist eine wichtige Hafenstadt.\",\n",
    "    \"K√∂ln ist eine alte Stadt mit einem ber√ºhmten Dom.\",\n",
    "    \"Frankfurt ist das Finanzzentrum Deutschlands mit vielen Banken.\",\n",
    "    \"Stuttgart ist die Heimat von Mercedes-Benz und Porsche.\",\n",
    "    \"Dresden ist eine kulturell reiche Stadt in Sachsen.\",\n",
    "    \"Leipzig ist eine Universit√§tsstadt mit langer Geschichte.\",\n",
    "    \"N√ºrnberg ist bekannt f√ºr Lebkuchen und Christkindlm√§rkte.\",\n",
    "    \"Bremen ist eine Hansestadt im Norden Deutschlands.\"\n",
    "]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "custom_model = train_german_word2vec(sample_german_texts, vector_size=50, window=3)\n",
    "\n",
    "if custom_model:\n",
    "    print(\"\\nüéØ Testing Custom Word2Vec Model:\")\n",
    "    test_word = \"Berlin\"\n",
    "    try:\n",
    "        similar_words = custom_model.wv.most_similar(test_word, topn=3)\n",
    "        print(f\"Words similar to '{test_word}':\")\n",
    "        for word, score in similar_words:\n",
    "            print(f\"  {word}: {score:.3f}\")\n",
    "    except KeyError:\n",
    "        print(f\"Word '{test_word}' not in vocabulary. Try: {list(custom_model.wv.key_to_index.keys())[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df602496",
   "metadata": {},
   "source": [
    "### Step 3: Word Analogy Tasks\n",
    "\n",
    "def solve_word_analogies(model, analogies):\n",
    "    \"\"\"\n",
    "    Solve word analogies using vector arithmetic.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained word embedding model (Word2Vec or spaCy)\n",
    "        analogies (list): List of (word1, word2, word3) tuples for \"word1 is to word2 as word3 is to ?\"\n",
    "    \n",
    "    Returns:\n",
    "        list: Solutions to analogies\n",
    "    \"\"\"\n",
    "    # TODO: Implement word analogy solving:\n",
    "    # 1. Use vector arithmetic: word2 - word1 + word3 = answer\n",
    "    # 2. Find the word closest to the result vector\n",
    "    # 3. Handle cases where words are not in vocabulary\n",
    "    \n",
    "    print(\"üß† Solving Word Analogies...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    results = []\n",
    "    for word1, word2, word3 in analogies:\n",
    "        try:\n",
    "            if hasattr(model, 'wv'):  # Word2Vec model\n",
    "                result = model.wv.most_similar(positive=[word2, word3], negative=[word1], topn=1)\n",
    "                answer = result[0][0]\n",
    "                confidence = result[0][1]\n",
    "            else:  # spaCy model\n",
    "                vec1 = model(word1)[0].vector\n",
    "                vec2 = model(word2)[0].vector\n",
    "                vec3 = model(word3)[0].vector\n",
    "                result_vec = vec2 - vec1 + vec3\n",
    "                # Find closest word (simplified approach)\n",
    "                answer = \"unknown\"\n",
    "                confidence = 0.0\n",
    "            \n",
    "            print(f\"{word1} : {word2} :: {word3} : {answer} (confidence: {confidence:.3f})\")\n",
    "            results.append((word1, word2, word3, answer, confidence))\n",
    "            \n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(f\"{word1} : {word2} :: {word3} : [word not in vocabulary]\")\n",
    "            results.append((word1, word2, word3, None, 0.0))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test analogies with our custom model\n",
    "german_analogies = [\n",
    "    (\"K√∂nig\", \"Mann\", \"Frau\"),      # King:Man :: Woman:?\n",
    "    (\"Berlin\", \"Deutschland\", \"Paris\"),  # Berlin:Germany :: Paris:?\n",
    "    (\"gro√ü\", \"gr√∂√üer\", \"klein\"),    # big:bigger :: small:?\n",
    "]\n",
    "\n",
    "if custom_model:\n",
    "    analogy_results = solve_word_analogies(custom_model, german_analogies)\n",
    "\n",
    "## Exercise Tasks\n",
    "\n",
    "Complete the following tasks to deepen your understanding:\n",
    "\n",
    "1. **Advanced Similarity Analysis**:\n",
    "   - Compare different similarity metrics (cosine, euclidean, manhattan)\n",
    "   - Analyze how similarity scores change with different vector dimensions\n",
    "   - Create word similarity heatmaps for related concepts\n",
    "\n",
    "2. **Embedding Visualization**:\n",
    "   - Use t-SNE or PCA to visualize word embeddings in 2D\n",
    "   - Create interactive plots with plotly\n",
    "   - Identify semantic clusters in the visualization\n",
    "\n",
    "3. **German-Specific Challenges**:\n",
    "   - Handle German compound words (Komposita)\n",
    "   - Analyze how umlauts affect similarity scores\n",
    "   - Compare performance on formal vs. informal German text\n",
    "\n",
    "4. **Model Comparison**:\n",
    "   - Compare Word2Vec, FastText, and transformer embeddings\n",
    "   - Evaluate on word similarity benchmarks\n",
    "   - Analyze computational efficiency trade-offs\n",
    "\n",
    "5. **Application Development**:\n",
    "   - Build a semantic search engine for German documents\n",
    "   - Create a word analogy game interface\n",
    "   - Implement document similarity using averaged word vectors\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. How do different training parameters affect embedding quality?\n",
    "2. What are the advantages and disadvantages of different embedding approaches?\n",
    "3. How can you evaluate embedding quality without labeled data?\n",
    "4. What challenges are specific to German language embeddings?\n",
    "5. How do embeddings capture semantic vs. syntactic relationships?\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore contextual embeddings (BERT, ELMo)\n",
    "- Learn about cross-lingual embeddings for translation\n",
    "- Study specialized domain embeddings (medical, legal, technical)\n",
    "- Investigate bias in word embeddings and mitigation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e4606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_spacy_embeddings(nlp_model, words):\n",
    "    \"\"\"\n",
    "    Explore word embeddings using spaCy.\n",
    "    \n",
    "    Args:\n",
    "        nlp_model: Loaded spaCy model\n",
    "        words (list): List of German words to analyze\n",
    "    \n",
    "    Returns:\n",
    "        dict: Word embeddings and similarities\n",
    "    \"\"\"\n",
    "    if nlp_model is None:\n",
    "        print(\"spaCy model not available\")\n",
    "        return None\n",
    "    \n",
    "    # TODO: Implement the following analysis:\n",
    "    # 1. Get word vectors for each word\n",
    "    # 2. Calculate pairwise similarities\n",
    "    # 3. Find most similar words\n",
    "    # 4. Explore word analogies\n",
    "    \n",
    "    results = {\n",
    "        'embeddings': {},\n",
    "        'similarities': {},\n",
    "        'most_similar': {}\n",
    "    }\n",
    "    \n",
    "    print(\"Analyzing spaCy word embeddings:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Get embeddings for each word\n",
    "    for word in words:\n",
    "        token = nlp_model(word)[0]\n",
    "        if token.has_vector:\n",
    "            results['embeddings'][word] = token.vector\n",
    "            print(f\"Word: {word}\")\n",
    "            print(f\"  Vector shape: {token.vector.shape}\")\n",
    "            print(f\"  Vector norm: {np.linalg.norm(token.vector):.3f}\")\n",
    "            \n",
    "            # Find most similar words in vocabulary\n",
    "            # Note: This is a simplified approach - spaCy doesn't have direct most_similar\n",
    "            similarities = []\n",
    "            sample_words = [\"Auto\", \"Haus\", \"Katze\", \"Hund\", \"Buch\", \"Computer\", \"Wasser\", \"Liebe\"]\n",
    "            \n",
    "            for other_word in sample_words:\n",
    "                if other_word != word:\n",
    "                    other_token = nlp_model(other_word)[0]\n",
    "                    if other_token.has_vector:\n",
    "                        similarity = token.similarity(other_token)\n",
    "                        similarities.append((other_word, similarity))\n",
    "            \n",
    "            # Sort by similarity\n",
    "            similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "            results['most_similar'][word] = similarities[:3]\n",
    "            \n",
    "            print(f\"  Most similar words:\")\n",
    "            for sim_word, sim_score in similarities[:3]:\n",
    "                print(f\"    {sim_word}: {sim_score:.3f}\")\n",
    "        else:\n",
    "            print(f\"Word '{word}' has no vector representation\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test words for analysis\n",
    "test_words = [\"K√∂nig\", \"K√∂nigin\", \"Mann\", \"Frau\", \"Berlin\", \"Deutschland\", \"Auto\", \"fahren\"]\n",
    "\n",
    "# Explore spaCy embeddings\n",
    "spacy_results = explore_spacy_embeddings(nlp, test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e404046",
   "metadata": {},
   "source": [
    "### Step 2: Creating Training Data for Custom Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ddc839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_corpus():\n",
    "    \"\"\"\n",
    "    Create a sample German corpus for training word embeddings.\n",
    "    In practice, you would load a much larger corpus from files.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tokenized sentences\n",
    "    \"\"\"\n",
    "    # TODO: Create a diverse German text corpus:\n",
    "    # 1. Include various topics and domains\n",
    "    # 2. Ensure sufficient word frequency for meaningful embeddings\n",
    "    # 3. Preprocess and tokenize the text\n",
    "    \n",
    "    german_texts = [\n",
    "        # Technology and computers\n",
    "        \"Computer sind heute sehr wichtig f√ºr die Arbeit und das Leben.\",\n",
    "        \"Das Internet verbindet Menschen auf der ganzen Welt miteinander.\",\n",
    "        \"K√ºnstliche Intelligenz wird immer wichtiger in der Technologie.\",\n",
    "        \"Smartphones und Tablets sind mobile Computer geworden.\",\n",
    "        \"Software und Hardware m√ºssen gut zusammenarbeiten.\",\n",
    "        \n",
    "        # Transportation\n",
    "        \"Autos fahren auf Stra√üen und Autobahnen durch die Stadt.\",\n",
    "        \"Der Zug f√§hrt schnell vom Bahnhof zum n√§chsten Bahnhof.\",\n",
    "        \"Flugzeuge fliegen hoch √ºber den Wolken zum Zielort.\",\n",
    "        \"Fahrr√§der sind umweltfreundliche Verkehrsmittel in der Stadt.\",\n",
    "        \"Busse transportieren viele Passagiere durch die Stadt.\",\n",
    "        \n",
    "        # Family and relationships\n",
    "        \"Die Familie ist sehr wichtig f√ºr das Gl√ºck der Menschen.\",\n",
    "        \"Eltern lieben ihre Kinder und sorgen f√ºr sie.\",\n",
    "        \"Freunde helfen sich gegenseitig in schwierigen Zeiten.\",\n",
    "        \"Gro√üeltern erz√§hlen ihren Enkeln interessante Geschichten.\",\n",
    "        \"Geschwister spielen zusammen und lernen voneinander.\",\n",
    "        \n",
    "        # Nature and animals\n",
    "        \"Hunde sind treue Freunde und beliebte Haustiere.\",\n",
    "        \"Katzen sind unabh√§ngige und elegante Tiere.\",\n",
    "        \"V√∂gel fliegen frei in der Luft und singen sch√∂ne Lieder.\",\n",
    "        \"B√§ume wachsen in W√§ldern und Parks der Stadt.\",\n",
    "        \"Blumen bl√ºhen im Fr√ºhling in bunten Farben.\",\n",
    "        \n",
    "        # Food and cooking\n",
    "        \"Deutsche essen gerne Brot, Wurst und K√§se zum Fr√ºhst√ºck.\",\n",
    "        \"Kochen macht Spa√ü und bringt Familien zusammen.\",\n",
    "        \"Restaurants servieren leckere Gerichte aus aller Welt.\",\n",
    "        \"Obst und Gem√ºse sind gesund und wichtig f√ºr die Ern√§hrung.\",\n",
    "        \"Kuchen und Torte sind beliebte Desserts in Deutschland.\",\n",
    "        \n",
    "        # Education and learning\n",
    "        \"Sch√ºler lernen in der Schule viele wichtige F√§cher.\",\n",
    "        \"Lehrer unterrichten mit Begeisterung und Geduld.\",\n",
    "        \"B√ºcher enthalten Wissen und spannende Geschichten.\",\n",
    "        \"Universit√§ten bieten h√∂here Bildung und Forschung.\",\n",
    "        \"Lernen ist ein lebenslanger Prozess f√ºr alle Menschen.\",\n",
    "        \n",
    "        # Work and professions\n",
    "        \"√Ñrzte helfen kranken Menschen und retten Leben.\",\n",
    "        \"Ingenieure entwickeln neue Technologien und Maschinen.\",\n",
    "        \"K√ºnstler schaffen sch√∂ne Werke und inspirieren andere.\",\n",
    "        \"Handwerker bauen und reparieren wichtige Dinge.\",\n",
    "        \"Wissenschaftler forschen und entdecken neue Erkenntnisse.\"\n",
    "    ]\n",
    "    \n",
    "    # Tokenize sentences\n",
    "    tokenized_corpus = []\n",
    "    for text in german_texts:\n",
    "        # Simple preprocessing and tokenization\n",
    "        tokens = simple_preprocess(text, deacc=True)  # Remove accents and punctuation\n",
    "        tokenized_corpus.append(tokens)\n",
    "    \n",
    "    print(f\"Created corpus with {len(tokenized_corpus)} sentences\")\n",
    "    print(f\"Sample sentence: {tokenized_corpus[0]}\")\n",
    "    \n",
    "    # Calculate vocabulary statistics\n",
    "    all_words = [word for sentence in tokenized_corpus for word in sentence]\n",
    "    unique_words = set(all_words)\n",
    "    \n",
    "    print(f\"Total words: {len(all_words)}\")\n",
    "    print(f\"Unique words: {len(unique_words)}\")\n",
    "    \n",
    "    return tokenized_corpus\n",
    "\n",
    "# Create training corpus\n",
    "training_corpus = create_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf47af7",
   "metadata": {},
   "source": [
    "### Step 3: Training Custom Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b12e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec_model(corpus, vector_size=100, window=5, min_count=1, workers=4):\n",
    "    \"\"\"\n",
    "    Train a custom Word2Vec model on the German corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus (list): Tokenized sentences\n",
    "        vector_size (int): Dimensionality of word vectors\n",
    "        window (int): Context window size\n",
    "        min_count (int): Minimum word frequency\n",
    "        workers (int): Number of worker threads\n",
    "    \n",
    "    Returns:\n",
    "        Word2Vec: Trained model\n",
    "    \"\"\"\n",
    "    # TODO: Train Word2Vec model with different parameters:\n",
    "    # 1. Try both CBOW and Skip-gram architectures\n",
    "    # 2. Experiment with different vector sizes\n",
    "    # 3. Test various window sizes\n",
    "    # 4. Analyze the impact of min_count\n",
    "    \n",
    "    print(\"Training Word2Vec model...\")\n",
    "    \n",
    "    # Train Skip-gram model (sg=1) - good for small datasets\n",
    "    model = Word2Vec(\n",
    "        sentences=corpus,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=1,  # Skip-gram (1) vs CBOW (0)\n",
    "        epochs=20,  # More epochs for better training\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Model trained successfully!\")\n",
    "    print(f\"Vocabulary size: {len(model.wv.key_to_index)}\")\n",
    "    print(f\"Vector dimensions: {model.wv.vector_size}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def analyze_word2vec_model(model, test_words):\n",
    "    \"\"\"\n",
    "    Analyze the trained Word2Vec model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Word2Vec model  \n",
    "        test_words (list): Words to analyze\n",
    "    \"\"\"\n",
    "    print(\"\\nWord2Vec Model Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    wv = model.wv  # KeyedVectors object\n",
    "    \n",
    "    for word in test_words:\n",
    "        if word in wv.key_to_index:\n",
    "            print(f\"\\nWord: {word}\")\n",
    "            \n",
    "            # Get most similar words\n",
    "            try:\n",
    "                similar_words = wv.most_similar(word, topn=3)\n",
    "                print(f\"Most similar words:\")\n",
    "                for sim_word, similarity in similar_words:\n",
    "                    print(f\"  {sim_word}: {similarity:.3f}\")\n",
    "            except:\n",
    "                print(f\"  Could not find similar words for '{word}'\")\n",
    "                \n",
    "            # Get vector\n",
    "            vector = wv[word]\n",
    "            print(f\"Vector shape: {vector.shape}\")\n",
    "            print(f\"Vector norm: {np.linalg.norm(vector):.3f}\")\n",
    "        else:\n",
    "            print(f\"\\nWord '{word}' not in vocabulary\")\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = train_word2vec_model(training_corpus)\n",
    "\n",
    "# Analyze the model\n",
    "analyze_word2vec_model(w2v_model, [\"computer\", \"auto\", \"hund\", \"haus\", \"lernen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79edb9",
   "metadata": {},
   "source": [
    "### Step 4: Word Similarity and Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_word_relationships(model):\n",
    "    \"\"\"\n",
    "    Explore word relationships and analogies in the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Word2Vec model\n",
    "    \"\"\"\n",
    "    # TODO: Implement word relationship analysis:\n",
    "    # 1. Calculate pairwise similarities\n",
    "    # 2. Test word analogies (A is to B as C is to D)\n",
    "    # 3. Find words that don't belong in a group\n",
    "    # 4. Explore semantic relationships\n",
    "    \n",
    "    wv = model.wv\n",
    "    \n",
    "    print(\"Exploring Word Relationships:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Test pairwise similarities\n",
    "    word_pairs = [\n",
    "        (\"hund\", \"katze\"),\n",
    "        (\"auto\", \"fahrrad\"),\n",
    "        (\"computer\", \"technologie\"),\n",
    "        (\"haus\", \"wohnung\"),\n",
    "        (\"lernen\", \"schule\")\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nPairwise Similarities:\")\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in wv.key_to_index and word2 in wv.key_to_index:\n",
    "            similarity = wv.similarity(word1, word2)\n",
    "            print(f\"{word1} <-> {word2}: {similarity:.3f}\")\n",
    "        else:\n",
    "            print(f\"{word1} <-> {word2}: Words not in vocabulary\")\n",
    "    \n",
    "    # Test analogies (if vocabulary is sufficient)\n",
    "    print(\"\\nWord Analogies:\")\n",
    "    analogies = [\n",
    "        (\"mann\", \"frau\", \"vater\"),  # mann:frau :: vater:?\n",
    "        (\"auto\", \"fahren\", \"flugzeug\"),  # auto:fahren :: flugzeug:?\n",
    "        (\"hund\", \"bellen\", \"katze\"),  # hund:bellen :: katze:?\n",
    "    ]\n",
    "    \n",
    "    for word1, word2, word3 in analogies:\n",
    "        if all(word in wv.key_to_index for word in [word1, word2, word3]):\n",
    "            try:\n",
    "                # A is to B as C is to ?\n",
    "                result = wv.most_similar(positive=[word2, word3], negative=[word1], topn=1)\n",
    "                if result:\n",
    "                    answer, score = result[0]\n",
    "                    print(f\"{word1}:{word2} :: {word3}:{answer} (score: {score:.3f})\")\n",
    "            except:\n",
    "                print(f\"Could not compute analogy for {word1}:{word2} :: {word3}:?\")\n",
    "        else:\n",
    "            print(f\"Analogy {word1}:{word2} :: {word3}:? - missing words in vocabulary\")\n",
    "    \n",
    "    # Find odd-one-out\n",
    "    print(\"\\nOdd-One-Out:\")\n",
    "    word_groups = [\n",
    "        [\"hund\", \"katze\", \"auto\"],  # auto should be odd\n",
    "        [\"computer\", \"internet\", \"baum\"],  # baum should be odd\n",
    "        [\"essen\", \"trinken\", \"fahren\"]  # fahren should be odd\n",
    "    ]\n",
    "    \n",
    "    for group in word_groups:\n",
    "        available_words = [word for word in group if word in wv.key_to_index]\n",
    "        if len(available_words) >= 3:\n",
    "            try:\n",
    "                odd_word = wv.doesnt_match(available_words)\n",
    "                print(f\"In {available_words}, the odd one is: {odd_word}\")\n",
    "            except:\n",
    "                print(f\"Could not find odd word in {available_words}\")\n",
    "        else:\n",
    "            print(f\"Not enough words from {group} in vocabulary\")\n",
    "\n",
    "# Explore relationships\n",
    "explore_word_relationships(w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab348de",
   "metadata": {},
   "source": [
    "### Step 5: Visualizing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(model, words_to_plot=None, method='tsne'):\n",
    "    \"\"\"\n",
    "    Visualize word embeddings in 2D space.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Word2Vec model\n",
    "        words_to_plot (list): Specific words to visualize\n",
    "        method (str): Dimensionality reduction method ('tsne' or 'pca')\n",
    "    \"\"\"\n",
    "    # TODO: Create 2D visualization of word embeddings:\n",
    "    # 1. Select representative words for visualization\n",
    "    # 2. Apply dimensionality reduction (t-SNE or PCA)\n",
    "    # 3. Create scatter plot with word labels\n",
    "    # 4. Color-code by semantic categories if possible\n",
    "    \n",
    "    wv = model.wv\n",
    "    \n",
    "    # Select words to plot\n",
    "    if words_to_plot is None:\n",
    "        # Select most frequent words\n",
    "        words_to_plot = list(wv.key_to_index.keys())[:30]  # Top 30 words\n",
    "    \n",
    "    # Filter words that exist in vocabulary\n",
    "    available_words = [word for word in words_to_plot if word in wv.key_to_index]\n",
    "    \n",
    "    if len(available_words) < 5:\n",
    "        print(\"Not enough words available for visualization\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Visualizing {len(available_words)} words using {method.upper()}\")\n",
    "    \n",
    "    # Get word vectors\n",
    "    word_vectors = [wv[word] for word in available_words]\n",
    "    word_vectors = np.array(word_vectors)\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    if method.lower() == 'tsne':\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(available_words)-1))\n",
    "    else:\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "    \n",
    "    word_vectors_2d = reducer.fit_transform(word_vectors)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Define semantic categories for coloring\n",
    "    categories = {\n",
    "        'animals': ['hund', 'katze', 'vogel', 'tier', 'tiere'],\n",
    "        'technology': ['computer', 'internet', 'technologie', 'software', 'hardware'],\n",
    "        'transport': ['auto', 'zug', 'flugzeug', 'fahrrad', 'fahren'],\n",
    "        'family': ['familie', 'mutter', 'vater', 'kind', 'kinder', 'eltern'],\n",
    "        'education': ['schule', 'lernen', 'lehrer', 'sch√ºler', 'universit√§t', 'buch'],\n",
    "        'other': []\n",
    "    }\n",
    "    \n",
    "    # Assign categories to words\n",
    "    word_categories = {}\n",
    "    for word in available_words:\n",
    "        assigned = False\n",
    "        for category, category_words in categories.items():\n",
    "            if word in category_words:\n",
    "                word_categories[word] = category\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            word_categories[word] = 'other'\n",
    "    \n",
    "    # Color map for categories\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'gray']\n",
    "    category_colors = {cat: colors[i] for i, cat in enumerate(categories.keys())}\n",
    "    \n",
    "    # Plot points\n",
    "    for i, word in enumerate(available_words):\n",
    "        x, y = word_vectors_2d[i]\n",
    "        color = category_colors[word_categories[word]]\n",
    "        plt.scatter(x, y, c=color, alpha=0.7, s=100)\n",
    "        plt.annotate(word, (x, y), xytext=(5, 5), textcoords='offset points', \n",
    "                    fontsize=10, alpha=0.8)\n",
    "    \n",
    "    # Create legend\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                 markerfacecolor=category_colors[cat], \n",
    "                                 markersize=10, label=cat.capitalize())\n",
    "                      for cat in categories.keys()]\n",
    "    plt.legend(handles=legend_elements, loc='best')\n",
    "    \n",
    "    plt.title(f'Word Embeddings Visualization ({method.upper()})')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize embeddings\n",
    "visualize_embeddings(w2v_model, method='tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259ac136",
   "metadata": {},
   "source": [
    "### Step 6: Clustering Words by Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_word_embeddings(model, n_clusters=5, words_to_cluster=None):\n",
    "    \"\"\"\n",
    "    Cluster words based on their embedding similarity.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Word2Vec model\n",
    "        n_clusters (int): Number of clusters\n",
    "        words_to_cluster (list): Specific words to cluster\n",
    "    \n",
    "    Returns:\n",
    "        dict: Clustering results\n",
    "    \"\"\"\n",
    "    # TODO: Implement word clustering:\n",
    "    # 1. Select words for clustering\n",
    "    # 2. Apply K-means clustering\n",
    "    # 3. Analyze cluster composition\n",
    "    # 4. Visualize clusters\n",
    "    \n",
    "    wv = model.wv\n",
    "    \n",
    "    # Select words to cluster\n",
    "    if words_to_cluster is None:\n",
    "        words_to_cluster = list(wv.key_to_index.keys())[:30]  # Top 30 words\n",
    "    \n",
    "    # Filter available words\n",
    "    available_words = [word for word in words_to_cluster if word in wv.key_to_index]\n",
    "    \n",
    "    if len(available_words) < n_clusters:\n",
    "        print(f\"Not enough words for {n_clusters} clusters\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Clustering {len(available_words)} words into {n_clusters} clusters\")\n",
    "    \n",
    "    # Get word vectors\n",
    "    word_vectors = np.array([wv[word] for word in available_words])\n",
    "    \n",
    "    # Apply K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(word_vectors)\n",
    "    \n",
    "    # Organize results by cluster\n",
    "    clusters = {i: [] for i in range(n_clusters)}\n",
    "    for word, label in zip(available_words, cluster_labels):\n",
    "        clusters[label].append(word)\n",
    "    \n",
    "    # Display clusters\n",
    "    print(\"\\nClustering Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    for cluster_id, words in clusters.items():\n",
    "        print(f\"Cluster {cluster_id}: {', '.join(words)}\")\n",
    "    \n",
    "    # Visualize clusters\n",
    "    # Reduce dimensionality for visualization\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    word_vectors_2d = pca.fit_transform(word_vectors)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']\n",
    "    \n",
    "    for i, (word, label) in enumerate(zip(available_words, cluster_labels)):\n",
    "        x, y = word_vectors_2d[i]\n",
    "        color = colors[label % len(colors)]\n",
    "        plt.scatter(x, y, c=color, alpha=0.7, s=100)\n",
    "        plt.annotate(word, (x, y), xytext=(5, 5), textcoords='offset points', \n",
    "                    fontsize=10, alpha=0.8)\n",
    "    \n",
    "    plt.title('Word Embedding Clusters')\n",
    "    plt.xlabel('PCA Dimension 1')\n",
    "    plt.ylabel('PCA Dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "# Cluster word embeddings\n",
    "clusters = cluster_word_embeddings(w2v_model, n_clusters=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68da0af",
   "metadata": {},
   "source": [
    "### Step 7: Comparing Different Embedding Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455de3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embedding_methods(corpus):\n",
    "    \"\"\"\n",
    "    Compare different embedding methods on the same corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus (list): Tokenized sentences\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comparison results\n",
    "    \"\"\"\n",
    "    # TODO: Train and compare different embedding methods:\n",
    "    # 1. Word2Vec CBOW vs Skip-gram\n",
    "    # 2. FastText (handles subwords)\n",
    "    # 3. Different vector dimensions\n",
    "    # 4. Compare performance on similarity tasks\n",
    "    \n",
    "    print(\"Comparing Different Embedding Methods:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    methods = {}\n",
    "    \n",
    "    # Word2Vec CBOW\n",
    "    print(\"Training Word2Vec CBOW...\")\n",
    "    w2v_cbow = Word2Vec(corpus, vector_size=100, window=5, min_count=1, \n",
    "                        sg=0, epochs=20, seed=42)  # sg=0 for CBOW\n",
    "    methods['Word2Vec CBOW'] = w2v_cbow\n",
    "    \n",
    "    # Word2Vec Skip-gram\n",
    "    print(\"Training Word2Vec Skip-gram...\")\n",
    "    w2v_skipgram = Word2Vec(corpus, vector_size=100, window=5, min_count=1, \n",
    "                           sg=1, epochs=20, seed=42)  # sg=1 for Skip-gram\n",
    "    methods['Word2Vec Skip-gram'] = w2v_skipgram\n",
    "    \n",
    "    # FastText\n",
    "    print(\"Training FastText...\")\n",
    "    fasttext_model = FastText(corpus, vector_size=100, window=5, min_count=1, \n",
    "                             epochs=20, seed=42)\n",
    "    methods['FastText'] = fasttext_model\n",
    "    \n",
    "    # Compare on similarity tasks\n",
    "    test_pairs = [\n",
    "        (\"hund\", \"katze\"),\n",
    "        (\"auto\", \"fahrrad\"),\n",
    "        (\"computer\", \"technologie\"),\n",
    "        (\"lernen\", \"schule\")\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nSimilarity Comparison:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    results = {}\n",
    "    for method_name, model in methods.items():\n",
    "        print(f\"\\n{method_name}:\")\n",
    "        similarities = []\n",
    "        \n",
    "        for word1, word2 in test_pairs:\n",
    "            if word1 in model.wv.key_to_index and word2 in model.wv.key_to_index:\n",
    "                sim = model.wv.similarity(word1, word2)\n",
    "                similarities.append(sim)\n",
    "                print(f\"  {word1}-{word2}: {sim:.3f}\")\n",
    "            else:\n",
    "                print(f\"  {word1}-{word2}: Words not in vocabulary\")\n",
    "        \n",
    "        results[method_name] = {\n",
    "            'model': model,\n",
    "            'vocab_size': len(model.wv.key_to_index),\n",
    "            'avg_similarity': np.mean(similarities) if similarities else 0\n",
    "        }\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Summary Comparison:\")\n",
    "    for method_name, result in results.items():\n",
    "        print(f\"{method_name}:\")\n",
    "        print(f\"  Vocabulary size: {result['vocab_size']}\")\n",
    "        print(f\"  Average similarity: {result['avg_similarity']:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare different methods\n",
    "comparison_results = compare_embedding_methods(training_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263dd06",
   "metadata": {},
   "source": [
    "### Step 8: Practical Application - Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e055a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_similarity_with_embeddings(model, documents):\n",
    "    \"\"\"\n",
    "    Calculate document similarity using word embeddings.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Word2Vec model\n",
    "        documents (list): List of documents (strings)\n",
    "    \n",
    "    Returns:\n",
    "        numpy.array: Document similarity matrix\n",
    "    \"\"\"\n",
    "    # TODO: Implement document similarity using embeddings:\n",
    "    # 1. Convert documents to word vectors\n",
    "    # 2. Aggregate words to document vectors (average, weighted average)\n",
    "    # 3. Calculate pairwise document similarities\n",
    "    # 4. Compare with traditional methods (TF-IDF)\n",
    "    \n",
    "    print(\"Calculating Document Similarity using Word Embeddings:\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    wv = model.wv\n",
    "    \n",
    "    def document_to_vector(doc_text, method='average'):\n",
    "        \"\"\"Convert document to vector representation.\"\"\"\n",
    "        words = simple_preprocess(doc_text)\n",
    "        word_vectors = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word in wv.key_to_index:\n",
    "                word_vectors.append(wv[word])\n",
    "        \n",
    "        if not word_vectors:\n",
    "            return np.zeros(wv.vector_size)\n",
    "        \n",
    "        if method == 'average':\n",
    "            return np.mean(word_vectors, axis=0)\n",
    "        elif method == 'sum':\n",
    "            return np.sum(word_vectors, axis=0)\n",
    "        else:\n",
    "            return np.mean(word_vectors, axis=0)\n",
    "    \n",
    "    # Convert documents to vectors\n",
    "    doc_vectors = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        doc_vec = document_to_vector(doc)\n",
    "        doc_vectors.append(doc_vec)\n",
    "        print(f\"Document {i+1}: {len(simple_preprocess(doc))} words -> vector shape {doc_vec.shape}\")\n",
    "    \n",
    "    doc_vectors = np.array(doc_vectors)\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    similarity_matrix = cosine_similarity(doc_vectors)\n",
    "    \n",
    "    print(\"\\nDocument Similarity Matrix:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Display similarity matrix\n",
    "    for i in range(len(documents)):\n",
    "        for j in range(len(documents)):\n",
    "            print(f\"{similarity_matrix[i][j]:.3f}\", end=\"  \")\n",
    "        print()\n",
    "    \n",
    "    # Find most similar document pairs\n",
    "    print(\"\\nMost Similar Document Pairs:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    for i in range(len(documents)):\n",
    "        for j in range(i+1, len(documents)):\n",
    "            similarity = similarity_matrix[i][j]\n",
    "            print(f\"Doc {i+1} <-> Doc {j+1}: {similarity:.3f}\")\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "# Test documents\n",
    "test_documents = [\n",
    "    \"Computer und Technologie sind sehr wichtig f√ºr die moderne Arbeit.\",\n",
    "    \"Hunde und Katzen sind beliebte Haustiere in deutschen Familien.\",\n",
    "    \"Das Internet und Software ver√§ndern unser Leben t√§glich.\",\n",
    "    \"Tiere wie V√∂gel und Fische leben in der freien Natur.\",\n",
    "    \"Autos und Z√ºge sind wichtige Verkehrsmittel f√ºr den Transport.\"\n",
    "]\n",
    "\n",
    "print(\"Test Documents:\")\n",
    "for i, doc in enumerate(test_documents):\n",
    "    print(f\"{i+1}. {doc}\")\n",
    "print()\n",
    "\n",
    "# Calculate document similarities\n",
    "doc_similarities = document_similarity_with_embeddings(w2v_model, test_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2329cb9",
   "metadata": {},
   "source": [
    "## Exercise Tasks\n",
    "\n",
    "Complete the following tasks to deepen your understanding:\n",
    "\n",
    "1. **Embedding Quality Analysis**:\n",
    "   - Load larger pre-trained German embeddings (e.g., from deepset.ai)\n",
    "   - Compare quality on word similarity benchmarks\n",
    "   - Analyze out-of-vocabulary handling with FastText\n",
    "\n",
    "2. **Custom Domain Embeddings**:\n",
    "   - Collect domain-specific German text (news, medical, legal)\n",
    "   - Train specialized embeddings for your domain\n",
    "   - Compare with general-purpose embeddings\n",
    "\n",
    "3. **Embedding Arithmetic**:\n",
    "   - Explore more complex analogies and relationships\n",
    "   - Test cultural and linguistic biases in embeddings\n",
    "   - Implement bias detection and mitigation\n",
    "\n",
    "4. **Application Development**:\n",
    "   - Build a semantic search engine using embeddings\n",
    "   - Create a document clustering system\n",
    "   - Implement recommendation systems with word embeddings\n",
    "\n",
    "5. **Evaluation Framework**:\n",
    "   - Create systematic evaluation metrics\n",
    "   - Benchmark different embedding methods\n",
    "   - Develop intrinsic and extrinsic evaluation tasks\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. What are the main advantages of dense embeddings over sparse representations?\n",
    "2. When would you choose Word2Vec CBOW vs Skip-gram?\n",
    "3. How do German language characteristics affect embedding quality?\n",
    "4. What are the limitations of static word embeddings?\n",
    "5. How can you evaluate embedding quality without labeled data?\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Study contextual embeddings (BERT, ELMo) in the next topic\n",
    "- Explore multilingual embeddings for cross-language tasks\n",
    "- Learn about sentence and document embeddings\n",
    "- Investigate embedding fine-tuning for specific tasks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
