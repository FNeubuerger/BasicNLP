{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f70c099",
   "metadata": {},
   "source": [
    "# Exercise 0: Regex and Text Processing Basics\n",
    "\n",
    "Welcome to your first NLP exercise! In this notebook, you'll learn the fundamental skills for working with text data.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this exercise, you will be able to:\n",
    "1. **Basic Text Operations**: Clean and normalize text data\n",
    "2. **Regular Expressions**: Find and extract patterns from text\n",
    "3. **German Text Processing**: Handle German language specifics (umlauts, compound words)\n",
    "4. **Data Extraction**: Extract emails, phone numbers, dates, and URLs from text\n",
    "5. **Text Validation**: Check if text matches specific patterns\n",
    "6. **Advanced Cleaning**: Build comprehensive text preprocessing pipelines\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge (strings, functions, loops)\n",
    "- No prior regex experience needed - we'll learn together!\n",
    "\n",
    "## What You'll Build\n",
    "- Text cleaning functions for German text\n",
    "- Pattern extraction tools (emails, phones, dates)\n",
    "- Text validation systems\n",
    "- A complete text preprocessing pipeline\n",
    "\n",
    "**Ready to become a text processing expert?** Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea7444",
   "metadata": {},
   "source": [
    "## Exercise 1: Basic Text Cleaning\n",
    "\n",
    "**Goal**: Learn fundamental text cleaning operations that form the foundation of all text processing tasks.\n",
    "\n",
    "**Your Task**: Implement basic text cleaning functions using both simple string methods and regular expressions.\n",
    "\n",
    "### Setup and Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b7c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Sample German text with various issues (use this for testing your functions)\n",
    "sample_german_text = \"\"\"\n",
    "    Das ist ein   Beispieltext!!!  Er enth√§lt GROSSBUCHSTABEN, \n",
    "    Zahlen wie 123, E-Mails wie test@uni-berlin.de,\n",
    "    URLs wie https://www.example.com und Sonderzeichen: @#$%!\n",
    "    Deutsche Umlaute: √§√∂√º√Ñ√ñ√ú√ü sind wichtig!   \n",
    "    \n",
    "    Telefonnummer: 030-12345678\n",
    "    Datum: 15.03.2025\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìÑ Sample Text to Work With:\")\n",
    "print(repr(sample_german_text))  # repr() shows whitespace and special characters\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce99b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1a: Basic String Cleaning\n",
    "def clean_basic_text(text):\n",
    "    \"\"\"\n",
    "    Clean text using basic string methods.\n",
    "    \n",
    "    Your task: Implement the following cleaning steps:\n",
    "    1. Remove leading/trailing whitespace\n",
    "    2. Replace multiple spaces with single spaces\n",
    "    3. Convert to lowercase\n",
    "    4. Remove common punctuation (but keep German umlauts!)\n",
    "    \n",
    "    Hints:\n",
    "    - Use .strip() to remove leading/trailing whitespace\n",
    "    - Use re.sub(r'\\s+', ' ', text) to replace multiple spaces\n",
    "    - Use .lower() for lowercase conversion\n",
    "    - For punctuation, use string.punctuation but be careful with German characters\n",
    "    - You can use .translate() with str.maketrans() for punctuation removal\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement your cleaning steps here\n",
    "    # Step 1: Remove leading/trailing whitespace\n",
    "    \n",
    "    # Step 2: Replace multiple whitespaces with single space\n",
    "    \n",
    "    # Step 3: Convert to lowercase\n",
    "    \n",
    "    # Step 4: Remove punctuation (keep German umlauts)\n",
    "    \n",
    "    pass  # Remove this when you implement the function\n",
    "\n",
    "# Test your function (uncomment after implementing)\n",
    "# cleaned_basic = clean_basic_text(sample_german_text)\n",
    "# print(\"Basic cleaning result:\")\n",
    "# print(repr(cleaned_basic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1710f3c",
   "metadata": {},
   "source": [
    "## Exercise 2: Introduction to Regular Expressions\n",
    "\n",
    "**Goal**: Learn regex basics for powerful pattern matching and text manipulation.\n",
    "\n",
    "### Regex Basics - Essential Patterns\n",
    "\n",
    "Before diving into exercises, here are the key regex patterns you'll need:\n",
    "\n",
    "**Basic Characters:**\n",
    "- `.` : Matches any single character\n",
    "- `*` : Matches 0 or more repetitions  \n",
    "- `+` : Matches 1 or more repetitions\n",
    "- `?` : Matches 0 or 1 repetition\n",
    "- `{n}` : Matches exactly n repetitions\n",
    "- `{n,m}` : Matches n to m repetitions\n",
    "\n",
    "**Character Classes:**\n",
    "- `[abc]` : Matches a, b, or c\n",
    "- `[a-z]` : Matches any lowercase letter\n",
    "- `[A-Z]` : Matches any uppercase letter  \n",
    "- `[0-9]` : Matches any digit\n",
    "- `\\d` : Matches any digit (equivalent to [0-9])\n",
    "- `\\w` : Matches word characters (letters, digits, underscore)\n",
    "- `\\s` : Matches whitespace characters\n",
    "\n",
    "**Anchors:**\n",
    "- `^` : Start of string\n",
    "- `$` : End of string\n",
    "- `\\b` : Word boundary\n",
    "\n",
    "**Special for German:**\n",
    "- `[a-zA-Z√§√∂√º√Ñ√ñ√ú√ü]` : German letters including umlauts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f949a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2a: Pattern Recognition Practice\n",
    "\n",
    "def find_phone_numbers(text):\n",
    "    \"\"\"\n",
    "    Find German phone numbers in text.\n",
    "    \n",
    "    Your task: Write a regex pattern to find phone numbers\n",
    "    \n",
    "    German phone formats to match:\n",
    "    - 030-12345678 (area code with dash)\n",
    "    - 030 12345678 (area code with space)  \n",
    "    - +49 30 12345678 (international format)\n",
    "    - (030) 12345678 (area code in parentheses)\n",
    "    \n",
    "    Hints:\n",
    "    - \\d matches digits\n",
    "    - {n} matches exactly n repetitions\n",
    "    - {n,m} matches n to m repetitions\n",
    "    - [-\\s] matches dash or space\n",
    "    - \\+ matches literal plus sign\n",
    "    - [\\(\\)] matches parentheses (need to escape them)\n",
    "    - Use | for alternatives: (pattern1|pattern2)\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to search in\n",
    "        \n",
    "    Returns:\n",
    "        list: List of found phone numbers\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Write your regex pattern here\n",
    "    # Hint: Start simple with one format, then expand\n",
    "    phone_pattern = r''  # Your pattern goes here\n",
    "    \n",
    "    # TODO: Use re.findall to find all matches\n",
    "    # phones = re.findall(phone_pattern, text)\n",
    "    # return phones\n",
    "    \n",
    "    pass  # Remove this when you implement\n",
    "\n",
    "def find_email_addresses(text):\n",
    "    \"\"\"\n",
    "    Find email addresses in text.\n",
    "    \n",
    "    Your task: Write a regex to match email addresses\n",
    "    \n",
    "    Email format: username@domain.extension\n",
    "    - Username: letters, numbers, dots, underscores, hyphens\n",
    "    - Domain: letters, numbers, dots, hyphens\n",
    "    - Extension: 2-4 letters\n",
    "    \n",
    "    Hints:\n",
    "    - [A-Za-z0-9._-] matches valid username characters\n",
    "    - + means one or more\n",
    "    - @ matches literal @ symbol\n",
    "    - \\. matches literal dot (. is special in regex)\n",
    "    - {2,4} matches 2 to 4 repetitions\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to search in\n",
    "        \n",
    "    Returns:\n",
    "        list: List of found email addresses\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Write your email pattern\n",
    "    email_pattern = r''  # Your pattern goes here\n",
    "    \n",
    "    # TODO: Find and return emails\n",
    "    pass\n",
    "\n",
    "# Test data for your functions\n",
    "test_text = \"\"\"\n",
    "Kontakt: Max Mustermann\n",
    "Telefon: 030-12345678 oder (030) 87654321\n",
    "Mobil: +49 175 1234567\n",
    "E-Mail: max.mustermann@uni-berlin.de\n",
    "Backup: support@example.com\n",
    "\"\"\"\n",
    "\n",
    "# Test your functions (uncomment after implementing)\n",
    "# phones = find_phone_numbers(test_text)\n",
    "# emails = find_email_addresses(test_text)\n",
    "# print(\"Found phones:\", phones)\n",
    "# print(\"Found emails:\", emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf1f4c2",
   "metadata": {},
   "source": [
    "## Exercise 3: Text Validation and Advanced Cleaning  \n",
    "\n",
    "**Goal**: Use regex for validation and text replacement operations.\n",
    "\n",
    "**Your Tasks**: Build validators for German data formats and text cleaners.\n",
    "\n",
    "### Key Regex Functions You'll Use:\n",
    "- `re.search()`: Find first match\n",
    "- `re.match()`: Match at beginning of string  \n",
    "- `re.findall()`: Find all matches\n",
    "- `re.sub()`: Replace matches\n",
    "- `re.fullmatch()`: Check if entire string matches pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84becacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different regex functions\n",
    "sample_text = \"Das Meeting ist am 15.03.2025 um 14:30 Uhr. N√§chstes Meeting: 22.03.2025 um 10:00 Uhr.\"\n",
    "\n",
    "# re.search() - Find first occurrence\n",
    "date_pattern = r'\\d{2}\\.\\d{2}\\.\\d{4}'\n",
    "first_date = re.search(date_pattern, sample_text)\n",
    "if first_date:\n",
    "    print(f\"Erstes Datum gefunden: {first_date.group()}\")\n",
    "    print(f\"Position: {first_date.span()}\")\n",
    "\n",
    "# re.findall() - Find all occurrences\n",
    "all_dates = re.findall(date_pattern, sample_text)\n",
    "print(f\"\\nAlle Daten: {all_dates}\")\n",
    "\n",
    "# re.finditer() - Iterator over all matches\n",
    "print(\"\\nDetaillierte Informationen zu allen Daten:\")\n",
    "for match in re.finditer(date_pattern, sample_text):\n",
    "    print(f\"  Datum: {match.group()}, Position: {match.span()}\")\n",
    "\n",
    "# re.sub() - Replace text\n",
    "censored = re.sub(date_pattern, '[DATUM ENTFERNT]', sample_text)\n",
    "print(f\"\\nZensierter Text: {censored}\")\n",
    "\n",
    "# re.split() - Split by pattern\n",
    "sentences = re.split(r'\\. ', sample_text)\n",
    "print(f\"\\nS√§tze: {sentences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56253787",
   "metadata": {},
   "source": [
    "## Part 2: Text Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f97e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample German text with various issues\n",
    "raw_text = \"\"\"\n",
    "    Das ist ein   Beispieltext!!!  \n",
    "    Er enth√§lt GROSSBUCHSTABEN, Zahlen wie 123, E-Mails wie test@example.com,\n",
    "    URLs wie https://www.example.com und Sonderzeichen: @#$%!\n",
    "    \n",
    "    Es gibt auch    mehrfache    Leerzeichen und\n",
    "    Zeilenumbr√ºche.\n",
    "    \n",
    "    Deutsche Umlaute: √§√∂√º√Ñ√ñ√ú√ü sind wichtig!\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f04bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, \n",
    "               lowercase=True, \n",
    "               remove_urls=True, \n",
    "               remove_emails=True,\n",
    "               remove_numbers=False,\n",
    "               remove_punctuation=False,\n",
    "               remove_extra_whitespace=True):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "        lowercase (bool): Convert to lowercase\n",
    "        remove_urls (bool): Remove URLs\n",
    "        remove_emails (bool): Remove email addresses\n",
    "        remove_numbers (bool): Remove numbers\n",
    "        remove_punctuation (bool): Remove punctuation\n",
    "        remove_extra_whitespace (bool): Remove extra whitespace\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    if remove_urls:\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    if remove_emails:\n",
    "        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation (but keep German umlauts)\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s\\u00C0-\\u017F]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    if remove_extra_whitespace:\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test cleaning function\n",
    "cleaned = clean_text(raw_text)\n",
    "print(\"Gereinigter Text:\")\n",
    "print(cleaned)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Mit verschiedenen Optionen:\")\n",
    "print(\"\\nOhne Zahlen:\")\n",
    "print(clean_text(raw_text, remove_numbers=True))\n",
    "\n",
    "print(\"\\nOhne Satzzeichen:\")\n",
    "print(clean_text(raw_text, remove_punctuation=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2eb992",
   "metadata": {},
   "source": [
    "## Part 3: Pattern Extraction and Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b2d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text with various entities\n",
    "sample_document = \"\"\"\n",
    "Prof. Dr. M√ºller lehrt an der Akademie f√ºr Wissenschaften. \n",
    "Sie k√∂nnen ihn unter mueller@akademie-wissen.de oder +49-30-1234-56789 erreichen.\n",
    "Die Vorlesung findet am 15.03.2025 um 14:00 Uhr in Raum B456 statt.\n",
    "Die Teilnahmegeb√ºhr betr√§gt 150,00 EUR. \n",
    "Weitere Informationen finden Sie unter https://www.akademie-wissen.de/vorlesungen.\n",
    "Anmeldeschluss ist der 01.03.2025.\n",
    "\"\"\"\n",
    "\n",
    "def extract_entities(text):\n",
    "    \"\"\"\n",
    "    Extract various entities from text using regex.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        dict: Extracted entities\n",
    "    \"\"\"\n",
    "    entities = {}\n",
    "    \n",
    "    # Extract email addresses\n",
    "    entities['emails'] = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "    \n",
    "    # Extract phone numbers (German format)\n",
    "    entities['phones'] = re.findall(r'\\+\\d{2}-\\d{2,3}-\\d{3,4}-\\d{4,5}', text)\n",
    "    \n",
    "    # Extract dates (German format: DD.MM.YYYY)\n",
    "    entities['dates'] = re.findall(r'\\d{2}\\.\\d{2}\\.\\d{4}', text)\n",
    "    \n",
    "    # Extract times (HH:MM format)\n",
    "    entities['times'] = re.findall(r'\\d{1,2}:\\d{2}', text)\n",
    "    \n",
    "    # Extract URLs\n",
    "    entities['urls'] = re.findall(r'https?://[^\\s]+', text)\n",
    "    \n",
    "    # Extract room numbers (pattern: A123, B456, etc.)\n",
    "    entities['rooms'] = re.findall(r'\\b[A-Z]\\d{3}\\b', text)\n",
    "    \n",
    "    # Extract prices (EUR format)\n",
    "    entities['prices'] = re.findall(r'\\d+,\\d{2}\\s*EUR', text)\n",
    "    \n",
    "    # Extract titles (Prof., Dr., etc.)\n",
    "    entities['titles'] = re.findall(r'\\b(Prof\\.|Dr\\.|Dipl\\.-Ing\\.)\\s+', text)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Extract entities\n",
    "extracted = extract_entities(sample_document)\n",
    "\n",
    "print(\"Extrahierte Entit√§ten:\")\n",
    "print(\"=\"*50)\n",
    "for entity_type, values in extracted.items():\n",
    "    if values:\n",
    "        print(f\"\\n{entity_type.upper()}:\")\n",
    "        for value in values:\n",
    "            print(f\"  - {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d4639",
   "metadata": {},
   "source": [
    "## Part 4: Text Tokenization\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units (tokens) such as words, sentences, or subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c9228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    \"\"\"\n",
    "    Simple word tokenization using regex.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        list: List of word tokens\n",
    "    \"\"\"\n",
    "    # Match word characters including German umlauts\n",
    "    tokens = re.findall(r'\\b[\\w\\u00C0-\\u017F]+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    \"\"\"\n",
    "    Simple sentence tokenization using regex.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        list: List of sentence tokens\n",
    "    \"\"\"\n",
    "    # Split on sentence-ending punctuation\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    # Clean up and remove empty strings\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# Test tokenization\n",
    "sample = \"Das ist ein Satz! Und hier ist noch einer. Was f√ºr ein sch√∂ner Tag?\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(sample)\n",
    "\n",
    "print(\"\\nWort-Tokenisierung:\")\n",
    "word_tokens = tokenize_words(sample)\n",
    "print(word_tokens)\n",
    "print(f\"Anzahl Tokens: {len(word_tokens)}\")\n",
    "\n",
    "print(\"\\nSatz-Tokenisierung:\")\n",
    "sentence_tokens = tokenize_sentences(sample)\n",
    "for i, sent in enumerate(sentence_tokens, 1):\n",
    "    print(f\"  {i}. {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302d020",
   "metadata": {},
   "source": [
    "## Part 5: Text Statistics and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ead80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    \"\"\"\n",
    "    Perform basic statistical analysis on text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        dict: Text statistics\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # Basic counts\n",
    "    stats['total_chars'] = len(text)\n",
    "    stats['total_chars_no_spaces'] = len(re.sub(r'\\s', '', text))\n",
    "    \n",
    "    # Word statistics\n",
    "    words = tokenize_words(text)\n",
    "    stats['total_words'] = len(words)\n",
    "    stats['unique_words'] = len(set(words))\n",
    "    stats['avg_word_length'] = sum(len(word) for word in words) / len(words) if words else 0\n",
    "    \n",
    "    # Sentence statistics\n",
    "    sentences = tokenize_sentences(text)\n",
    "    stats['total_sentences'] = len(sentences)\n",
    "    stats['avg_words_per_sentence'] = len(words) / len(sentences) if sentences else 0\n",
    "    \n",
    "    # Most common words\n",
    "    word_freq = Counter(words)\n",
    "    stats['most_common_words'] = word_freq.most_common(10)\n",
    "    \n",
    "    # Count digits\n",
    "    stats['digit_count'] = len(re.findall(r'\\d', text))\n",
    "    \n",
    "    # Count uppercase letters\n",
    "    stats['uppercase_count'] = len(re.findall(r'[A-Z]', text))\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Analyze sample text\n",
    "analysis_text = \"\"\"\n",
    "Natural Language Processing (NLP) ist ein spannendes Forschungsgebiet der Informatik.\n",
    "Es kombiniert Linguistik, maschinelles Lernen und k√ºnstliche Intelligenz.\n",
    "Mit NLP k√∂nnen Computer menschliche Sprache verstehen und verarbeiten.\n",
    "Anwendungen umfassen Chatbots, maschinelle √úbersetzung und Sentimentanalyse.\n",
    "Die Entwicklung von NLP hat in den letzten Jahren enorme Fortschritte gemacht.\n",
    "\"\"\"\n",
    "\n",
    "stats = analyze_text(analysis_text)\n",
    "\n",
    "print(\"Text-Analyse:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Gesamtzeichen: {stats['total_chars']}\")\n",
    "print(f\"Zeichen ohne Leerzeichen: {stats['total_chars_no_spaces']}\")\n",
    "print(f\"\\nGesamtw√∂rter: {stats['total_words']}\")\n",
    "print(f\"Eindeutige W√∂rter: {stats['unique_words']}\")\n",
    "print(f\"Durchschnittliche Wortl√§nge: {stats['avg_word_length']:.2f}\")\n",
    "print(f\"\\nGesamts√§tze: {stats['total_sentences']}\")\n",
    "print(f\"Durchschnittliche W√∂rter pro Satz: {stats['avg_words_per_sentence']:.2f}\")\n",
    "print(f\"\\nZiffern: {stats['digit_count']}\")\n",
    "print(f\"Gro√übuchstaben: {stats['uppercase_count']}\")\n",
    "print(f\"\\nH√§ufigste W√∂rter:\")\n",
    "for word, count in stats['most_common_words']:\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388199c",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Pattern Matching Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9a6d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# German-specific patterns\n",
    "def validate_german_patterns(text):\n",
    "    \"\"\"\n",
    "    Validate various German text patterns.\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        'postal_code': r'\\b\\d{5}\\b',  # German postal codes\n",
    "        'iban': r'\\bDE\\d{20}\\b',  # German IBAN (simplified)\n",
    "        'license_plate': r'\\b[A-Z√Ñ√ñ√ú]{1,3}-[A-Z√Ñ√ñ√ú]{1,2}\\s?\\d{1,4}\\b',  # German license plates\n",
    "        'academic_titles': r'\\b(Prof\\.|Dr\\.|Dipl\\.-Ing\\.)\\s+',\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for pattern_name, pattern in patterns.items():\n",
    "        matches = re.findall(pattern, text)\n",
    "        if matches:\n",
    "            results[pattern_name] = matches\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test German patterns\n",
    "german_text = \"\"\"\n",
    "Prof. Dr. M√ºller wohnt in 10115 Berlin.\n",
    "Seine IBAN lautet DE89370400440532013000.\n",
    "Das Auto mit dem Kennzeichen B-MW 1234 geh√∂rt ihm.\n",
    "Dr. Schmidt hat einen M.Sc. in Informatik.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Deutsche Muster-Erkennung:\")\n",
    "print(\"=\"*50)\n",
    "found_patterns = validate_german_patterns(german_text)\n",
    "for pattern_type, matches in found_patterns.items():\n",
    "    print(f\"\\n{pattern_type.upper()}:\")\n",
    "    for match in matches:\n",
    "        print(f\"  - {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c952f437",
   "metadata": {},
   "source": [
    "## Part 7: Building a Text Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428751dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    \"\"\"\n",
    "    A comprehensive text processing pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.processing_steps = []\n",
    "    \n",
    "    def add_step(self, step_name, step_function):\n",
    "        \"\"\"Add a processing step to the pipeline.\"\"\"\n",
    "        self.processing_steps.append((step_name, step_function))\n",
    "        return self\n",
    "    \n",
    "    def process(self, text, verbose=True):\n",
    "        \"\"\"Process text through all steps in the pipeline.\"\"\"\n",
    "        if verbose:\n",
    "            print(\"Text Processing Pipeline\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Original text length: {len(text)} characters\\n\")\n",
    "        \n",
    "        current_text = text\n",
    "        \n",
    "        for step_name, step_function in self.processing_steps:\n",
    "            current_text = step_function(current_text)\n",
    "            if verbose:\n",
    "                print(f\"After {step_name}:\")\n",
    "                print(f\"  Length: {len(current_text)} characters\")\n",
    "                print(f\"  Preview: {current_text[:100]}...\\n\")\n",
    "        \n",
    "        return current_text\n",
    "\n",
    "# Define processing functions\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def remove_emails(text):\n",
    "    return re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    return re.sub(r'[^\\w\\s\\u00C0-\\u017F]', '', text)\n",
    "\n",
    "# Create and configure pipeline\n",
    "pipeline = TextProcessor()\n",
    "pipeline.add_step(\"URL Removal\", remove_urls)\n",
    "pipeline.add_step(\"Email Removal\", remove_emails)\n",
    "pipeline.add_step(\"Lowercase Conversion\", to_lowercase)\n",
    "pipeline.add_step(\"Special Character Removal\", remove_special_chars)\n",
    "pipeline.add_step(\"Whitespace Normalization\", normalize_whitespace)\n",
    "\n",
    "# Test pipeline\n",
    "test_text = \"\"\"\n",
    "    Besuchen Sie unsere Website unter https://www.example.com!\n",
    "    Kontaktieren Sie uns: info@example.com\n",
    "    WICHTIGE INFORMATIONEN √ºber NLP und maschinelles Lernen!!!\n",
    "    Es gibt    viele    Leerzeichen    hier.\n",
    "\"\"\"\n",
    "\n",
    "processed_text = pipeline.process(test_text)\n",
    "\n",
    "print(\"\\nFinal Result:\")\n",
    "print(\"=\"*50)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6caaf2f",
   "metadata": {},
   "source": [
    "## Part 8: Practical Exercise - Document Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bab8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document(text):\n",
    "    \"\"\"\n",
    "    Comprehensive document analysis combining all techniques.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input document\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Extract entities\n",
    "    results['entities'] = extract_entities(text)\n",
    "    \n",
    "    # Text statistics\n",
    "    results['statistics'] = analyze_text(text)\n",
    "    \n",
    "    # Pattern validation\n",
    "    results['german_patterns'] = validate_german_patterns(text)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Sample document for analysis\n",
    "document = \"\"\"\n",
    "Sehr geehrte Damen und Herren,\n",
    "\n",
    "hiermit lade ich Sie zur Konferenz \"NLP in der Praxis\" ein.\n",
    "Die Veranstaltung findet am 15.05.2025 um 09:00 Uhr statt.\n",
    "Ort: Freie Universit√§t Berlin, Raum A123, 14195 Berlin.\n",
    "\n",
    "Keynote-Speaker: Prof. Dr. Anna Schmidt (anna.schmidt@fu-berlin.de)\n",
    "Thema: \"Moderne Ans√§tze im Natural Language Processing\"\n",
    "\n",
    "Anmeldung unter: https://www.nlp-konferenz.de\n",
    "R√ºckfragen an: info@nlp-konferenz.de oder +49-30-838-12345\n",
    "\n",
    "Teilnahmegeb√ºhr: 299,00 EUR\n",
    "IBAN f√ºr √úberweisung: DE89370400440532013000\n",
    "\n",
    "Mit freundlichen Gr√º√üen,\n",
    "Dr. Max M√ºller\n",
    "\"\"\"\n",
    "\n",
    "print(\"Dokumenten-Analyse:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nOriginaldokument:\")\n",
    "print(document)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "analysis = analyze_document(document)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìß EXTRAHIERTE ENTIT√ÑTEN:\")\n",
    "print(\"-\"*70)\n",
    "for entity_type, values in analysis['entities'].items():\n",
    "    if values:\n",
    "        print(f\"\\n{entity_type.upper()}:\")\n",
    "        for value in values:\n",
    "            print(f\"  ‚úì {value}\")\n",
    "\n",
    "print(\"\\n\\nüìä STATISTISCHE ANALYSE:\")\n",
    "print(\"-\"*70)\n",
    "stats = analysis['statistics']\n",
    "print(f\"W√∂rter: {stats['total_words']} (davon {stats['unique_words']} eindeutig)\")\n",
    "print(f\"S√§tze: {stats['total_sentences']}\")\n",
    "print(f\"Zeichen: {stats['total_chars']}\")\n",
    "print(f\"Durchschnittliche Wortl√§nge: {stats['avg_word_length']:.2f}\")\n",
    "print(f\"Durchschnittliche W√∂rter pro Satz: {stats['avg_words_per_sentence']:.2f}\")\n",
    "\n",
    "print(\"\\n\\nüá©üá™ DEUTSCHE MUSTER:\")\n",
    "print(\"-\"*70)\n",
    "for pattern_type, matches in analysis['german_patterns'].items():\n",
    "    print(f\"\\n{pattern_type.upper()}:\")\n",
    "    for match in matches:\n",
    "        print(f\"  ‚úì {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba82c37",
   "metadata": {},
   "source": [
    "## Exercise Tasks\n",
    "\n",
    "Complete the following tasks to practice your regex and text processing skills:\n",
    "\n",
    "1. **Pattern Creation**:\n",
    "   - Create a regex pattern to extract German street addresses\n",
    "   - Write a pattern to find all capitalized words (potential proper nouns)\n",
    "   - Develop a pattern for German phone numbers in various formats\n",
    "\n",
    "2. **Text Cleaning**:\n",
    "   - Build a function to remove HTML tags from text\n",
    "   - Create a function to normalize different quotation marks\n",
    "   - Implement a function to expand common German abbreviations\n",
    "\n",
    "3. **Information Extraction**:\n",
    "   - Extract all monetary amounts from a text (EUR, $, etc.)\n",
    "   - Find and categorize all numbers (integers, floats, percentages)\n",
    "   - Extract compound words (typical in German)\n",
    "\n",
    "4. **Text Validation**:\n",
    "   - Validate German postal codes\n",
    "   - Check if text contains proper sentence structure\n",
    "   - Identify potential spelling errors using pattern matching\n",
    "\n",
    "5. **Advanced Pipeline**:\n",
    "   - Create a pipeline that anonymizes personal information\n",
    "   - Build a text normalizer for social media content\n",
    "   - Develop a preprocessing pipeline for sentiment analysis\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. When should you use regex vs. specialized NLP libraries?\n",
    "2. What are the limitations of regex for text processing?\n",
    "3. How can regex patterns be optimized for performance?\n",
    "4. Why is text preprocessing important for NLP tasks?\n",
    "5. What challenges are specific to German text processing?\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Proceed to **Notebook 01**: Introduction to NLP and Text Processing\n",
    "- Explore advanced tokenization with NLTK and spaCy\n",
    "- Learn about stemming and lemmatization\n",
    "- Study language-specific text processing challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd482fd1",
   "metadata": {},
   "source": [
    "## Exercise 1: Basic Text Cleaning\n",
    "\n",
    "**Goal**: Learn fundamental text cleaning operations that form the foundation of all text processing tasks.\n",
    "\n",
    "**Your Task**: Implement basic text cleaning functions using both simple string methods and regular expressions.\n",
    "\n",
    "### Setup and Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Sample German text with various issues (use this for testing your functions)\n",
    "sample_german_text = \"\"\"\n",
    "    Das ist ein   Beispieltext!!!  Er enth√§lt GROSSBUCHSTABEN, \n",
    "    Zahlen wie 123, E-Mails wie test@uni-berlin.de,\n",
    "    URLs wie https://www.example.com und Sonderzeichen: @#$%!\n",
    "    Deutsche Umlaute: √§√∂√º√Ñ√ñ√ú√ü sind wichtig!   \n",
    "    \n",
    "    Telefonnummer: 030-12345678\n",
    "    Datum: 15.03.2025\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìÑ Sample Text to Work With:\")\n",
    "print(repr(sample_german_text))  # repr() shows whitespace and special characters\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1a: Basic String Cleaning\n",
    "def clean_basic_text(text):\n",
    "    \"\"\"\n",
    "    Clean text using basic string methods.\n",
    "    \n",
    "    Your task: Implement the following cleaning steps:\n",
    "    1. Remove leading/trailing whitespace\n",
    "    2. Replace multiple spaces with single spaces\n",
    "    3. Convert to lowercase\n",
    "    4. Remove common punctuation (but keep German umlauts!)\n",
    "    \n",
    "    Hints:\n",
    "    - Use .strip() to remove leading/trailing whitespace\n",
    "    - Use re.sub(r'\\s+', ' ', text) to replace multiple spaces\n",
    "    - Use .lower() for lowercase conversion\n",
    "    - For punctuation, use string.punctuation but be careful with German characters\n",
    "    - You can use .translate() with str.maketrans() for punctuation removal\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement your cleaning steps here\n",
    "    # Step 1: Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # Step 2: Replace multiple whitespaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Step 3: Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 4: Remove punctuation (keep German umlauts)\n",
    "    # Create a translation table that maps each punctuation character to None\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    return text  # Return the cleaned text\n",
    "\n",
    "# Test your function (uncomment after implementing)\n",
    "cleaned_basic = clean_basic_text(sample_german_text)\n",
    "print(\"Basic cleaning result:\")\n",
    "print(repr(cleaned_basic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d3097e",
   "metadata": {},
   "source": [
    "## Exercise 2: Introduction to Regular Expressions\n",
    "\n",
    "**Goal**: Learn regex basics for powerful pattern matching and text manipulation.\n",
    "\n",
    "### Regex Basics - Essential Patterns\n",
    "\n",
    "Before diving into exercises, here are the key regex patterns you'll need:\n",
    "\n",
    "**Basic Characters:**\n",
    "- `.` : Matches any single character\n",
    "- `*` : Matches 0 or more repetitions  \n",
    "- `+` : Matches 1 or more repetitions\n",
    "- `?` : Matches 0 or 1 repetition\n",
    "- `{n}` : Matches exactly n repetitions\n",
    "- `{n,m}` : Matches n to m repetitions\n",
    "\n",
    "**Character Classes:**\n",
    "- `[abc]` : Matches a, b, or c\n",
    "- `[a-z]` : Matches any lowercase letter\n",
    "- `[A-Z]` : Matches any uppercase letter  \n",
    "- `[0-9]` : Matches any digit\n",
    "- `\\d` : Matches any digit (equivalent to [0-9])\n",
    "- `\\w` : Matches word characters (letters, digits, underscore)\n",
    "- `\\s` : Matches whitespace characters\n",
    "\n",
    "**Anchors:**\n",
    "- `^` : Start of string\n",
    "- `$` : End of string\n",
    "- `\\b` : Word boundary\n",
    "\n",
    "**Special for German:**\n",
    "- `[a-zA-Z√§√∂√º√Ñ√ñ√ú√ü]` : German letters including umlauts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ae243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2a: Pattern Recognition Practice\n",
    "\n",
    "def find_phone_numbers(text):\n",
    "    \"\"\"\n",
    "    Find German phone numbers in text.\n",
    "    \n",
    "    Your task: Write a regex pattern to find phone numbers\n",
    "    \n",
    "    German phone formats to match:\n",
    "    - 030-12345678 (area code with dash)\n",
    "    - 030 12345678 (area code with space)  \n",
    "    - +49 30 12345678 (international format)\n",
    "    - (030) 12345678 (area code in parentheses)\n",
    "    \n",
    "    Hints:\n",
    "    - \\d matches digits\n",
    "    - {n} matches exactly n repetitions\n",
    "    - {n,m} matches n to m repetitions\n",
    "    - [-\\s] matches dash or space\n",
    "    - \\+ matches literal plus sign\n",
    "    - [\\(\\)] matches parentheses (need to escape them)\n",
    "    - Use | for alternatives: (pattern1|pattern2)\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to search in\n",
    "        \n",
    "    Returns:\n",
    "        list: List of found phone numbers\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Write your regex pattern here\n",
    "    # Hint: Start simple with one format, then expand\n",
    "    phone_pattern = r'(\\+49\\s?|0)(30|40|70|80|90)?[-\\s]?\\d{3,4}[-\\s]?\\d{4}'  # Your pattern goes here\n",
    "    \n",
    "    # TODO: Use re.findall to find all matches\n",
    "    phones = re.findall(phone_pattern, text)\n",
    "    return phones\n",
    "    \n",
    "    pass  # Remove this when you implement\n",
    "\n",
    "def find_email_addresses(text):\n",
    "    \"\"\"\n",
    "    Find email addresses in text.\n",
    "    \n",
    "    Your task: Write a regex to match email addresses\n",
    "    \n",
    "    Email format: username@domain.extension\n",
    "    - Username: letters, numbers, dots, underscores, hyphens\n",
    "    - Domain: letters, numbers, dots, hyphens\n",
    "    - Extension: 2-4 letters\n",
    "    \n",
    "    Hints:\n",
    "    - [A-Za-z0-9._-] matches valid username characters\n",
    "    - + means one or more\n",
    "    - @ matches literal @ symbol\n",
    "    - \\. matches literal dot (. is special in regex)\n",
    "    - {2,4} matches 2 to 4 repetitions\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to search in\n",
    "        \n",
    "    Returns:\n",
    "        list: List of found email addresses\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Write your email pattern\n",
    "    email_pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,4}'  # Your pattern goes here\n",
    "    \n",
    "    # TODO: Find and return emails\n",
    "    emails = re.findall(email_pattern, text)\n",
    "    return emails\n",
    "\n",
    "# Test data for your functions\n",
    "test_text = \"\"\"\n",
    "Kontakt: Max Mustermann\n",
    "Telefon: 030-12345678 oder (030) 87654321\n",
    "Mobil: +49 175 1234567\n",
    "E-Mail: max.mustermann@uni-berlin.de\n",
    "Backup: support@example.com\n",
    "\"\"\"\n",
    "\n",
    "# Test your functions (uncomment after implementing)\n",
    "phones = find_phone_numbers(test_text)\n",
    "emails = find_email_addresses(test_text)\n",
    "print(\"Found phones:\", phones)\n",
    "print(\"Found emails:\", emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a235e365",
   "metadata": {},
   "source": [
    "### Regex Functions in Python\n",
    "\n",
    "Python's `re` module provides several key functions:\n",
    "- `re.search()`: Find first match\n",
    "- `re.match()`: Match at beginning of string\n",
    "- `re.findall()`: Find all matches\n",
    "- `re.finditer()`: Find all matches (returns iterator)\n",
    "- `re.sub()`: Replace matches\n",
    "- `re.split()`: Split string by pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd8049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different regex functions\n",
    "sample_text = \"Das Meeting ist am 15.03.2025 um 14:30 Uhr. N√§chstes Meeting: 22.03.2025 um 10:00 Uhr.\"\n",
    "\n",
    "# re.search() - Find first occurrence\n",
    "date_pattern = r'\\d{2}\\.\\d{2}\\.\\d{4}'\n",
    "first_date = re.search(date_pattern, sample_text)\n",
    "if first_date:\n",
    "    print(f\"Erstes Datum gefunden: {first_date.group()}\")\n",
    "    print(f\"Position: {first_date.span()}\")\n",
    "\n",
    "# re.findall() - Find all occurrences\n",
    "all_dates = re.findall(date_pattern, sample_text)\n",
    "print(f\"\\nAlle Daten: {all_dates}\")\n",
    "\n",
    "# re.finditer() - Iterator over all matches\n",
    "print(\"\\nDetaillierte Informationen zu allen Daten:\")\n",
    "for match in re.finditer(date_pattern, sample_text):\n",
    "    print(f\"  Datum: {match.group()}, Position: {match.span()}\")\n",
    "\n",
    "# re.sub() - Replace text\n",
    "censored = re.sub(date_pattern, '[DATUM ENTFERNT]', sample_text)\n",
    "print(f\"\\nZensierter Text: {censored}\")\n",
    "\n",
    "# re.split() - Split by pattern\n",
    "sentences = re.split(r'\\. ', sample_text)\n",
    "print(f\"\\nS√§tze: {sentences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2169a673",
   "metadata": {},
   "source": [
    "## Part 2: Text Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample German text with various issues\n",
    "raw_text = \"\"\"\n",
    "    Das ist ein   Beispieltext!!!  \n",
    "    Er enth√§lt GROSSBUCHSTABEN, Zahlen wie 123, E-Mails wie test@example.com,\n",
    "    URLs wie https://www.example.com und Sonderzeichen: @#$%!\n",
    "    \n",
    "    Es gibt auch    mehrfache    Leerzeichen und\n",
    "    Zeilenumbr√ºche.\n",
    "    \n",
    "    Deutsche Umlaute: √§√∂√º√Ñ√ñ√ú√ü sind wichtig!\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1988ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, \n",
    "               lowercase=True, \n",
    "               remove_urls=True, \n",
    "               remove_emails=True,\n",
    "               remove_numbers=False,\n",
    "               remove_punctuation=False,\n",
    "               remove_extra_whitespace=True):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "        lowercase (bool): Convert to lowercase\n",
    "        remove_urls (bool): Remove URLs\n",
    "        remove_emails (bool): Remove email addresses\n",
    "        remove_numbers (bool): Remove numbers\n",
    "        remove_punctuation (bool): Remove punctuation\n",
    "        remove_extra_whitespace (bool): Remove extra whitespace\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    if remove_urls:\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    if remove_emails:\n",
    "        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation (but keep German umlauts)\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s\\u00C0-\\u017F]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    if remove_extra_whitespace:\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test cleaning function\n",
    "cleaned = clean_text(raw_text)\n",
    "print(\"Gereinigter Text:\")\n",
    "print(cleaned)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Mit verschiedenen Optionen:\")\n",
    "print(\"\\nOhne Zahlen:\")\n",
    "print(clean_text(raw_text, remove_numbers=True))\n",
    "\n",
    "print(\"\\nOhne Satzzeichen:\")\n",
    "print(clean_text(raw_text, remove_punctuation=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a69c54e",
   "metadata": {},
   "source": [
    "## Part 3: Pattern Extraction and Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3317f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text with various entities\n",
    "sample_document = \"\"\"\n",
    "Prof. Dr. M√ºller lehrt an der Akademie f√ºr Wissenschaften. \n",
    "Sie k√∂nnen ihn unter mueller@akademie-wissen.de oder +49-30-1234-56789 erreichen.\n",
    "Die Vorlesung findet am 15.03.2025 um 14:00 Uhr in Raum B456 statt.\n",
    "Die Teilnahmegeb√ºhr betr√§gt 150,00 EUR. \n",
    "Weitere Informationen finden Sie unter https://www.akademie-wissen.de/vorlesungen.\n",
    "Anmeldeschluss ist der 01.03.2025.\n",
    "\"\"\"\n",
    "\n",
    "def extract_entities(text):\n",
    "    \"\"\"\n",
    "    Extract various entities from text using regex.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        dict: Extracted entities\n",
    "    \"\"\"\n",
    "    entities = {}\n",
    "    \n",
    "    # Extract email addresses\n",
    "    entities['emails'] = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "    \n",
    "    # Extract phone numbers (German format)\n",
    "    entities['phones'] = re.findall(r'\\+\\d{2}-\\d{2,3}-\\d{3,4}-\\d{4,5}', text)\n",
    "    \n",
    "    # Extract dates (German format: DD.MM.YYYY)\n",
    "    entities['dates'] = re.findall(r'\\d{2}\\.\\d{2}\\.\\d{4}', text)\n",
    "    \n",
    "    # Extract times (HH:MM format)\n",
    "    entities['times'] = re.findall(r'\\d{1,2}:\\d{2}', text)\n",
    "    \n",
    "    # Extract URLs\n",
    "    entities['urls'] = re.findall(r'https?://[^\\s]+', text)\n",
    "    \n",
    "    # Extract room numbers (pattern: A123, B456, etc.)\n",
    "    entities['rooms'] = re.findall(r'\\b[A-Z]\\d{3}\\b', text)\n",
    "    \n",
    "    # Extract prices (EUR format)\n",
    "    entities['prices'] = re.findall(r'\\d+,\\d{2}\\s*EUR', text)\n",
    "    \n",
    "    # Extract titles (Prof., Dr., etc.)\n",
    "    entities['titles'] = re.findall(r'\\b(Prof\\.|Dr\\.|Dipl\\.-Ing\\.)\\s+', text)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Extract entities\n",
    "extracted = extract_entities(sample_document)\n",
    "\n",
    "print(\"Extrahierte Entit√§ten:\")\n",
    "print(\"=\"*50)\n",
    "for entity_type, values in extracted.items():\n",
    "    if values:\n",
    "        print(f\"\\n{entity_type.upper()}:\")\n",
    "        for value in values:\n",
    "            print(f\"  - {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127db0d",
   "metadata": {},
   "source": [
    "## Part 4: Text Tokenization\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units (tokens) such as words, sentences, or subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8bfbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    \"\"\"\n",
    "    Simple word tokenization using regex.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        list: List of word tokens\n",
    "    \"\"\"\n",
    "    # Match word characters including German umlauts\n",
    "    tokens = re.findall(r'\\b[\\w\\u00C0-\\u017F]+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    \"\"\"\n",
    "    Simple sentence tokenization using regex.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        list: List of sentence tokens\n",
    "    \"\"\"\n",
    "    # Split on sentence-ending punctuation\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    # Clean up and remove empty strings\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# Test tokenization\n",
    "sample = \"Das ist ein Satz! Und hier ist noch einer. Was f√ºr ein sch√∂ner Tag?\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(sample)\n",
    "\n",
    "print(\"\\nWort-Tokenisierung:\")\n",
    "word_tokens = tokenize_words(sample)\n",
    "print(word_tokens)\n",
    "print(f\"Anzahl Tokens: {len(word_tokens)}\")\n",
    "\n",
    "print(\"\\nSatz-Tokenisierung:\")\n",
    "sentence_tokens = tokenize_sentences(sample)\n",
    "for i, sent in enumerate(sentence_tokens, 1):\n",
    "    print(f\"  {i}. {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86338be6",
   "metadata": {},
   "source": [
    "## Part 5: Text Statistics and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de64dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    \"\"\"\n",
    "    Perform basic statistical analysis on text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        dict: Text statistics\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # Basic counts\n",
    "    stats['total_chars'] = len(text)\n",
    "    stats['total_chars_no_spaces'] = len(re.sub(r'\\s', '', text))\n",
    "    \n",
    "    # Word statistics\n",
    "    words = tokenize_words(text)\n",
    "    stats['total_words'] = len(words)\n",
    "    stats['unique_words'] = len(set(words))\n",
    "    stats['avg_word_length'] = sum(len(word) for word in words) / len(words) if words else 0\n",
    "    \n",
    "    # Sentence statistics\n",
    "    sentences = tokenize_sentences(text)\n",
    "    stats['total_sentences'] = len(sentences)\n",
    "    stats['avg_words_per_sentence'] = len(words) / len(sentences) if sentences else 0\n",
    "    \n",
    "    # Most common words\n",
    "    word_freq = Counter(words)\n",
    "    stats['most_common_words'] = word_freq.most_common(10)\n",
    "    \n",
    "    # Count digits\n",
    "    stats['digit_count'] = len(re.findall(r'\\d', text))\n",
    "    \n",
    "    # Count uppercase letters\n",
    "    stats['uppercase_count'] = len(re.findall(r'[A-Z]', text))\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Analyze sample text\n",
    "analysis_text = \"\"\"\n",
    "Natural Language Processing (NLP) ist ein spannendes Forschungsgebiet der Informatik.\n",
    "Es kombiniert Linguistik, maschinelles Lernen und k√ºnstliche Intelligenz.\n",
    "Mit NLP k√∂nnen Computer menschliche Sprache verstehen und verarbeiten.\n",
    "Anwendungen umfassen Chatbots, maschinelle √úbersetzung und Sentimentanalyse.\n",
    "Die Entwicklung von NLP hat in den letzten Jahren enorme Fortschritte gemacht.\n",
    "\"\"\"\n",
    "\n",
    "stats = analyze_text(analysis_text)\n",
    "\n",
    "print(\"Text-Analyse:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Gesamtzeichen: {stats['total_chars']}\")\n",
    "print(f\"Zeichen ohne Leerzeichen: {stats['total_chars_no_spaces']}\")\n",
    "print(f\"\\nGesamtw√∂rter: {stats['total_words']}\")\n",
    "print(f\"Eindeutige W√∂rter: {stats['unique_words']}\")\n",
    "print(f\"Durchschnittliche Wortl√§nge: {stats['avg_word_length']:.2f}\")\n",
    "print(f\"\\nGesamts√§tze: {stats['total_sentences']}\")\n",
    "print(f\"Durchschnittliche W√∂rter pro Satz: {stats['avg_words_per_sentence']:.2f}\")\n",
    "print(f\"\\nZiffern: {stats['digit_count']}\")\n",
    "print(f\"Gro√übuchstaben: {stats['uppercase_count']}\")\n",
    "print(f\"\\nH√§ufigste W√∂rter:\")\n",
    "for word, count in stats['most_common_words']:\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc18105",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Pattern Matching Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# German-specific patterns\n",
    "def validate_german_patterns(text):\n",
    "    \"\"\"\n",
    "    Validate various German text patterns.\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        'postal_code': r'\\b\\d{5}\\b',  # German postal codes\n",
    "        'iban': r'\\bDE\\d{20}\\b',  # German IBAN (simplified)\n",
    "        'license_plate': r'\\b[A-Z√Ñ√ñ√ú]{1,3}-[A-Z√Ñ√ñ√ú]{1,2}\\s?\\d{1,4}\\b',  # German license plates\n",
    "        'academic_titles': r'\\b(Prof\\.|Dr\\.|Dipl\\.-Ing\\.)\\s+',\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for pattern_name, pattern in patterns.items():\n",
    "        matches = re.findall(pattern, text)\n",
    "        if matches:\n",
    "            results[pattern_name] = matches\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test German patterns\n",
    "german_text = \"\"\"\n",
    "Prof. Dr. M√ºller wohnt in 10115 Berlin.\n",
    "Seine IBAN lautet DE89370400440532013000.\n",
    "Das Auto mit dem Kennzeichen B-MW 1234 geh√∂rt ihm.\n",
    "Dr. Schmidt hat einen M.Sc. in Informatik.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Deutsche Muster-Erkennung:\")\n",
    "print(\"=\"*50)\n",
    "found_patterns = validate_german_patterns(german_text)\n",
    "for pattern_type, matches in found_patterns.items():\n",
    "    print(f\"\\n{pattern_type.upper()}:\")\n",
    "    for match in matches:\n",
    "        print(f\"  - {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e0fff",
   "metadata": {},
   "source": [
    "## Part 7: Building a Text Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    \"\"\"\n",
    "    A comprehensive text processing pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.processing_steps = []\n",
    "    \n",
    "    def add_step(self, step_name, step_function):\n",
    "        \"\"\"Add a processing step to the pipeline.\"\"\"\n",
    "        self.processing_steps.append((step_name, step_function))\n",
    "        return self\n",
    "    \n",
    "    def process(self, text, verbose=True):\n",
    "        \"\"\"Process text through all steps in the pipeline.\"\"\"\n",
    "        if verbose:\n",
    "            print(\"Text Processing Pipeline\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Original text length: {len(text)} characters\\n\")\n",
    "        \n",
    "        current_text = text\n",
    "        \n",
    "        for step_name, step_function in self.processing_steps:\n",
    "            current_text = step_function(current_text)\n",
    "            if verbose:\n",
    "                print(f\"After {step_name}:\")\n",
    "                print(f\"  Length: {len(current_text)} characters\")\n",
    "                print(f\"  Preview: {current_text[:100]}...\\n\")\n",
    "        \n",
    "        return current_text\n",
    "\n",
    "# Define processing functions\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def remove_emails(text):\n",
    "    return re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    return re.sub(r'[^\\w\\s\\u00C0-\\u017F]', '', text)\n",
    "\n",
    "# Create and configure pipeline\n",
    "pipeline = TextProcessor()\n",
    "pipeline.add_step(\"URL Removal\", remove_urls)\n",
    "pipeline.add_step(\"Email Removal\", remove_emails)\n",
    "pipeline.add_step(\"Lowercase Conversion\", to_lowercase)\n",
    "pipeline.add_step(\"Special Character Removal\", remove_special_chars)\n",
    "pipeline.add_step(\"Whitespace Normalization\", normalize_whitespace)\n",
    "\n",
    "# Test pipeline\n",
    "test_text = \"\"\"\n",
    "    Besuchen Sie unsere Website unter https://www.example.com!\n",
    "    Kontaktieren Sie uns: info@example.com\n",
    "    WICHTIGE INFORMATIONEN √ºber NLP und maschinelles Lernen!!!\n",
    "    Es gibt    viele    Leerzeichen    hier.\n",
    "\"\"\"\n",
    "\n",
    "processed_text = pipeline.process(test_text)\n",
    "\n",
    "print(\"\\nFinal Result:\")\n",
    "print(\"=\"*50)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c0895f",
   "metadata": {},
   "source": [
    "## Part 8: Practical Exercise - Document Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d754dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document(text):\n",
    "    \"\"\"\n",
    "    Comprehensive document analysis combining all techniques.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input document\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Extract entities\n",
    "    results['entities'] = extract_entities(text)\n",
    "    \n",
    "    # Text statistics\n",
    "    results['statistics'] = analyze_text(text)\n",
    "    \n",
    "    # Pattern validation\n",
    "    results['german_patterns'] = validate_german_patterns(text)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Sample document for analysis\n",
    "document = \"\"\"\n",
    "Sehr geehrte Damen und Herren,\n",
    "\n",
    "hiermit lade ich Sie zur Konferenz \"NLP in der Praxis\" ein.\n",
    "Die Veranstaltung findet am 15.05.2025 um 09:00 Uhr statt.\n",
    "Ort: Freie Universit√§t Berlin, Raum A123, 14195 Berlin.\n",
    "\n",
    "Keynote-Speaker: Prof. Dr. Anna Schmidt (anna.schmidt@fu-berlin.de)\n",
    "Thema: \"Moderne Ans√§tze im Natural Language Processing\"\n",
    "\n",
    "Anmeldung unter: https://www.nlp-konferenz.de\n",
    "R√ºckfragen an: info@nlp-konferenz.de oder +49-30-838-12345\n",
    "\n",
    "Teilnahmegeb√ºhr: 299,00 EUR\n",
    "IBAN f√ºr √úberweisung: DE89370400440532013000\n",
    "\n",
    "Mit freundlichen Gr√º√üen,\n",
    "Dr. Max M√ºller\n",
    "\"\"\"\n",
    "\n",
    "print(\"Dokumenten-Analyse:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nOriginaldokument:\")\n",
    "print(document)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "analysis = analyze_document(document)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìß EXTRAHIERTE ENTIT√ÑTEN:\")\n",
    "print(\"-\"*70)\n",
    "for entity_type, values in analysis['entities'].items():\n",
    "    if values:\n",
    "        print(f\"\\n{entity_type.upper()}:\")\n",
    "        for value in values:\n",
    "            print(f\"  ‚úì {value}\")\n",
    "\n",
    "print(\"\\n\\nüìä STATISTISCHE ANALYSE:\")\n",
    "print(\"-\"*70)\n",
    "stats = analysis['statistics']\n",
    "print(f\"W√∂rter: {stats['total_words']} (davon {stats['unique_words']} eindeutig)\")\n",
    "print(f\"S√§tze: {stats['total_sentences']}\")\n",
    "print(f\"Zeichen: {stats['total_chars']}\")\n",
    "print(f\"Durchschnittliche Wortl√§nge: {stats['avg_word_length']:.2f}\")\n",
    "print(f\"Durchschnittliche W√∂rter pro Satz: {stats['avg_words_per_sentence']:.2f}\")\n",
    "\n",
    "print(\"\\n\\nüá©üá™ DEUTSCHE MUSTER:\")\n",
    "print(\"-\"*70)\n",
    "for pattern_type, matches in analysis['german_patterns'].items():\n",
    "    print(f\"\\n{pattern_type.upper()}:\")\n",
    "    for match in matches:\n",
    "        print(f\"  ‚úì {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f29a0",
   "metadata": {},
   "source": [
    "## Exercise Tasks\n",
    "\n",
    "Complete the following tasks to practice your regex and text processing skills:\n",
    "\n",
    "1. **Pattern Creation**:\n",
    "   - Create a regex pattern to extract German street addresses\n",
    "   - Write a pattern to find all capitalized words (potential proper nouns)\n",
    "   - Develop a pattern for German phone numbers in various formats\n",
    "\n",
    "2. **Text Cleaning**:\n",
    "   - Build a function to remove HTML tags from text\n",
    "   - Create a function to normalize different quotation marks\n",
    "   - Implement a function to expand common German abbreviations\n",
    "\n",
    "3. **Information Extraction**:\n",
    "   - Extract all monetary amounts from a text (EUR, $, etc.)\n",
    "   - Find and categorize all numbers (integers, floats, percentages)\n",
    "   - Extract compound words (typical in German)\n",
    "\n",
    "4. **Text Validation**:\n",
    "   - Validate German postal codes\n",
    "   - Check if text contains proper sentence structure\n",
    "   - Identify potential spelling errors using pattern matching\n",
    "\n",
    "5. **Advanced Pipeline**:\n",
    "   - Create a pipeline that anonymizes personal information\n",
    "   - Build a text normalizer for social media content\n",
    "   - Develop a preprocessing pipeline for sentiment analysis\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. When should you use regex vs. specialized NLP libraries?\n",
    "2. What are the limitations of regex for text processing?\n",
    "3. How can regex patterns be optimized for performance?\n",
    "4. Why is text preprocessing important for NLP tasks?\n",
    "5. What challenges are specific to German text processing?\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Proceed to **Notebook 01**: Introduction to NLP and Text Processing\n",
    "- Explore advanced tokenization with NLTK and spaCy\n",
    "- Learn about stemming and lemmatization\n",
    "- Study language-specific text processing challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d2cd9d",
   "metadata": {},
   "source": [
    "## Exercise 1: Basic Text Cleaning\n",
    "\n",
    "**Goal**: Learn fundamental text cleaning operations that form the foundation of all text processing tasks.\n",
    "\n",
    "**Your Task**: Implement basic text cleaning functions using both simple string methods and regular expressions.\n",
    "\n",
    "### Setup and Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81746548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Sample German text with various issues (use this for testing your functions)\n",
    "sample_german_text = \"\"\"\n",
    "    Das ist ein   Beispieltext!!!  Er enth√§lt GROSSBUCHSTABEN, \n",
    "    Zahlen wie 123, E-Mails wie test@uni-berlin.de,\n",
    "    URLs wie https://www.example.com und Sonderzeichen: @#$%!\n",
    "    Deutsche Umlaute: √§√∂√º√Ñ√ñ√ú√ü sind wichtig!   \n",
    "    \n",
    "    Telefonnummer: 030-12345678\n",
    "    Datum: 15.03.2025\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìÑ Sample Text to Work With:\")\n",
    "print(repr(sample_german_text))  # repr() shows whitespace and special characters\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1a: Basic String Cleaning\n",
    "def clean_basic_text(text):\n",
    "    \"\"\"\n",
    "    Clean text using basic string methods.\n",
    "    \n",
    "    Your task: Implement the following cleaning steps:\n",
    "    1. Remove leading/trailing whitespace\n",
    "    2. Replace multiple spaces with single spaces\n",
    "    3. Convert to lowercase\n",
    "    4. Remove common punctuation (but keep German umlauts!)\n",
    "    \n",
    "    Hints:\n",
    "    - Use .strip() to remove leading/trailing whitespace\n",
    "    - Use re.sub(r'\\s+', ' ', text) to replace multiple spaces\n",
    "    - Use .lower() for lowercase conversion\n",
    "    - For punctuation, use string.punctuation but be careful with German characters\n",
    "    - You can use .translate() with str.maketrans() for punctuation removal\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement your cleaning steps here\n",
    "    # Step 1: Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # Step 2: Replace multiple whitespaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Step 3: Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 4: Remove punctuation (keep German umlauts)\n",
    "    # Create a translation table that maps each punctuation character to None\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    return text  # Return the cleaned text\n",
    "\n",
    "# Test your function (uncomment after implementing)\n",
    "cleaned_basic = clean_basic_text(sample_german_text)\n",
    "print(\"Basic cleaning result:\")\n",
    "print(repr(cleaned_basic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e323da6e",
   "metadata": {},
   "source": [
    "## Exercise 2: Introduction to Regular Expressions\n",
    "\n",
    "**Goal**: Learn regex basics for powerful pattern matching and text manipulation.\n",
    "\n",
    "### Regex Basics - Essential Patterns\n",
    "\n",
    "Before diving into exercises, here are the key regex patterns you'll need:\n",
    "\n",
    "**Basic Characters:**\n",
    "- `.` : Matches any single character\n",
    "- `*` : Matches 0 or more repetitions  \n",
    "- `+` : Matches 1 or more repetitions\n",
    "- `?` : Matches 0 or 1 repetition\n",
    "- `{n}` : Matches exactly n repetitions\n",
    "- `{n,m}` : Matches n to m repetitions\n",
    "\n",
    "**Character Classes:**\n",
    "- `[abc]` : Matches a, b, or c\n",
    "- `[a-z]` : Matches any lowercase letter\n",
    "- `[A-Z]` : Matches any uppercase letter  \n",
    "- `[0-9]` : Matches any digit\n",
    "- `\\d` : Matches any digit (equivalent to [0-9])\n",
    "- `\\w` : Matches word characters (letters, digits, underscore)\n",
    "- `\\s` : Matches whitespace characters\n",
    "\n",
    "**Anchors:**\n",
    "- `^` : Start of string\n",
    "- `$` : End of string\n",
    "- `\\b` : Word boundary\n",
    "\n",
    "**Special for German:**\n",
    "- `[a-zA-Z√§√∂√º√Ñ√ñ√ú√ü]` : German letters including umlauts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2a: Pattern Recognition Practice\n",
    "\n",
    "def find_phone_numbers(text):\n",
    "    \"\"\"\n",
    "    Find German phone numbers in text.\n",
    "    \n",
    "    Your task: Write a regex pattern to find phone numbers\n",
    "    \n",
    "    German phone formats to match:\n",
    "    - 030-12345678 (area code with dash)\n",
    "    - 030 12345678 (area code with space)  \n",
    "    - +49 30 12345678 (international format)\n",
    "    - (030) 12345678 (area code in parentheses)\n",
    "    \n",
    "    Hints:\n",
    "    - \\d matches digits\n",
    "    - {n} matches exactly n repetitions\n",
    "    - {n,m} matches n to m repetitions\n",
    "    - [-\\s] matches dash or space\n",
    "    - \\+ matches literal plus sign\n",
    "    - [\\(\\)] matches parentheses (need to escape them)\n",
    "    - Use | for alternatives: (pattern1|pattern2)\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to search in\n",
    "        \n",
    "    Returns:\n",
    "        list: List of found phone numbers\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Write your regex pattern here\n",
    "    # Hint: Start simple with one format, then expand\n",
    "    phone_pattern = r'(\\+49\\s?|0)(30|40|70|80|90)?[-\\s]?\\d{3,4}[-\\s]?\\d{4}'  # Your pattern goes here\n",
    "    \n",
    "    # TODO: Use re.findall to find all matches\n",
    "    phones = re.findall(phone_pattern, text)\n",
    "    return phones\n",
    "    \n",
    "    pass  # Remove this when you implement\n",
    "\n",
    "def find_email_addresses(text):\n",
    "    \"\"\"\n",
    "    Find email addresses in text.\n",
    "    \n",
    "    Your task: Write a regex to match email addresses\n",
    "    \n",
    "    Email format: username@domain.extension\n",
    "    - Username: letters, numbers, dots, underscores, hyphens\n",
    "    - Domain: letters, numbers, dots, hyphens\n",
    "    - Extension: 2-4 letters\n",
    "    \n",
    "    Hints:\n",
    "    - [A-Za-z0-9._-] matches valid username characters\n",
    "    - + means one or more\n",
    "    - @ matches literal @ symbol\n",
    "    - \\. matches literal dot (. is special in regex)\n",
    "    - {2,4} matches 2 to 4 repetitions\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to search in\n",
    "        \n",
    "    Returns:\n",
    "        list: List of found email addresses\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Write your email pattern\n",
    "    email_pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,4}'  # Your pattern goes here\n",
    "    \n",
    "    # TODO: Find and return emails\n",
    "    emails = re.findall(email_pattern, text)\n",
    "    return emails\n",
    "\n",
    "# Test data for your functions\n",
    "test_text = \"\"\"\n",
    "Kontakt: Max Mustermann\n",
    "Telefon: 030-12345678 oder (030) 87654321\n",
    "Mobil: +49 175 1234567\n",
    "E-Mail: max.mustermann@uni-berlin.de\n",
    "Backup: support@example.com\n",
    "\"\"\"\n",
    "\n",
    "# Test your functions (uncomment after implementing)\n",
    "phones = find_phone_numbers(test_text)\n",
    "emails = find_email_addresses(test_text)\n",
    "print(\"Found phones:\", phones)\n",
    "print(\"Found emails:\", emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079328e0",
   "metadata": {},
   "source": [
    "### Regex Functions in Python\n",
    "\n",
    "Python's `re` module provides several key functions:\n",
    "- `re.search()`: Find first match\n",
    "- `re.match()`: Match at beginning of string\n",
    "- `re.findall()`: Find all matches\n",
    "- `re.finditer()`: Find all matches (returns iterator)\n",
    "- `re.sub()`: Replace matches\n",
    "- `re.split()`: Split string by pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different regex functions\n",
    "sample_text = \"Das Meeting ist am 15.03.2025 um 14:30 Uhr. N√§chstes Meeting: 22.03.2025 um 10:00 Uhr.\"\n",
    "\n",
    "# re.search() - Find first occurrence\n",
    "date_pattern = r'\\d{2}\\.\\d{2}\\.\\d{4}'\n",
    "first_date = re.search(date_pattern, sample_text)\n",
    "if first_date:\n",
    "    print(f\"Erstes Datum gefunden: {first_date.group()}\")\n",
    "    print(f\"Position: {first_date.span()}\")\n",
    "\n",
    "# re.findall() - Find all occurrences\n",
    "all_dates = re.findall(date_pattern, sample_text)\n",
    "print(f\"\\nAlle Daten: {all_dates}\")\n",
    "\n",
    "# re.finditer() - Iterator over all matches\n",
    "print(\"\\nDetaillierte Informationen zu allen Daten:\")\n",
    "for match in re.finditer(date_pattern, sample_text):\n",
    "    print(f\"  Datum: {match.group()}, Position: {match.span()}\")\n",
    "\n",
    "# re.sub() - Replace text\n",
    "censored = re.sub(date_pattern, '[DATUM ENTFERNT]', sample_text)\n",
    "print(f\"\\nZensierter Text: {censored}\")\n",
    "\n",
    "# re.split() - Split by pattern\n",
    "sentences = re.split(r'\\. ', sample_text)\n",
    "print(f\"\\nS√§tze: {sentences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aa7747",
   "metadata": {},
   "source": [
    "## Part 2: Text Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c497e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample German text with various issues\n",
    "raw_text = \"\"\"\n",
    "    Das ist ein   Beispieltext!!!  \n",
    "    Er enth√§lt GROSSBUCHSTABEN, Zahlen wie 123, E-Mails wie test@example.com,\n",
    "    URLs wie https://www.example.com und Sonderzeichen: @#$%!\n",
    "    \n",
    "    Es gibt auch    mehrfache    Leerzeichen und\n",
    "    Zeilenumbr√ºche.\n",
    "    \n",
    "    Deutsche Umlaute: √§√∂√º√Ñ√ñ√ú√ü sind wichtig!\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fd981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, \n",
    "               lowercase=True, \n",
    "               remove_urls=True, \n",
    "               remove_emails=True,\n",
    "               remove_numbers=False,\n",
    "               remove_punctuation=False,\n",
    "               remove_extra_whitespace=True):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "        lowercase (bool): Convert to lowercase\n",
    "        remove_urls (bool): Remove URLs\n",
    "        remove_emails (bool): Remove email addresses\n",
    "        remove_numbers (bool): Remove numbers\n",
    "        remove_punctuation (bool): Remove punctuation\n",
    "        remove_extra_whitespace (bool): Remove extra whitespace\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    if remove_urls:\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    if remove_emails:\n",
    "        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation (but keep German umlauts)\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s\\u00C0-\\u017F]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    if remove_extra_whitespace:\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test cleaning function\n",
    "cleaned = clean_text(raw_text)\n",
    "print(\"Gereinigter Text:\")\n",
    "print(cleaned)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Mit verschiedenen Optionen:\")\n",
    "print(\"\\nOhne Zahlen:\")\n",
    "print(clean_text(raw_text, remove_numbers=True))\n",
    "\n",
    "print(\"\\nOhne Satzzeichen:\")\n",
    "print(clean_text(raw_text, remove_punctuation=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f8dcd",
   "metadata": {},
   "source": [
    "## Part 3: Pattern Extraction and Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c95b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text with various entities\n",
    "sample_document = \"\"\"\n",
    "Prof. Dr. M√ºller lehrt an der Akademie f√ºr Wissenschaften. \n",
    "Sie k√∂nnen ihn unter mueller@akademie-wissen.de oder +49-30-1234-56789 erreichen.\n",
    "Die Vorlesung findet am 15.03.2025 um 14:00 Uhr in Raum B456 statt.\n",
    "Die Teilnahmegeb√ºhr betr√§gt 150,00 EUR. \n",
    "Weitere Informationen finden Sie unter https://www.akademie-wissen.de/vorlesungen.\n",
    "Anmeldeschluss ist der 01.03.2025.\n",
    "\"\"\"\n",
    "\n",
    "def extract_entities(text):\n",
    "    \"\"\"\n",
    "    Extract various entities from text using regex.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        dict: Extracted entities\n",
    "    \"\"\"\n",
    "    entities = {}\n",
    "    \n",
    "    # Extract email addresses\n",
    "    entities['emails'] = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "    \n",
    "    # Extract phone numbers (German format)\n",
    "    entities['phones'] = re.findall(r'\\+\\d{2}-\\d{2,3}-\\d{3,4}-\\d{4,5}', text)\n",
    "    \n",
    "    # Extract dates (German format: DD.MM.YYYY)\n",
    "    entities['dates'] = re.findall(r'\\d{2}\\.\\d{2}\\.\\d{4}', text)\n",
    "    \n",
    "    # Extract times (HH:MM format)\n",
    "    entities['times'] = re.findall(r'\\d{1,2}:\\d{2}', text)\n",
    "    \n",
    "    # Extract URLs\n",
    "    entities['urls'] = re.findall(r'https?://[^\\s]+', text)\n",
    "    \n",
    "    # Extract room numbers (pattern: A123, B456, etc.)\n",
    "    entities['rooms'] = re.findall(r'\\b[A-Z]\\d{3}\\b', text)\n",
    "    \n",
    "    # Extract prices (EUR format)\n",
    "    entities['prices'] = re.findall(r'\\d+,\\d{2}\\s*EUR', text)\n",
    "    \n",
    "    # Extract titles (Prof., Dr., etc.)\n",
    "    entities['titles'] = re.findall(r'\\b(Prof\\.|Dr\\.|Dipl\\.-Ing\\.)\\s+', text)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Extract entities\n",
    "extracted = extract_entities(sample_document)\n",
    "\n",
    "print(\"Extrahierte Entit√§ten:\")\n",
    "print(\"=\"*50)\n",
    "for entity_type, values in extracted.items():\n",
    "    if values:\n",
    "        print(f\"\\n{entity_type.upper()}:\")\n",
    "        for value in values:\n",
    "            print(f\"  - {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb804901",
   "metadata": {},
   "source": [
    "## Part 4: Text Tokenization\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units (tokens) such as words, sentences, or subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be630244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    \"\"\"\n",
    "    Simple word tokenization using regex.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        list: List of word tokens\n",
    "    \"\"\"\n",
    "    # Match word characters including German umlauts\n",
    "    tokens = re.findall(r'\\b[\\w\\u00C0-\\u017F]+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    \"\"\"\n",
    "    Simple sentence tokenization using regex.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        list: List of sentence tokens\n",
    "    \"\"\"\n",
    "    # Split on sentence-ending punctuation\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    # Clean up and remove empty strings\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# Test tokenization\n",
    "sample = \"Das ist ein Satz! Und hier ist noch einer. Was f√ºr ein sch√∂ner Tag?\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(sample)\n",
    "\n",
    "print(\"\\nWort-Tokenisierung:\")\n",
    "word_tokens = tokenize_words(sample)\n",
    "print(word_tokens)\n",
    "print(f\"Anzahl Tokens: {len(word_tokens)}\")\n",
    "\n",
    "print(\"\\nSatz-Tokenisierung:\")\n",
    "sentence_tokens = tokenize_sentences(sample)\n",
    "for i, sent in enumerate(sentence_tokens, 1):\n",
    "    print(f\"  {i}. {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9075c90",
   "metadata": {},
   "source": [
    "## Part 5: Text Statistics and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    \"\"\"\n",
    "    Perform basic statistical analysis on text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        dict: Text statistics\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # Basic counts\n",
    "    stats['total_chars'] = len(text)\n",
    "    stats['total_chars_no_spaces'] = len(re.sub(r'\\s', '', text))\n",
    "    \n",
    "    # Word statistics\n",
    "    words = tokenize_words(text)\n",
    "    stats['total_words'] = len(words)\n",
    "    stats['unique_words'] = len(set(words))\n",
    "    stats['avg_word_length'] = sum(len(word) for word in words) / len(words) if words else 0\n",
    "    \n",
    "    # Sentence statistics\n",
    "    sentences = tokenize_sentences(text)\n",
    "    stats['total_sentences'] = len(sentences)\n",
    "    stats['avg_words_per_sentence'] = len(words) / len(sentences) if sentences else 0\n",
    "    \n",
    "    # Most common words\n",
    "    word_freq = Counter(words)\n",
    "    stats['most_common_words'] = word_freq.most_common(10)\n",
    "    \n",
    "    # Count digits\n",
    "    stats['digit_count'] = len(re.findall(r'\\d', text))\n",
    "    \n",
    "    # Count uppercase letters\n",
    "    stats['uppercase_count'] = len(re.findall(r'[A-Z]', text))\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Analyze sample text\n",
    "analysis_text = \"\"\"\n",
    "Natural Language Processing (NLP) ist ein spannendes Forschungsgebiet der Informatik.\n",
    "Es kombiniert Linguistik, maschinelles Lernen und k√ºnstliche Intelligenz.\n",
    "Mit NLP k√∂nnen Computer menschliche Sprache verstehen und verarbeiten.\n",
    "Anwendungen umfassen Chatbots, maschinelle √úbersetzung und Sentimentanalyse.\n",
    "Die Entwicklung von NLP hat in den letzten Jahren enorme Fortschritte gemacht.\n",
    "\"\"\"\n",
    "\n",
    "stats = analyze_text(analysis_text)\n",
    "\n",
    "print(\"Text-Analyse:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Gesamtzeichen: {stats['total_chars']}\")\n",
    "print(f\"Zeichen ohne Leerzeichen: {stats['total_chars_no_spaces']}\")\n",
    "print(f\"\\nGesamtw√∂rter: {stats['total_words']}\")\n",
    "print(f\"Eindeutige W√∂rter: {stats"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
