{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b437d7d9",
   "metadata": {},
   "source": [
    "# Exercise 4: BERT Classification\n",
    "\n",
    "Welcome to modern NLP with BERT! You'll learn how to fine-tune state-of-the-art language models for classification tasks.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this exercise, you will be able to:\n",
    "1. **BERT Architecture**: Understand transformer-based language models and attention mechanisms\n",
    "2. **German BERT Models**: Work with pre-trained German BERT variants (GBERT, DistilBERT)\n",
    "3. **Fine-tuning Process**: Adapt pre-trained models for specific classification tasks\n",
    "4. **Tokenization**: Handle BERT's WordPiece tokenization for German text\n",
    "5. **Performance Comparison**: Compare BERT with traditional ML approaches\n",
    "6. **Model Evaluation**: Assess BERT model performance with appropriate metrics\n",
    "\n",
    "## What You'll Build\n",
    "- German sentiment classifier using BERT\n",
    "- Performance comparison framework\n",
    "- BERT model fine-tuning pipeline\n",
    "- Attention visualization system\n",
    "- Production-ready classification API\n",
    "\n",
    "## Applications\n",
    "- **Advanced Sentiment Analysis**: More nuanced emotion detection\n",
    "- **Document Classification**: High-accuracy topic categorization\n",
    "- **Intent Recognition**: Chatbot and voice assistant improvements\n",
    "- **Content Moderation**: Sophisticated harmful content detection\n",
    "\n",
    "**Ready to harness the power of transformers?** ü§ñ‚ö°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d0af32",
   "metadata": {},
   "source": [
    "## Exercise 1: German BERT Classification Pipeline\n",
    "\n",
    "**Goal**: Build and fine-tune a German BERT model for sentiment classification.\n",
    "\n",
    "**Your Tasks**: \n",
    "1. Load and explore pre-trained German BERT models\n",
    "2. Prepare data for BERT fine-tuning\n",
    "3. Fine-tune BERT on German sentiment data\n",
    "4. Compare BERT performance with traditional methods\n",
    "\n",
    "**Hints**:\n",
    "- Use 'dbmdz/bert-base-german-cased' for German text\n",
    "- BERT requires special tokenization with [CLS] and [SEP] tokens\n",
    "- Fine-tuning typically needs only a few epochs\n",
    "- Learning rates for BERT are usually much smaller (2e-5)\n",
    "\n",
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501987ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple import - just try the pipeline\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    print(\"Transformers library available!\")\n",
    "    \n",
    "    # Try to load a simple German sentiment pipeline\n",
    "    classifier = pipeline(\"sentiment-analysis\", model=\"oliverguhr/german-sentiment-bert\")\n",
    "    print(\"German BERT model loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"BERT libraries not available. Install with: pip install transformers\")\n",
    "    classifier = None\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import Dataset as HFDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80b1a8",
   "metadata": {},
   "source": [
    "### Step 1: Load German BERT Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf434dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_german_bert_model(model_name=\"bert-base-german-cased\", num_labels=3):\n",
    "    \"\"\"\n",
    "    Load German BERT model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the pre-trained model\n",
    "        num_labels (int): Number of classification labels\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (tokenizer, model)\n",
    "    \"\"\"\n",
    "    # TODO: Load and configure German BERT model:\n",
    "    # 1. Load tokenizer for German text processing\n",
    "    # 2. Load pre-trained model for sequence classification\n",
    "    # 3. Configure for the specific number of labels\n",
    "    # 4. Move model to appropriate device (GPU/CPU)\n",
    "    \n",
    "    print(f\"Loading German BERT model: {model_name}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(f\"Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
    "    \n",
    "    # Load model for sequence classification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Model loaded successfully!\")\n",
    "    print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "def test_tokenizer(tokenizer, sample_texts):\n",
    "    \"\"\"\n",
    "    Test the tokenizer with sample German texts.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: Loaded tokenizer\n",
    "        sample_texts (list): Sample texts to tokenize\n",
    "    \"\"\"\n",
    "    print(\"\\nTokenizer Testing:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    for i, text in enumerate(sample_texts):\n",
    "        print(f\"\\nSample {i+1}: {text}\")\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        \n",
    "        # Encode\n",
    "        encoded = tokenizer.encode(text, add_special_tokens=True, max_length=64, truncation=True)\n",
    "        print(f\"Token IDs: {encoded}\")\n",
    "        \n",
    "        # Decode back\n",
    "        decoded = tokenizer.decode(encoded)\n",
    "        print(f\"Decoded: {decoded}\")\n",
    "\n",
    "# Load German BERT model\n",
    "tokenizer, model = load_german_bert_model()\n",
    "\n",
    "# Test tokenizer with German samples\n",
    "sample_texts = [\n",
    "    \"Das ist ein gro√üartiger Film!\",\n",
    "    \"Ich bin sehr entt√§uscht von diesem Produkt.\",\n",
    "    \"Ein durchschnittliches Restaurant mit normalem Service.\"\n",
    "]\n",
    "\n",
    "test_tokenizer(tokenizer, sample_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b63b8",
   "metadata": {},
   "source": [
    "### Step 2: Create Enhanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_sentiment_dataset():\n",
    "    \"\"\"\n",
    "    Create a larger, more diverse German sentiment dataset.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Enhanced dataset\n",
    "    \"\"\"\n",
    "    # TODO: Create a comprehensive German sentiment dataset:\n",
    "    # 1. Include more diverse examples\n",
    "    # 2. Add different domains (movies, products, services)\n",
    "    # 3. Ensure balanced classes\n",
    "    # 4. Include varying text lengths\n",
    "    \n",
    "    data = {\n",
    "        'text': [\n",
    "            # Positive examples (movies)\n",
    "            \"Dieser Film ist absolut fantastisch und sehr bewegend!\",\n",
    "            \"Ein Meisterwerk des deutschen Kinos mit hervorragenden Schauspielern.\",\n",
    "            \"Brillante Regie und eine fesselnde Geschichte von Anfang bis Ende.\",\n",
    "            \"Ich war begeistert von der visuellen Pracht und der emotionalen Tiefe.\",\n",
    "            \"Ein Film, den man immer wieder sehen kann - einfach wunderbar!\",\n",
    "            \n",
    "            # Positive examples (products)\n",
    "            \"Dieses Produkt √ºbertrifft alle meine Erwartungen bei weitem.\",\n",
    "            \"Ausgezeichnete Qualit√§t und sehr benutzerfreundlich.\",\n",
    "            \"Ich bin absolut zufrieden mit diesem Kauf und kann es nur empfehlen.\",\n",
    "            \"Hervorragende Verarbeitung und tolles Design.\",\n",
    "            \"Das beste Produkt in dieser Preisklasse, ohne Zweifel.\",\n",
    "            \n",
    "            # Positive examples (services)\n",
    "            \"Der Kundenservice war au√üergew√∂hnlich hilfsbereit und kompetent.\",\n",
    "            \"Schnelle Lieferung und perfekte Verpackung.\",\n",
    "            \"Das Personal war sehr freundlich und professionell.\",\n",
    "            \"Eine rundum positive Erfahrung, die ich gerne wiederholen w√ºrde.\",\n",
    "            \"Exzellenter Service mit pers√∂nlicher Betreuung.\",\n",
    "            \n",
    "            # Negative examples (movies)\n",
    "            \"Dieser Film ist langweilig und vorhersehbar.\",\n",
    "            \"Schlechte Schauspieler und eine verwirrende Handlung.\",\n",
    "            \"Zwei Stunden verschwendete Zeit - absolut entt√§uschend.\",\n",
    "            \"Die Dialoge sind schlecht und die Effekte wirken billig.\",\n",
    "            \"Ein Film ohne Seele und ohne jeden k√ºnstlerischen Wert.\",\n",
    "            \n",
    "            # Negative examples (products)\n",
    "            \"Das Produkt ist v√∂llig unbrauchbar und schlecht verarbeitet.\",\n",
    "            \"Schlechte Qualit√§t f√ºr einen so hohen Preis.\",\n",
    "            \"Nach einer Woche bereits kaputt - nie wieder!\",\n",
    "            \"Funktioniert nicht wie beschrieben und der Support ist unh√∂flich.\",\n",
    "            \"Geldverschwendung - w√ºrde es nicht einmal verschenken.\",\n",
    "            \n",
    "            # Negative examples (services)\n",
    "            \"Furchtbarer Kundenservice und sehr lange Wartezeiten.\",\n",
    "            \"Unprofessionelles Personal und schlechte Organisation.\",\n",
    "            \"Versp√§tete Lieferung und besch√§digte Ware.\",\n",
    "            \"Unfreundlich und inkompetent - eine Katastrophe.\",\n",
    "            \"Der schlechteste Service, den ich je erlebt habe.\",\n",
    "            \n",
    "            # Neutral examples (mixed)\n",
    "            \"Das Produkt erf√ºllt seinen Zweck, mehr aber auch nicht.\",\n",
    "            \"Ein durchschnittlicher Film mit einigen guten Momenten.\",\n",
    "            \"Der Service war in Ordnung, nichts Besonderes.\",\n",
    "            \"Akzeptable Qualit√§t f√ºr den Preis.\",\n",
    "            \"Weder besonders gut noch besonders schlecht.\",\n",
    "            \"Ein normales Restaurant mit standardm√§√üigem Essen.\",\n",
    "            \"Das Personal war h√∂flich, aber nicht sehr aufmerksam.\",\n",
    "            \"Funktioniert wie erwartet, ohne √úberraschungen.\",\n",
    "            \"Ein gew√∂hnliches Produkt ohne besondere Merkmale.\",\n",
    "            \"Mittelm√§√üige Leistung in allen Bereichen.\",\n",
    "            \"Eine durchschnittliche Erfahrung, die man vergisst.\",\n",
    "            \"Solide Qualit√§t, aber nichts Au√üergew√∂hnliches.\",\n",
    "            \"Der Film war okay, aber nicht unvergesslich.\",\n",
    "            \"Angemessener Preis f√ºr angemessene Leistung.\",\n",
    "            \"Eine neutrale Bewertung f√ºr ein neutrales Erlebnis.\"\n",
    "        ],\n",
    "        'sentiment': (\n",
    "            ['positive'] * 15 + \n",
    "            ['negative'] * 15 + \n",
    "            ['neutral'] * 15\n",
    "        ),\n",
    "        'domain': (\n",
    "            ['movie'] * 5 + ['product'] * 5 + ['service'] * 5 +  # positive\n",
    "            ['movie'] * 5 + ['product'] * 5 + ['service'] * 5 +  # negative\n",
    "            ['mixed'] * 15  # neutral\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add text length information\n",
    "    df['text_length'] = df['text'].apply(len)\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create enhanced dataset\n",
    "df = create_enhanced_sentiment_dataset()\n",
    "\n",
    "print(f\"Dataset created with {len(df)} samples\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(f\"\\nDomain distribution:\")\n",
    "print(df['domain'].value_counts())\n",
    "print(f\"\\nText length statistics:\")\n",
    "print(df['text_length'].describe())\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ab11a1",
   "metadata": {},
   "source": [
    "### Step 3: Prepare Data for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6786182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for sentiment analysis with BERT.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): List of text samples\n",
    "            labels (list): List of labels\n",
    "            tokenizer: BERT tokenizer\n",
    "            max_length (int): Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Create label mapping\n",
    "        self.label_to_id = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "        self.id_to_label = {v: k for k, v in self.label_to_id.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Sample index\n",
    "        \n",
    "        Returns:\n",
    "            dict: Tokenized input with labels\n",
    "        \"\"\"\n",
    "        # TODO: Implement data preparation for BERT:\n",
    "        # 1. Tokenize text with proper padding and truncation\n",
    "        # 2. Convert labels to numerical format\n",
    "        # 3. Return tensors in the correct format\n",
    "        \n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.label_to_id[label], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def prepare_datasets(df, tokenizer, test_size=0.2, val_size=0.1, max_length=128):\n",
    "    \"\"\"\n",
    "    Prepare train, validation, and test datasets.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input dataframe\n",
    "        tokenizer: BERT tokenizer\n",
    "        test_size (float): Test set proportion\n",
    "        val_size (float): Validation set proportion\n",
    "        max_length (int): Maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_dataset, val_dataset, test_dataset)\n",
    "    \"\"\"\n",
    "    # TODO: Split data and create datasets:\n",
    "    # 1. Split into train/validation/test sets\n",
    "    # 2. Ensure stratified sampling for balanced classes\n",
    "    # 3. Create Dataset objects for each split\n",
    "    \n",
    "    # First split: separate test set\n",
    "    train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(\n",
    "        df['text'].tolist(),\n",
    "        df['sentiment'].tolist(),\n",
    "        test_size=test_size,\n",
    "        random_state=42,\n",
    "        stratify=df['sentiment'].tolist()\n",
    "    )\n",
    "    \n",
    "    # Second split: separate train and validation\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        train_val_texts,\n",
    "        train_val_labels,\n",
    "        test_size=val_size/(1-test_size),  # Adjust for already removed test set\n",
    "        random_state=42,\n",
    "        stratify=train_val_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset splits:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Validation: {len(val_texts)} samples\")\n",
    "    print(f\"  Test: {len(test_texts)} samples\")\n",
    "    \n",
    "    # Create Dataset objects\n",
    "    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "    val_dataset = SentimentDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "    test_dataset = SentimentDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset, val_dataset, test_dataset = prepare_datasets(df, tokenizer)\n",
    "\n",
    "# Test dataset loading\n",
    "print(\"\\nSample from training dataset:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Label: {sample['labels']}\")\n",
    "print(f\"Decoded text: {tokenizer.decode(sample['input_ids'], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070853a2",
   "metadata": {},
   "source": [
    "### Step 4: Fine-tune BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7cf162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training_arguments(output_dir=\"./bert-sentiment-german\"):\n",
    "    \"\"\"\n",
    "    Setup training arguments for BERT fine-tuning.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Directory to save model outputs\n",
    "    \n",
    "    Returns:\n",
    "        TrainingArguments: Configured training arguments\n",
    "    \"\"\"\n",
    "    # TODO: Configure training hyperparameters:\n",
    "    # 1. Set learning rate, batch size, number of epochs\n",
    "    # 2. Configure evaluation strategy\n",
    "    # 3. Set up model saving and logging\n",
    "    # 4. Add early stopping if needed\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3,  # Small number for demonstration\n",
    "        per_device_train_batch_size=8,  # Adjust based on GPU memory\n",
    "        per_device_eval_batch_size=8,\n",
    "        learning_rate=2e-5,  # Common learning rate for BERT fine-tuning\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'{output_dir}/logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        seed=42,\n",
    "        report_to=None,  # Disable wandb/tensorboard for simplicity\n",
    "    )\n",
    "    \n",
    "    return training_args\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: Evaluation predictions\n",
    "    \n",
    "    Returns:\n",
    "        dict: Computed metrics\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "    }\n",
    "\n",
    "def fine_tune_bert(model, tokenizer, train_dataset, val_dataset):\n",
    "    \"\"\"\n",
    "    Fine-tune BERT model for sentiment classification.\n",
    "    \n",
    "    Args:\n",
    "        model: Pre-trained BERT model\n",
    "        tokenizer: BERT tokenizer\n",
    "        train_dataset: Training dataset\n",
    "        val_dataset: Validation dataset\n",
    "    \n",
    "    Returns:\n",
    "        Trainer: Trained model\n",
    "    \"\"\"\n",
    "    # TODO: Implement BERT fine-tuning:\n",
    "    # 1. Setup training arguments\n",
    "    # 2. Create Trainer object\n",
    "    # 3. Add callbacks (early stopping)\n",
    "    # 4. Train the model\n",
    "    # 5. Evaluate performance\n",
    "    \n",
    "    print(\"Setting up BERT fine-tuning...\")\n",
    "    \n",
    "    # Setup training arguments\n",
    "    training_args = setup_training_arguments()\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(\"Note: This may take several minutes depending on your hardware.\")\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"\\nValidation Results:\")\n",
    "    for key, value in eval_results.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Fine-tune BERT model\n",
    "print(\"Starting BERT fine-tuning process...\")\n",
    "trainer = fine_tune_bert(model, tokenizer, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c05cfc",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bert_model(trainer, test_dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the fine-tuned BERT model.\n",
    "    \n",
    "    Args:\n",
    "        trainer: Trained Trainer object\n",
    "        test_dataset: Test dataset\n",
    "        tokenizer: BERT tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    # TODO: Implement comprehensive model evaluation:\n",
    "    # 1. Evaluate on test set\n",
    "    # 2. Generate predictions and probabilities\n",
    "    # 3. Create confusion matrix\n",
    "    # 4. Generate classification report\n",
    "    # 5. Analyze errors and model behavior\n",
    "    \n",
    "    print(\"Evaluating BERT model on test set...\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "    \n",
    "    print(f\"\\nTest Set Results:\")\n",
    "    for key, value in test_results.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    y_true = predictions.label_ids\n",
    "    \n",
    "    # Label mapping\n",
    "    id_to_label = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "    label_names = ['negative', 'neutral', 'positive']\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_names))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.title('BERT Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    \n",
    "    # Plot class distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    unique_labels, counts = np.unique(y_true, return_counts=True)\n",
    "    plt.bar([label_names[i] for i in unique_labels], counts, color=['red', 'gray', 'green'])\n",
    "    plt.title('Test Set Class Distribution')\n",
    "    plt.xlabel('Sentiment Class')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Error analysis\n",
    "    print(\"\\nError Analysis:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    errors = []\n",
    "    for i, (true_label, pred_label) in enumerate(zip(y_true, y_pred)):\n",
    "        if true_label != pred_label:\n",
    "            # Get original text\n",
    "            sample = test_dataset[i]\n",
    "            text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "            \n",
    "            errors.append({\n",
    "                'text': text,\n",
    "                'true_label': id_to_label[true_label],\n",
    "                'predicted_label': id_to_label[pred_label],\n",
    "                'confidence': np.max(predictions.predictions[i])\n",
    "            })\n",
    "    \n",
    "    print(f\"Total errors: {len(errors)} out of {len(y_true)} samples\")\n",
    "    print(f\"Error rate: {len(errors)/len(y_true)*100:.2f}%\")\n",
    "    \n",
    "    # Show some error examples\n",
    "    if errors:\n",
    "        print(\"\\nSample errors:\")\n",
    "        for i, error in enumerate(errors[:5]):\n",
    "            print(f\"\\nError {i+1}:\")\n",
    "            print(f\"  Text: {error['text'][:100]}...\")\n",
    "            print(f\"  True: {error['true_label']}, Predicted: {error['predicted_label']}\")\n",
    "            print(f\"  Confidence: {error['confidence']:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'test_results': test_results,\n",
    "        'predictions': predictions,\n",
    "        'confusion_matrix': cm,\n",
    "        'errors': errors\n",
    "    }\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = evaluate_bert_model(trainer, test_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73a4d0b",
   "metadata": {},
   "source": [
    "### Step 6: Compare with Traditional ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e4495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_traditional_ml(df, bert_results):\n",
    "    \"\"\"\n",
    "    Compare BERT performance with traditional ML approaches.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Original dataset\n",
    "        bert_results (dict): BERT evaluation results\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comparison results\n",
    "    \"\"\"\n",
    "    # TODO: Implement comparison with traditional methods:\n",
    "    # 1. Train TF-IDF + Logistic Regression baseline\n",
    "    # 2. Train TF-IDF + SVM baseline\n",
    "    # 3. Compare accuracy, precision, recall, F1-score\n",
    "    # 4. Analyze training time and computational requirements\n",
    "    # 5. Visualize performance comparison\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    import time\n",
    "    \n",
    "    print(\"Comparing BERT with Traditional ML Methods:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df['text'].tolist()\n",
    "    y = df['sentiment'].tolist()\n",
    "    \n",
    "    # Same split as BERT\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # TF-IDF Vectorization\n",
    "    print(\"\\n1. Training TF-IDF + Logistic Regression...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "    X_test_tfidf = tfidf.transform(X_test)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    lr_model.fit(X_train_tfidf, y_train)\n",
    "    lr_pred = lr_model.predict(X_test_tfidf)\n",
    "    \n",
    "    lr_train_time = time.time() - start_time\n",
    "    lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "    lr_precision, lr_recall, lr_f1, _ = precision_recall_fscore_support(\n",
    "        y_test, lr_pred, average='weighted'\n",
    "    )\n",
    "    \n",
    "    print(f\"  Training time: {lr_train_time:.2f} seconds\")\n",
    "    print(f\"  Accuracy: {lr_accuracy:.4f}\")\n",
    "    print(f\"  F1-score: {lr_f1:.4f}\")\n",
    "    \n",
    "    # SVM\n",
    "    print(\"\\n2. Training TF-IDF + SVM...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    svm_model = SVC(kernel='linear', random_state=42)\n",
    "    svm_model.fit(X_train_tfidf, y_train)\n",
    "    svm_pred = svm_model.predict(X_test_tfidf)\n",
    "    \n",
    "    svm_train_time = time.time() - start_time\n",
    "    svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "    svm_precision, svm_recall, svm_f1, _ = precision_recall_fscore_support(\n",
    "        y_test, svm_pred, average='weighted'\n",
    "    )\n",
    "    \n",
    "    print(f\"  Training time: {svm_train_time:.2f} seconds\")\n",
    "    print(f\"  Accuracy: {svm_accuracy:.4f}\")\n",
    "    print(f\"  F1-score: {svm_f1:.4f}\")\n",
    "    \n",
    "    # Get BERT metrics\n",
    "    bert_accuracy = bert_results['test_results']['eval_accuracy']\n",
    "    \n",
    "    # Create comparison visualization\n",
    "    methods = ['Logistic Regression', 'SVM', 'BERT']\n",
    "    accuracies = [lr_accuracy, svm_accuracy, bert_accuracy]\n",
    "    f1_scores = [lr_f1, svm_f1, bert_accuracy]  # Approximation for BERT\n",
    "    train_times = [lr_train_time, svm_train_time, 300]  # Estimated BERT time\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    axes[0].bar(methods, accuracies, color=['blue', 'green', 'red'])\n",
    "    axes[0].set_title('Accuracy Comparison')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    for i, v in enumerate(accuracies):\n",
    "        axes[0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # F1-score comparison\n",
    "    axes[1].bar(methods, f1_scores, color=['blue', 'green', 'red'])\n",
    "    axes[1].set_title('F1-Score Comparison')\n",
    "    axes[1].set_ylabel('F1-Score')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    for i, v in enumerate(f1_scores):\n",
    "        axes[1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Training time comparison (log scale)\n",
    "    axes[2].bar(methods, train_times, color=['blue', 'green', 'red'])\n",
    "    axes[2].set_title('Training Time Comparison')\n",
    "    axes[2].set_ylabel('Training Time (seconds)')\n",
    "    axes[2].set_yscale('log')\n",
    "    for i, v in enumerate(train_times):\n",
    "        axes[2].text(i, v * 1.1, f'{v:.0f}s', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nComparison Summary:\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"BERT Accuracy: {bert_accuracy:.4f} (+{bert_accuracy - max(lr_accuracy, svm_accuracy):.4f})\")\n",
    "    print(f\"Best Traditional: {max(lr_accuracy, svm_accuracy):.4f}\")\n",
    "    print(f\"BERT vs Traditional: {((bert_accuracy - max(lr_accuracy, svm_accuracy)) / max(lr_accuracy, svm_accuracy) * 100):.1f}% improvement\")\n",
    "    \n",
    "    return {\n",
    "        'logistic_regression': {'accuracy': lr_accuracy, 'f1': lr_f1, 'time': lr_train_time},\n",
    "        'svm': {'accuracy': svm_accuracy, 'f1': svm_f1, 'time': svm_train_time},\n",
    "        'bert': {'accuracy': bert_accuracy, 'time': 300}\n",
    "    }\n",
    "\n",
    "# Compare with traditional methods\n",
    "comparison_results = compare_with_traditional_ml(df, evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952f163",
   "metadata": {},
   "source": [
    "### Step 7: Attention Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5103240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_patterns(model, tokenizer, sample_text):\n",
    "    \"\"\"\n",
    "    Visualize BERT attention patterns for interpretability.\n",
    "    Note: This is a simplified visualization. Full attention analysis requires additional tools.\n",
    "    \n",
    "    Args:\n",
    "        model: Fine-tuned BERT model\n",
    "        tokenizer: BERT tokenizer\n",
    "        sample_text (str): Text to analyze\n",
    "    \"\"\"\n",
    "    # TODO: Implement attention visualization:\n",
    "    # 1. Get model attention weights\n",
    "    # 2. Visualize attention patterns\n",
    "    # 3. Identify important tokens\n",
    "    # 4. Analyze layer-wise attention\n",
    "    \n",
    "    print(f\"Analyzing attention patterns for: '{sample_text}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        sample_text,\n",
    "        return_tensors='pt',\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get model predictions and attention weights\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "    confidence = torch.max(predictions).item()\n",
    "    \n",
    "    label_names = ['negative', 'neutral', 'positive']\n",
    "    \n",
    "    print(f\"Prediction: {label_names[predicted_class]} (confidence: {confidence:.3f})\")\n",
    "    print(f\"Class probabilities:\")\n",
    "    for i, prob in enumerate(predictions[0]):\n",
    "        print(f\"  {label_names[i]}: {prob:.3f}\")\n",
    "    \n",
    "    # Get tokens (remove padding)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    # Find actual length (before padding)\n",
    "    actual_length = len([t for t in tokens if t != '[PAD]'])\n",
    "    tokens = tokens[:actual_length]\n",
    "    \n",
    "    print(f\"\\nTokens ({len(tokens)}): {tokens}\")\n",
    "    \n",
    "    # Simple attention analysis (last layer, first head)\n",
    "    if outputs.attentions:\n",
    "        # Get attention from last layer, first head\n",
    "        last_layer_attention = outputs.attentions[-1][0, 0, :actual_length, :actual_length]\n",
    "        \n",
    "        # Average attention received by each token\n",
    "        token_importance = torch.mean(last_layer_attention, dim=0).cpu().numpy()\n",
    "        \n",
    "        print(f\"\\nToken Importance (averaged attention):\")\n",
    "        for token, importance in zip(tokens, token_importance):\n",
    "            print(f\"  {token}: {importance:.3f}\")\n",
    "        \n",
    "        # Simple visualization\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Token importance plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        bars = plt.bar(range(len(tokens)), token_importance[:len(tokens)])\n",
    "        plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "        plt.title('Token Importance (Attention)')\n",
    "        plt.ylabel('Attention Score')\n",
    "        \n",
    "        # Color bars by importance\n",
    "        max_importance = max(token_importance[:len(tokens)])\n",
    "        for bar, importance in zip(bars, token_importance[:len(tokens)]):\n",
    "            bar.set_color(plt.cm.Reds(importance / max_importance))\n",
    "        \n",
    "        # Attention heatmap (simplified)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        attention_matrix = last_layer_attention.cpu().numpy()\n",
    "        sns.heatmap(\n",
    "            attention_matrix,\n",
    "            xticklabels=tokens,\n",
    "            yticklabels=tokens,\n",
    "            cmap='Blues',\n",
    "            cbar=True\n",
    "        )\n",
    "        plt.title('Attention Matrix (Last Layer, Head 1)')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nAttention weights not available (model may not output attentions)\")\n",
    "\n",
    "# Analyze attention for sample texts\n",
    "sample_texts = [\n",
    "    \"Dieser Film ist absolut fantastisch und sehr bewegend!\",\n",
    "    \"Das Produkt ist v√∂llig unbrauchbar und schlecht verarbeitet.\",\n",
    "    \"Ein durchschnittliches Restaurant mit normalem Service.\"\n",
    "]\n",
    "\n",
    "print(\"Attention Pattern Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    visualize_attention_patterns(trainer.model, tokenizer, text)\n",
    "    if i < len(sample_texts) - 1:\n",
    "        print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3833a39c",
   "metadata": {},
   "source": [
    "## Exercise Tasks\n",
    "\n",
    "Complete the following tasks to deepen your understanding:\n",
    "\n",
    "1. **Model Variants**:\n",
    "   - Try different German BERT variants (distilbert-base-german-cased, bert-base-multilingual-cased)\n",
    "   - Compare model sizes, speed, and performance\n",
    "   - Experiment with RoBERTa or ELECTRA architectures\n",
    "\n",
    "2. **Hyperparameter Tuning**:\n",
    "   - Optimize learning rate, batch size, and number of epochs\n",
    "   - Implement learning rate scheduling\n",
    "   - Add dropout and weight decay regularization\n",
    "\n",
    "3. **Advanced Evaluation**:\n",
    "   - Implement cross-validation for BERT\n",
    "   - Add more evaluation metrics (ROC-AUC, precision-recall curves)\n",
    "   - Perform statistical significance testing\n",
    "\n",
    "4. **Interpretability Analysis**:\n",
    "   - Implement LIME or SHAP for BERT explanations\n",
    "   - Analyze attention patterns across layers and heads\n",
    "   - Create token importance visualizations\n",
    "\n",
    "5. **Production Deployment**:\n",
    "   - Optimize model for inference (quantization, distillation)\n",
    "   - Create REST API for sentiment classification\n",
    "   - Implement batch processing and caching\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. What makes BERT more effective than traditional ML approaches?\n",
    "2. How does bidirectional context improve understanding?\n",
    "3. What are the computational trade-offs of using BERT?\n",
    "4. How can attention weights help interpret model decisions?\n",
    "5. When might traditional ML still be preferable to BERT?\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore task-specific BERT variants (sentiment-specific models)\n",
    "- Learn about BERT for other NLP tasks (NER, QA, etc.)\n",
    "- Study recent transformer developments (GPT, T5, etc.)\n",
    "- Investigate multilingual and cross-lingual applications"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
