{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82cdd57",
   "metadata": {},
   "source": [
    "# Exercise 5: Named Entity Recognition\n",
    "\n",
    "Welcome to Named Entity Recognition! You'll learn how to automatically identify and classify important entities in German text.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this exercise, you will be able to:\n",
    "1. **Entity Types**: Identify different types of named entities (PERSON, LOCATION, ORG, MISC)\n",
    "2. **Multi-Model Comparison**: Compare spaCy, Flair, and transformer-based NER models\n",
    "3. **German NER Challenges**: Handle German-specific issues (compound words, capitalization)\n",
    "4. **Custom Entity Training**: Train NER models for domain-specific entities\n",
    "5. **Entity Linking**: Connect entities to knowledge bases (Wikipedia, Wikidata)\n",
    "6. **Performance Evaluation**: Assess NER model quality using precision, recall, and F1-score\n",
    "\n",
    "## What You'll Build\n",
    "- Multi-model German NER system\n",
    "- Entity extraction and analysis pipeline\n",
    "- Custom entity recognizer for specific domains\n",
    "- Entity relationship visualization\n",
    "- Knowledge base linking system\n",
    "\n",
    "## Applications\n",
    "- **Information Extraction**: Extract structured data from unstructured text\n",
    "- **Document Analysis**: Analyze legal documents, news articles, research papers\n",
    "- **Privacy Protection**: Identify and anonymize personal information\n",
    "- **Knowledge Graphs**: Build connections between entities across documents\n",
    "\n",
    "**Ready to find the hidden structure in text?** üîçüìÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e1cd64",
   "metadata": {},
   "source": [
    "## Exercise 1: Multi-Model German NER Comparison\n",
    "\n",
    "**Goal**: Compare different NER approaches on German text and analyze their strengths and weaknesses.\n",
    "\n",
    "**Your Tasks**: \n",
    "1. Extract entities using spaCy's German NER model\n",
    "2. Apply Flair's German NER for comparison\n",
    "3. Use transformer-based NER models\n",
    "4. Evaluate and compare model performance\n",
    "\n",
    "**Hints**:\n",
    "- spaCy provides fast, rule-based + statistical NER\n",
    "- Flair offers context-sensitive embeddings for better accuracy\n",
    "- Transformer models (BERT-based) give state-of-the-art results\n",
    "- German capitalization rules can help with entity detection\n",
    "\n",
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e686a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple import - try to load spaCy for German\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "    print(\"German NER model loaded! Ready to find entities.\")\n",
    "except:\n",
    "    print(\"Please install German spaCy model: python -m spacy download de_core_news_sm\")\n",
    "    nlp = None\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to load German spaCy model with NER\n",
    "try:\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "    print(\"German spaCy model loaded successfully!\")\n",
    "    print(f\"Available entity types: {list(nlp.get_pipe('ner').labels)}\")\n",
    "except IOError:\n",
    "    print(\"Please install German spaCy model: python -m spacy download de_core_news_sm\")\n",
    "    nlp = None\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373fb4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for Named Entity Recognition\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to load German spaCy model with NER\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "    print(\"‚úÖ German spaCy model loaded successfully!\")\n",
    "    print(f\"   Available entity types: {list(nlp.get_pipe('ner').labels)}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Please install spaCy: pip install spacy\")\n",
    "    nlp = None\n",
    "except IOError:\n",
    "    print(\"‚ùå Please install German spaCy model: python -m spacy download de_core_news_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Try to load Flair for German NER (optional, more accurate)\n",
    "try:\n",
    "    from flair.data import Sentence\n",
    "    from flair.models import SequenceTagger\n",
    "    flair_tagger = SequenceTagger.load('de-ner')\n",
    "    print(\"‚úÖ Flair German NER model loaded!\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Flair not available. Install with: pip install flair\")\n",
    "    flair_tagger = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Flair model loading failed: {e}\")\n",
    "    flair_tagger = None\n",
    "\n",
    "# Import for transformer-based NER (optional)\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "    print(\"‚úÖ Transformers library available for BERT-based NER!\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Transformers not available. Install with: pip install transformers\")\n",
    "\n",
    "print(\"\\nüîç NER Toolkit Ready!\")\n",
    "print(\"Available tools: spaCy NER, Flair NER, Transformer-based NER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f272e1cc",
   "metadata": {},
   "source": [
    "### Step 1: Basic NER with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_spacy(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Extract named entities using spaCy.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        nlp_model: Loaded spaCy model\n",
    "    \n",
    "    Returns:\n",
    "        dict: Extracted entities with metadata\n",
    "    \"\"\"\n",
    "    if nlp_model is None:\n",
    "        print(\"spaCy model not available\")\n",
    "        return None\n",
    "    \n",
    "    # TODO: Implement NER extraction:\n",
    "    # 1. Process text with spaCy\n",
    "    # 2. Extract entities with labels and positions\n",
    "    # 3. Group entities by type\n",
    "    # 4. Calculate confidence scores if available\n",
    "    # 5. Handle overlapping entities\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    entities = []\n",
    "    entity_types = defaultdict(list)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        entity_info = {\n",
    "            'text': ent.text,\n",
    "            'label': ent.label_,\n",
    "            'description': spacy.explain(ent.label_),\n",
    "            'start': ent.start_char,\n",
    "            'end': ent.end_char,\n",
    "            'start_token': ent.start,\n",
    "            'end_token': ent.end\n",
    "        }\n",
    "        \n",
    "        entities.append(entity_info)\n",
    "        entity_types[ent.label_].append(ent.text)\n",
    "    \n",
    "    return {\n",
    "        'entities': entities,\n",
    "        'entity_types': dict(entity_types),\n",
    "        'doc': doc\n",
    "    }\n",
    "\n",
    "def display_entities(ner_result):\n",
    "    \"\"\"\n",
    "    Display extracted entities in organized format.\n",
    "    \n",
    "    Args:\n",
    "        ner_result (dict): Result from extract_entities_spacy\n",
    "    \"\"\"\n",
    "    if ner_result is None:\n",
    "        return\n",
    "    \n",
    "    entities = ner_result['entities']\n",
    "    entity_types = ner_result['entity_types']\n",
    "    \n",
    "    print(f\"Found {len(entities)} entities:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Display all entities\n",
    "    for entity in entities:\n",
    "        print(f\"'{entity['text']}' -> {entity['label']} ({entity['description']})\")\n",
    "        print(f\"  Position: {entity['start']}-{entity['end']}\")\n",
    "    \n",
    "    print(\"\\nEntities by type:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for entity_type, entity_list in entity_types.items():\n",
    "        unique_entities = list(set(entity_list))\n",
    "        print(f\"{entity_type} ({spacy.explain(entity_type)}):\")\n",
    "        print(f\"  {', '.join(unique_entities)}\")\n",
    "        print(f\"  Count: {len(entity_list)} (unique: {len(unique_entities)})\")\n",
    "        print()\n",
    "\n",
    "# Sample German texts for NER\n",
    "sample_texts = [\n",
    "    \"\"\"Angela Merkel war von 2005 bis 2021 Bundeskanzlerin von Deutschland. \n",
    "    Sie wurde am 17. Juli 1954 in Hamburg geboren und studierte Physik an der \n",
    "    Universit√§t Leipzig. Vor ihrer politischen Laufbahn arbeitete sie als \n",
    "    Wissenschaftlerin am Zentralinstitut f√ºr Physikalische Chemie in Berlin.\"\"\",\n",
    "    \n",
    "    \"\"\"Die BMW AG mit Hauptsitz in M√ºnchen ist ein deutscher Automobilhersteller. \n",
    "    Das Unternehmen wurde 1916 gegr√ºndet und besch√§ftigt heute √ºber 120.000 \n",
    "    Mitarbeiter weltweit. Der Umsatz betrug 2022 etwa 142,6 Milliarden Euro.\"\"\",\n",
    "    \n",
    "    \"\"\"Am 15. September 2023 fand in der Allianz Arena in M√ºnchen das Spiel zwischen \n",
    "    dem FC Bayern M√ºnchen und Borussia Dortmund statt. Thomas M√ºller erzielte in der \n",
    "    78. Minute das entscheidende Tor zum 2:1-Sieg.\"\"\"\n",
    "]\n",
    "\n",
    "# Extract entities from sample texts\n",
    "print(\"Named Entity Recognition Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"\\nText {i+1}:\")\n",
    "    print(f\"'{text[:100]}...'\")\n",
    "    print()\n",
    "    \n",
    "    ner_result = extract_entities_spacy(text, nlp)\n",
    "    display_entities(ner_result)\n",
    "    \n",
    "    if i < len(sample_texts) - 1:\n",
    "        print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0fc01",
   "metadata": {},
   "source": [
    "### Step 2: NER Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28f1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_entities(text, nlp_model, style=\"ent\"):\n",
    "    \"\"\"\n",
    "    Visualize named entities in text using spaCy's displacy.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        nlp_model: spaCy model\n",
    "        style (str): Visualization style ('ent' or 'dep')\n",
    "    \"\"\"\n",
    "    if nlp_model is None:\n",
    "        print(\"spaCy model not available\")\n",
    "        return\n",
    "    \n",
    "    # TODO: Create entity visualizations:\n",
    "    # 1. Use displacy for web-based visualization\n",
    "    # 2. Create custom matplotlib-based visualization\n",
    "    # 3. Add color coding for different entity types\n",
    "    # 4. Include entity statistics\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"\\nEntity Visualization:\")\n",
    "    \n",
    "    # Use displacy for HTML visualization (Jupyter)\n",
    "    try:\n",
    "        from IPython.display import HTML\n",
    "        html = displacy.render(doc, style=style, jupyter=False)\n",
    "        # Clean HTML for basic display\n",
    "        print(\"HTML visualization generated (use displacy.render for full view)\")\n",
    "        \n",
    "        # Alternative: simple text-based visualization\n",
    "        print(\"\\nSimple entity highlighting:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        highlighted_text = text\n",
    "        entities_info = []\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            entities_info.append(f\"[{ent.text}:{ent.label_}]\")\n",
    "        \n",
    "        print(f\"Entities found: {', '.join(entities_info)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Visualization error: {e}\")\n",
    "        \n",
    "        # Fallback to simple highlighting\n",
    "        print(\"Simple entity list:\")\n",
    "        for ent in doc.ents:\n",
    "            print(f\"  {ent.text} -> {ent.label_} ({spacy.explain(ent.label_)})\")\n",
    "\n",
    "def create_entity_statistics_plot(texts, nlp_model):\n",
    "    \"\"\"\n",
    "    Create statistical plots for entity analysis.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of texts to analyze\n",
    "        nlp_model: spaCy model\n",
    "    \"\"\"\n",
    "    if nlp_model is None:\n",
    "        print(\"spaCy model not available\")\n",
    "        return\n",
    "    \n",
    "    # TODO: Create comprehensive entity statistics:\n",
    "    # 1. Entity type distribution\n",
    "    # 2. Entity frequency analysis\n",
    "    # 3. Text length vs entity count\n",
    "    # 4. Most common entities by type\n",
    "    \n",
    "    all_entities = []\n",
    "    entity_type_counts = Counter()\n",
    "    entity_text_counts = Counter()\n",
    "    text_stats = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        doc = nlp_model(text)\n",
    "        text_entities = []\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            all_entities.append({\n",
    "                'text': ent.text,\n",
    "                'label': ent.label_,\n",
    "                'text_id': i\n",
    "            })\n",
    "            text_entities.append(ent.label_)\n",
    "            entity_type_counts[ent.label_] += 1\n",
    "            entity_text_counts[ent.text.lower()] += 1\n",
    "        \n",
    "        text_stats.append({\n",
    "            'text_id': i,\n",
    "            'text_length': len(text),\n",
    "            'entity_count': len(text_entities),\n",
    "            'unique_entity_types': len(set(text_entities))\n",
    "        })\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Entity type distribution\n",
    "    if entity_type_counts:\n",
    "        types, counts = zip(*entity_type_counts.most_common())\n",
    "        axes[0, 0].bar(types, counts)\n",
    "        axes[0, 0].set_title('Entity Type Distribution')\n",
    "        axes[0, 0].set_xlabel('Entity Type')\n",
    "        axes[0, 0].set_ylabel('Count')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Most common entity texts\n",
    "    if entity_text_counts:\n",
    "        top_entities = entity_text_counts.most_common(10)\n",
    "        entities, counts = zip(*top_entities)\n",
    "        axes[0, 1].barh(range(len(entities)), counts)\n",
    "        axes[0, 1].set_yticks(range(len(entities)))\n",
    "        axes[0, 1].set_yticklabels(entities)\n",
    "        axes[0, 1].set_title('Most Common Entities')\n",
    "        axes[0, 1].set_xlabel('Frequency')\n",
    "    \n",
    "    # Text length vs entity count\n",
    "    if text_stats:\n",
    "        text_lengths = [stat['text_length'] for stat in text_stats]\n",
    "        entity_counts = [stat['entity_count'] for stat in text_stats]\n",
    "        axes[1, 0].scatter(text_lengths, entity_counts)\n",
    "        axes[1, 0].set_title('Text Length vs Entity Count')\n",
    "        axes[1, 0].set_xlabel('Text Length (characters)')\n",
    "        axes[1, 0].set_ylabel('Entity Count')\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(text_lengths) > 1:\n",
    "            z = np.polyfit(text_lengths, entity_counts, 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes[1, 0].plot(text_lengths, p(text_lengths), \"r--\", alpha=0.8)\n",
    "    \n",
    "    # Entity diversity by text\n",
    "    if text_stats:\n",
    "        text_ids = [stat['text_id'] for stat in text_stats]\n",
    "        unique_types = [stat['unique_entity_types'] for stat in text_stats]\n",
    "        axes[1, 1].bar(text_ids, unique_types)\n",
    "        axes[1, 1].set_title('Entity Type Diversity by Text')\n",
    "        axes[1, 1].set_xlabel('Text ID')\n",
    "        axes[1, 1].set_ylabel('Unique Entity Types')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'entity_type_counts': entity_type_counts,\n",
    "        'entity_text_counts': entity_text_counts,\n",
    "        'text_stats': text_stats,\n",
    "        'all_entities': all_entities\n",
    "    }\n",
    "\n",
    "# Visualize entities in sample texts\n",
    "print(\"Entity Visualization Examples:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i, text in enumerate(sample_texts[:2]):  # First two texts\n",
    "    print(f\"\\nVisualization {i+1}:\")\n",
    "    visualize_entities(text, nlp)\n",
    "\n",
    "# Create statistical analysis\n",
    "print(\"\\n\\nEntity Statistical Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "entity_stats = create_entity_statistics_plot(sample_texts, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f5fc18",
   "metadata": {},
   "source": [
    "### Step 3: Entity Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf96c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_entity_cooccurrence(texts, nlp_model, window_size=3):\n",
    "    \"\"\"\n",
    "    Analyze co-occurrence patterns between entities.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of texts to analyze\n",
    "        nlp_model: spaCy model\n",
    "        window_size (int): Window size for co-occurrence\n",
    "    \n",
    "    Returns:\n",
    "        dict: Co-occurrence analysis results\n",
    "    \"\"\"\n",
    "    if nlp_model is None:\n",
    "        print(\"spaCy model not available\")\n",
    "        return None\n",
    "    \n",
    "    # TODO: Implement entity co-occurrence analysis:\n",
    "    # 1. Find entities that appear together in sentences\n",
    "    # 2. Calculate co-occurrence frequencies\n",
    "    # 3. Create entity relationship network\n",
    "    # 4. Identify entity clusters\n",
    "    # 5. Calculate relationship strength\n",
    "    \n",
    "    cooccurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "    entity_sentences = defaultdict(list)\n",
    "    all_relationships = []\n",
    "    \n",
    "    for text_id, text in enumerate(texts):\n",
    "        doc = nlp_model(text)\n",
    "        \n",
    "        # Group entities by sentence\n",
    "        for sent in doc.sents:\n",
    "            sent_entities = []\n",
    "            for ent in sent.ents:\n",
    "                sent_entities.append(ent.text.lower())\n",
    "                entity_sentences[ent.text.lower()].append((text_id, sent.text))\n",
    "            \n",
    "            # Calculate co-occurrences within sentence\n",
    "            for i, entity1 in enumerate(sent_entities):\n",
    "                for j, entity2 in enumerate(sent_entities):\n",
    "                    if i != j and abs(i - j) <= window_size:\n",
    "                        cooccurrence_matrix[entity1][entity2] += 1\n",
    "                        all_relationships.append((entity1, entity2))\n",
    "    \n",
    "    return {\n",
    "        'cooccurrence_matrix': dict(cooccurrence_matrix),\n",
    "        'entity_sentences': dict(entity_sentences),\n",
    "        'relationships': all_relationships\n",
    "    }\n",
    "\n",
    "def create_entity_network(cooccurrence_data, min_cooccurrence=1):\n",
    "    \"\"\"\n",
    "    Create and visualize entity relationship network.\n",
    "    \n",
    "    Args:\n",
    "        cooccurrence_data (dict): Co-occurrence analysis results\n",
    "        min_cooccurrence (int): Minimum co-occurrence threshold\n",
    "    \n",
    "    Returns:\n",
    "        networkx.Graph: Entity network graph\n",
    "    \"\"\"\n",
    "    if cooccurrence_data is None:\n",
    "        return None\n",
    "    \n",
    "    # TODO: Create entity relationship network:\n",
    "    # 1. Build NetworkX graph from co-occurrence data\n",
    "    # 2. Add node attributes (entity type, frequency)\n",
    "    # 3. Add edge weights (co-occurrence strength)\n",
    "    # 4. Apply layout algorithms for visualization\n",
    "    # 5. Color nodes by entity type\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    cooccurrence_matrix = cooccurrence_data['cooccurrence_matrix']\n",
    "    \n",
    "    # Add nodes and edges\n",
    "    for entity1, connections in cooccurrence_matrix.items():\n",
    "        if not G.has_node(entity1):\n",
    "            G.add_node(entity1)\n",
    "        \n",
    "        for entity2, weight in connections.items():\n",
    "            if weight >= min_cooccurrence:\n",
    "                if not G.has_node(entity2):\n",
    "                    G.add_node(entity2)\n",
    "                G.add_edge(entity1, entity2, weight=weight)\n",
    "    \n",
    "    print(f\"Entity Network Statistics:\")\n",
    "    print(f\"  Nodes (entities): {G.number_of_nodes()}\")\n",
    "    print(f\"  Edges (relationships): {G.number_of_edges()}\")\n",
    "    \n",
    "    if G.number_of_nodes() > 0:\n",
    "        # Calculate network metrics\n",
    "        if G.number_of_edges() > 0:\n",
    "            density = nx.density(G)\n",
    "            print(f\"  Network density: {density:.3f}\")\n",
    "            \n",
    "            # Find most connected entities\n",
    "            degree_centrality = nx.degree_centrality(G)\n",
    "            top_entities = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            print(f\"  Most connected entities:\")\n",
    "            for entity, centrality in top_entities:\n",
    "                print(f\"    {entity}: {centrality:.3f}\")\n",
    "        \n",
    "        # Visualize network\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        if G.number_of_nodes() <= 20:  # Only visualize if not too crowded\n",
    "            pos = nx.spring_layout(G, k=1, iterations=50)\n",
    "            \n",
    "            # Draw edges\n",
    "            edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "            nx.draw_networkx_edges(G, pos, width=[w*0.5 for w in edge_weights], alpha=0.6)\n",
    "            \n",
    "            # Draw nodes\n",
    "            node_sizes = [degree_centrality.get(node, 0) * 3000 + 100 for node in G.nodes()]\n",
    "            nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightblue', alpha=0.8)\n",
    "            \n",
    "            # Draw labels\n",
    "            nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "            \n",
    "            plt.title('Entity Relationship Network')\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"  Network too large for visualization (>20 nodes)\")\n",
    "    \n",
    "    return G\n",
    "\n",
    "def find_entity_clusters(network_graph):\n",
    "    \"\"\"\n",
    "    Find clusters of related entities.\n",
    "    \n",
    "    Args:\n",
    "        network_graph (networkx.Graph): Entity network\n",
    "    \n",
    "    Returns:\n",
    "        list: List of entity clusters\n",
    "    \"\"\"\n",
    "    if network_graph is None or network_graph.number_of_nodes() == 0:\n",
    "        return []\n",
    "    \n",
    "    # TODO: Implement entity clustering:\n",
    "    # 1. Use community detection algorithms\n",
    "    # 2. Find connected components\n",
    "    # 3. Identify entity groups by type\n",
    "    # 4. Analyze cluster characteristics\n",
    "    \n",
    "    # Find connected components (basic clustering)\n",
    "    clusters = [list(component) for component in nx.connected_components(network_graph)]\n",
    "    \n",
    "    print(f\"\\nEntity Clusters Found:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    for i, cluster in enumerate(clusters):\n",
    "        if len(cluster) > 1:  # Only show clusters with multiple entities\n",
    "            print(f\"Cluster {i+1}: {', '.join(cluster)}\")\n",
    "            print(f\"  Size: {len(cluster)} entities\")\n",
    "            \n",
    "            # Calculate cluster metrics\n",
    "            if len(cluster) > 2:\n",
    "                subgraph = network_graph.subgraph(cluster)\n",
    "                if subgraph.number_of_edges() > 0:\n",
    "                    density = nx.density(subgraph)\n",
    "                    print(f\"  Internal density: {density:.3f}\")\n",
    "            print()\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "# Analyze entity relationships\n",
    "print(\"Entity Relationship Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "cooccurrence_data = analyze_entity_cooccurrence(sample_texts, nlp)\n",
    "\n",
    "if cooccurrence_data:\n",
    "    print(\"\\nCo-occurrence Analysis Results:\")\n",
    "    print(f\"Found relationships between {len(cooccurrence_data['cooccurrence_matrix'])} entities\")\n",
    "    \n",
    "    # Create network visualization\n",
    "    entity_network = create_entity_network(cooccurrence_data)\n",
    "    \n",
    "    # Find clusters\n",
    "    if entity_network:\n",
    "        clusters = find_entity_clusters(entity_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca642a99",
   "metadata": {},
   "source": [
    "### Step 4: NER Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37b983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ner_performance(texts, true_entities, nlp_model):\n",
    "    \"\"\"\n",
    "    Evaluate NER model performance against ground truth.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of texts\n",
    "        true_entities (list): List of ground truth entities for each text\n",
    "        nlp_model: spaCy model\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    if nlp_model is None:\n",
    "        print(\"spaCy model not available\")\n",
    "        return None\n",
    "    \n",
    "    # TODO: Implement NER evaluation:\n",
    "    # 1. Extract entities from texts using model\n",
    "    # 2. Compare with ground truth annotations\n",
    "    # 3. Calculate precision, recall, F1-score\n",
    "    # 4. Analyze errors by entity type\n",
    "    # 5. Create confusion matrix for entity types\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_true = []\n",
    "    entity_type_metrics = defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0})\n",
    "    \n",
    "    for text, true_ents in zip(texts, true_entities):\n",
    "        doc = nlp_model(text)\n",
    "        predicted_ents = [(ent.text.lower(), ent.label_) for ent in doc.ents]\n",
    "        \n",
    "        # Convert ground truth to same format\n",
    "        true_ents_formatted = [(ent['text'].lower(), ent['label']) for ent in true_ents]\n",
    "        \n",
    "        all_predictions.extend(predicted_ents)\n",
    "        all_true.extend(true_ents_formatted)\n",
    "        \n",
    "        # Calculate metrics per entity type\n",
    "        pred_set = set(predicted_ents)\n",
    "        true_set = set(true_ents_formatted)\n",
    "        \n",
    "        # True positives\n",
    "        for ent in pred_set.intersection(true_set):\n",
    "            entity_type_metrics[ent[1]]['tp'] += 1\n",
    "        \n",
    "        # False positives\n",
    "        for ent in pred_set - true_set:\n",
    "            entity_type_metrics[ent[1]]['fp'] += 1\n",
    "        \n",
    "        # False negatives\n",
    "        for ent in true_set - pred_set:\n",
    "            entity_type_metrics[ent[1]]['fn'] += 1\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    total_tp = sum(metrics['tp'] for metrics in entity_type_metrics.values())\n",
    "    total_fp = sum(metrics['fp'] for metrics in entity_type_metrics.values())\n",
    "    total_fn = sum(metrics['fn'] for metrics in entity_type_metrics.values())\n",
    "    \n",
    "    overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "    \n",
    "    # Calculate per-type metrics\n",
    "    type_metrics = {}\n",
    "    for entity_type, metrics in entity_type_metrics.items():\n",
    "        precision = metrics['tp'] / (metrics['tp'] + metrics['fp']) if (metrics['tp'] + metrics['fp']) > 0 else 0\n",
    "        recall = metrics['tp'] / (metrics['tp'] + metrics['fn']) if (metrics['tp'] + metrics['fn']) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        type_metrics[entity_type] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'support': metrics['tp'] + metrics['fn']\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'overall': {\n",
    "            'precision': overall_precision,\n",
    "            'recall': overall_recall,\n",
    "            'f1': overall_f1\n",
    "        },\n",
    "        'by_type': type_metrics,\n",
    "        'confusion_data': entity_type_metrics\n",
    "    }\n",
    "\n",
    "def create_mock_ground_truth():\n",
    "    \"\"\"\n",
    "    Create mock ground truth annotations for demonstration.\n",
    "    In practice, this would come from human annotations.\n",
    "    \n",
    "    Returns:\n",
    "        list: Mock ground truth entities\n",
    "    \"\"\"\n",
    "    # TODO: Create realistic ground truth annotations:\n",
    "    # 1. Manual annotation of sample texts\n",
    "    # 2. Include various entity types\n",
    "    # 3. Handle edge cases and ambiguous entities\n",
    "    # 4. Ensure consistency in annotation scheme\n",
    "    \n",
    "    ground_truth = [\n",
    "        # Text 1: Angela Merkel biography\n",
    "        [\n",
    "            {'text': 'Angela Merkel', 'label': 'PER'},\n",
    "            {'text': '2005', 'label': 'DATE'},\n",
    "            {'text': '2021', 'label': 'DATE'},\n",
    "            {'text': 'Deutschland', 'label': 'LOC'},\n",
    "            {'text': '17. Juli 1954', 'label': 'DATE'},\n",
    "            {'text': 'Hamburg', 'label': 'LOC'},\n",
    "            {'text': 'Universit√§t Leipzig', 'label': 'ORG'},\n",
    "            {'text': 'Zentralinstitut f√ºr Physikalische Chemie', 'label': 'ORG'},\n",
    "            {'text': 'Berlin', 'label': 'LOC'}\n",
    "        ],\n",
    "        # Text 2: BMW information\n",
    "        [\n",
    "            {'text': 'BMW AG', 'label': 'ORG'},\n",
    "            {'text': 'M√ºnchen', 'label': 'LOC'},\n",
    "            {'text': '1916', 'label': 'DATE'},\n",
    "            {'text': '120.000', 'label': 'CARDINAL'},\n",
    "            {'text': '2022', 'label': 'DATE'},\n",
    "            {'text': '142,6 Milliarden Euro', 'label': 'MONEY'}\n",
    "        ],\n",
    "        # Text 3: Football match\n",
    "        [\n",
    "            {'text': '15. September 2023', 'label': 'DATE'},\n",
    "            {'text': 'Allianz Arena', 'label': 'FAC'},\n",
    "            {'text': 'M√ºnchen', 'label': 'LOC'},\n",
    "            {'text': 'FC Bayern M√ºnchen', 'label': 'ORG'},\n",
    "            {'text': 'Borussia Dortmund', 'label': 'ORG'},\n",
    "            {'text': 'Thomas M√ºller', 'label': 'PER'},\n",
    "            {'text': '78. Minute', 'label': 'TIME'}\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    return ground_truth\n",
    "\n",
    "def display_evaluation_results(evaluation_results):\n",
    "    \"\"\"\n",
    "    Display NER evaluation results in organized format.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results (dict): Evaluation results\n",
    "    \"\"\"\n",
    "    if evaluation_results is None:\n",
    "        return\n",
    "    \n",
    "    print(\"NER Evaluation Results:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall = evaluation_results['overall']\n",
    "    print(f\"Overall Performance:\")\n",
    "    print(f\"  Precision: {overall['precision']:.3f}\")\n",
    "    print(f\"  Recall: {overall['recall']:.3f}\")\n",
    "    print(f\"  F1-Score: {overall['f1']:.3f}\")\n",
    "    \n",
    "    # Per-type metrics\n",
    "    print(f\"\\nPer-Type Performance:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Type':<10} {'Precision':<10} {'Recall':<10} {'F1':<10} {'Support':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for entity_type, metrics in evaluation_results['by_type'].items():\n",
    "        print(f\"{entity_type:<10} {metrics['precision']:<10.3f} {metrics['recall']:<10.3f} {metrics['f1']:<10.3f} {metrics['support']:<10}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    if evaluation_results['by_type']:\n",
    "        types = list(evaluation_results['by_type'].keys())\n",
    "        precisions = [evaluation_results['by_type'][t]['precision'] for t in types]\n",
    "        recalls = [evaluation_results['by_type'][t]['recall'] for t in types]\n",
    "        f1s = [evaluation_results['by_type'][t]['f1'] for t in types]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Performance by type\n",
    "        x = np.arange(len(types))\n",
    "        width = 0.25\n",
    "        \n",
    "        ax1.bar(x - width, precisions, width, label='Precision', alpha=0.8)\n",
    "        ax1.bar(x, recalls, width, label='Recall', alpha=0.8)\n",
    "        ax1.bar(x + width, f1s, width, label='F1-Score', alpha=0.8)\n",
    "        \n",
    "        ax1.set_xlabel('Entity Types')\n",
    "        ax1.set_ylabel('Score')\n",
    "        ax1.set_title('NER Performance by Entity Type')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(types, rotation=45)\n",
    "        ax1.legend()\n",
    "        ax1.set_ylim(0, 1)\n",
    "        \n",
    "        # Overall metrics pie chart\n",
    "        overall_metrics = [overall['precision'], overall['recall'], overall['f1']]\n",
    "        metric_names = ['Precision', 'Recall', 'F1-Score']\n",
    "        \n",
    "        ax2.pie(overall_metrics, labels=metric_names, autopct='%1.3f', startangle=90)\n",
    "        ax2.set_title('Overall Performance Metrics')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create evaluation\n",
    "print(\"NER Model Evaluation:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create mock ground truth\n",
    "ground_truth = create_mock_ground_truth()\n",
    "\n",
    "# Evaluate model\n",
    "evaluation_results = evaluate_ner_performance(sample_texts, ground_truth, nlp)\n",
    "\n",
    "# Display results\n",
    "display_evaluation_results(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da67dfc8",
   "metadata": {},
   "source": [
    "### Step 5: Domain-Specific NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1d0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_domain_specific_ner_rules(nlp_model):\n",
    "    \"\"\"\n",
    "    Add domain-specific NER rules to enhance entity recognition.\n",
    "    \n",
    "    Args:\n",
    "        nlp_model: spaCy model\n",
    "    \n",
    "    Returns:\n",
    "        spaCy model with additional rules\n",
    "    \"\"\"\n",
    "    if nlp_model is None:\n",
    "        return None\n",
    "    \n",
    "    # TODO: Implement domain-specific enhancements:\n",
    "    # 1. Add custom entity patterns\n",
    "    # 2. Create rule-based entity recognizers\n",
    "    # 3. Handle domain-specific terminology\n",
    "    # 4. Improve entity boundary detection\n",
    "    # 5. Add post-processing rules\n",
    "    \n",
    "    from spacy.matcher import Matcher\n",
    "    from spacy.tokens import Span\n",
    "    \n",
    "    # Create matcher for custom patterns\n",
    "    matcher = Matcher(nlp_model.vocab)\n",
    "    \n",
    "    # Define patterns for German-specific entities\n",
    "    patterns = {\n",
    "        \"GERMAN_UNIVERSITY\": [\n",
    "            [{\"LOWER\": \"universit√§t\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "            [{\"LOWER\": \"hochschule\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "            [{\"LOWER\": \"fachhochschule\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}]\n",
    "        ],\n",
    "        \"GERMAN_COMPANY\": [\n",
    "            [{\"IS_TITLE\": True, \"OP\": \"+\"}, {\"LOWER\": \"ag\"}],\n",
    "            [{\"IS_TITLE\": True, \"OP\": \"+\"}, {\"LOWER\": \"gmbh\"}],\n",
    "            [{\"IS_TITLE\": True, \"OP\": \"+\"}, {\"LOWER\": \"se\"}]\n",
    "        ],\n",
    "        \"GERMAN_CITY\": [\n",
    "            [{\"LOWER\": {\"IN\": [\"berlin\", \"m√ºnchen\", \"hamburg\", \"k√∂ln\", \"frankfurt\", \"stuttgart\", \"d√ºsseldorf\", \"dortmund\", \"essen\", \"leipzig\"]}}]\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Add patterns to matcher\n",
    "    for pattern_name, pattern_list in patterns.items():\n",
    "        matcher.add(pattern_name, pattern_list)\n",
    "    \n",
    "    def add_custom_entities(doc):\n",
    "        \"\"\"Add custom entities to doc based on matcher results.\"\"\"\n",
    "        matches = matcher(doc)\n",
    "        new_ents = []\n",
    "        \n",
    "        for match_id, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            label_name = nlp_model.vocab.strings[match_id]\n",
    "            \n",
    "            # Map custom labels to standard ones\n",
    "            if \"UNIVERSITY\" in label_name or \"COMPANY\" in label_name:\n",
    "                label = \"ORG\"\n",
    "            elif \"CITY\" in label_name:\n",
    "                label = \"LOC\"\n",
    "            else:\n",
    "                label = \"MISC\"  # Miscellaneous\n",
    "            \n",
    "            new_ents.append(Span(doc, start, end, label=label))\n",
    "        \n",
    "        # Merge with existing entities, avoiding overlaps\n",
    "        existing_ents = list(doc.ents)\n",
    "        all_ents = existing_ents + new_ents\n",
    "        \n",
    "        # Remove overlapping entities (keep longer ones)\n",
    "        filtered_ents = []\n",
    "        for ent in sorted(all_ents, key=len, reverse=True):\n",
    "            if not any(ent.start < existing.end and ent.end > existing.start \n",
    "                      for existing in filtered_ents):\n",
    "                filtered_ents.append(ent)\n",
    "        \n",
    "        doc.ents = filtered_ents\n",
    "        return doc\n",
    "    \n",
    "    # Add the custom component to the pipeline\n",
    "    if \"custom_ner\" not in nlp_model.pipe_names:\n",
    "        nlp_model.add_pipe(\"custom_ner\", after=\"ner\", config={\"callback\": add_custom_entities})\n",
    "    \n",
    "    return nlp_model\n",
    "\n",
    "def compare_ner_models(text, models_dict):\n",
    "    \"\"\"\n",
    "    Compare different NER models on the same text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to analyze\n",
    "        models_dict (dict): Dictionary of model names and models\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comparison results\n",
    "    \"\"\"\n",
    "    # TODO: Implement NER model comparison:\n",
    "    # 1. Apply different models to same text\n",
    "    # 2. Compare entity extraction results\n",
    "    # 3. Analyze differences in entity recognition\n",
    "    # 4. Calculate performance metrics\n",
    "    # 5. Visualize comparison results\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(f\"Comparing NER Models on Text:\")\n",
    "    print(f\"'{text[:100]}...'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        if model is None:\n",
    "            continue\n",
    "            \n",
    "        doc = model(text)\n",
    "        entities = []\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            entities.append({\n",
    "                'text': ent.text,\n",
    "                'label': ent.label_,\n",
    "                'start': ent.start_char,\n",
    "                'end': ent.end_char\n",
    "            })\n",
    "        \n",
    "        results[model_name] = entities\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Found {len(entities)} entities\")\n",
    "        for ent in entities:\n",
    "            print(f\"    {ent['text']} -> {ent['label']} ({spacy.explain(ent['label']) or 'Unknown'})\")\n",
    "    \n",
    "    # Analyze differences\n",
    "    if len(results) > 1:\n",
    "        print(f\"\\nModel Comparison Analysis:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        all_entities = set()\n",
    "        for entities in results.values():\n",
    "            for ent in entities:\n",
    "                all_entities.add((ent['text'], ent['label']))\n",
    "        \n",
    "        print(f\"Total unique entities across all models: {len(all_entities)}\")\n",
    "        \n",
    "        # Find entities recognized by all models\n",
    "        common_entities = set([(ent['text'], ent['label']) for ent in list(results.values())[0]])\n",
    "        for entities in list(results.values())[1:]:\n",
    "            model_entities = set([(ent['text'], ent['label']) for ent in entities])\n",
    "            common_entities = common_entities.intersection(model_entities)\n",
    "        \n",
    "        print(f\"Entities recognized by all models: {len(common_entities)}\")\n",
    "        for ent_text, ent_label in common_entities:\n",
    "            print(f\"  {ent_text} ({ent_label})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test domain-specific NER enhancements\n",
    "print(\"Domain-Specific NER Enhancement:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create enhanced model\n",
    "if nlp is not None:\n",
    "    enhanced_nlp = create_domain_specific_ner_rules(nlp)\n",
    "    \n",
    "    # Test text with domain-specific entities\n",
    "    test_text = \"\"\"Die Universit√§t M√ºnchen und die BMW AG arbeiten zusammen an \n",
    "    einem Forschungsprojekt. Das Projekt wird von der Siemens AG und der \n",
    "    Fachhochschule K√∂ln unterst√ºtzt. Die Ergebnisse werden in Berlin pr√§sentiert.\"\"\"\n",
    "    \n",
    "    # Compare original and enhanced models\n",
    "    models_to_compare = {\n",
    "        \"Original spaCy\": nlp,\n",
    "        \"Enhanced spaCy\": enhanced_nlp\n",
    "    }\n",
    "    \n",
    "    comparison_results = compare_ner_models(test_text, models_to_compare)\n",
    "else:\n",
    "    print(\"spaCy model not available for domain-specific enhancement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f088d29",
   "metadata": {},
   "source": [
    "## Exercise Tasks\n",
    "\n",
    "Complete the following tasks to deepen your understanding:\n",
    "\n",
    "1. **Advanced Entity Analysis**:\n",
    "   - Implement entity disambiguation (linking entities to knowledge bases)\n",
    "   - Create entity timeline analysis for temporal entities\n",
    "   - Build entity importance ranking based on frequency and context\n",
    "\n",
    "2. **Custom NER Training**:\n",
    "   - Create training data for domain-specific entities\n",
    "   - Fine-tune a BERT model for German NER\n",
    "   - Compare custom model with pre-trained models\n",
    "\n",
    "3. **Multi-language NER**:\n",
    "   - Test NER on mixed German-English texts\n",
    "   - Compare monolingual vs multilingual models\n",
    "   - Handle code-switching scenarios\n",
    "\n",
    "4. **NER Applications**:\n",
    "   - Build an entity-based document search system\n",
    "   - Create automatic content tagging using entities\n",
    "   - Implement entity-based text summarization\n",
    "\n",
    "5. **Error Analysis and Improvement**:\n",
    "   - Analyze common NER errors and their causes\n",
    "   - Implement post-processing rules for error correction\n",
    "   - Create ensemble methods combining multiple NER approaches\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. What are the main challenges in German NER compared to English?\n",
    "2. How do you handle ambiguous entities (e.g., \"Apple\" as fruit vs company)?\n",
    "3. What evaluation metrics are most appropriate for NER tasks?\n",
    "4. How can domain knowledge improve NER performance?\n",
    "5. What are the privacy implications of NER in text processing?\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore relation extraction between identified entities\n",
    "- Study entity linking and knowledge graph construction\n",
    "- Learn about nested and overlapping entity recognition\n",
    "- Investigate zero-shot and few-shot NER approaches"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
