{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce87c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook placeholder for 09_offensive_language_detection.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56406823",
   "metadata": {},
   "source": [
    "# Exercise 9: Offensive Language Detection\n",
    "\n",
    "Welcome to content moderation! You'll learn how to build systems that can identify harmful, offensive, or inappropriate content in German text.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this exercise, you will be able to:\n",
    "1. **Content Moderation**: Understand different types of harmful content (hate speech, toxicity, abuse)\n",
    "2. **Ethical Considerations**: Address bias, fairness, and cultural sensitivity in moderation systems\n",
    "3. **Multi-class Classification**: Distinguish between different severity levels of offensive content\n",
    "4. **German Language Challenges**: Handle German-specific offensive language patterns\n",
    "5. **False Positive Handling**: Balance accuracy with avoiding over-censorship\n",
    "6. **Real-world Deployment**: Consider scalability and human-in-the-loop systems\n",
    "\n",
    "## What You'll Build\n",
    "- German offensive language classifier\n",
    "- Multi-level toxicity detection system\n",
    "- Bias analysis and mitigation tools\n",
    "- Content moderation dashboard\n",
    "- Human review integration system\n",
    "\n",
    "## Applications\n",
    "- **Social Media Platforms**: Automated content moderation for posts and comments\n",
    "- **Gaming Communities**: Chat filtering and player behavior monitoring\n",
    "- **Educational Platforms**: Safe learning environment maintenance\n",
    "- **Customer Service**: Identifying and escalating abusive interactions\n",
    "\n",
    "## ‚ö†Ô∏è Important Ethical Note\n",
    "This exercise deals with offensive content for educational purposes. We approach this topic responsibly, focusing on protection and safety rather than harm.\n",
    "\n",
    "**Ready to build safer digital spaces?** üõ°Ô∏è‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5fbd2",
   "metadata": {},
   "source": [
    "## Exercise 1: German Content Moderation System\n",
    "\n",
    "**Goal**: Build an ethical and effective German offensive language detection system.\n",
    "\n",
    "**Your Tasks**: \n",
    "1. Analyze different types of harmful content\n",
    "2. Build traditional ML baseline for offensive language detection\n",
    "3. Implement BERT-based detection for improved accuracy\n",
    "4. Evaluate model fairness and identify potential biases\n",
    "\n",
    "**Hints**:\n",
    "- Use GermEval datasets for German offensive language\n",
    "- Consider context - some words are offensive only in certain contexts\n",
    "- Balance precision and recall - false positives can be censorship\n",
    "- Always include human review for edge cases\n",
    "\n",
    "**Ethical Guidelines**:\n",
    "- Respect privacy and data protection laws\n",
    "- Consider cultural and linguistic nuances\n",
    "- Implement appeals processes for users\n",
    "- Regular bias auditing and model updates\n",
    "\n",
    "### Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514148df",
   "metadata": {},
   "source": [
    "## Exercise Tasks\n",
    "\n",
    "Complete the following tasks to deepen your understanding:\n",
    "\n",
    "1. **Dataset Analysis**:\n",
    "   - Analyze the distribution of offensive vs. non-offensive content\n",
    "   - Identify common patterns in harmful language\n",
    "   - Study the impact of different types of content (explicit vs. implicit)\n",
    "\n",
    "2. **Bias Detection and Mitigation**:\n",
    "   - Test model performance across different demographic groups\n",
    "   - Identify potential biases in classification decisions\n",
    "   - Implement bias mitigation techniques (data augmentation, fairness constraints)\n",
    "\n",
    "3. **Multi-level Classification**:\n",
    "   - Build classifiers for different severity levels (mild, moderate, severe)\n",
    "   - Implement fine-grained toxicity detection (hate speech, cyberbullying, threats)\n",
    "   - Compare binary vs. multi-class approaches\n",
    "\n",
    "4. **Context-Aware Detection**:\n",
    "   - Handle context-dependent offensive language\n",
    "   - Analyze conversation threads for escalating toxicity\n",
    "   - Implement user history-based scoring\n",
    "\n",
    "5. **Production Deployment**:\n",
    "   - Design human-in-the-loop review processes\n",
    "   - Implement appeals and feedback mechanisms\n",
    "   - Create moderation dashboard for community managers\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. How do you balance free speech with safety in content moderation?\n",
    "2. What cultural and linguistic factors affect offensive language detection?\n",
    "3. How can you ensure fairness across different user groups?\n",
    "4. What are the psychological impacts of content moderation on human reviewers?\n",
    "5. How do you handle evolving language and new forms of harmful content?\n",
    "\n",
    "## Ethical Considerations\n",
    "\n",
    "- **Privacy**: Respect user privacy while ensuring safety\n",
    "- **Transparency**: Provide clear moderation policies and explanations\n",
    "- **Appeals**: Allow users to contest moderation decisions\n",
    "- **Cultural Sensitivity**: Consider different cultural contexts and norms\n",
    "- **Continuous Monitoring**: Regular audits for bias and effectiveness\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Study adversarial attacks on content moderation systems\n",
    "- Explore multilingual and cross-platform moderation\n",
    "- Learn about legal frameworks for content moderation\n",
    "- Investigate the role of AI in broader content governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe09198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "MODEL_DIR = PROJECT_ROOT / 'models'\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def create_comprehensive_german_dataset():\n",
    "    \"\"\"\n",
    "    Create a comprehensive German offensive language dataset.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with text, label, and severity columns\n",
    "    \"\"\"\n",
    "    # TODO: In practice, use real datasets like GermEval 2018/2019\n",
    "    # This is a demonstration dataset for educational purposes\n",
    "    \n",
    "    # Non-offensive examples\n",
    "    non_offensive_texts = [\n",
    "        \"Das ist ein wirklich gutes Restaurant.\",\n",
    "        \"Ich bin sehr zufrieden mit dem Service.\",\n",
    "        \"Das Wetter ist heute sch√∂n.\",\n",
    "        \"Die Veranstaltung war interessant und lehrreich.\",\n",
    "        \"Vielen Dank f√ºr Ihre Hilfe.\",\n",
    "        \"Das Buch hat mir sehr gut gefallen.\",\n",
    "        \"Die Stadt ist wundersch√∂n im Herbst.\",\n",
    "        \"Das war eine tolle Erfahrung.\"\n",
    "    ]\n",
    "    \n",
    "    # Mildly offensive examples (inappropriate but not severe)\n",
    "    mild_offensive_texts = [\n",
    "        \"Das ist doch v√∂llig bescheuert.\",\n",
    "        \"Du spinnst ja wohl.\",\n",
    "        \"Was f√ºr ein Quatsch.\",\n",
    "        \"Das ist ja l√§cherlich.\",\n",
    "        \"Du bist echt nervig.\",\n",
    "        \"So ein Schwachsinn.\",\n",
    "        \"Das ist ja peinlich.\",\n",
    "        \"Du hast keine Ahnung.\"\n",
    "    ]\n",
    "    \n",
    "    # Highly offensive examples (strong language, but educational context)\n",
    "    highly_offensive_texts = [\n",
    "        \"Du bist ein Idiot.\",\n",
    "        \"Das ist komplett beschissen.\",\n",
    "        \"Du bist so dumm.\",\n",
    "        \"Was f√ºr ein Vollidiot.\",\n",
    "        \"Du nervst gewaltig.\",\n",
    "        \"Das ist der gr√∂√üte Mist.\",\n",
    "        \"Du bist echt bl√∂d.\",\n",
    "        \"So eine Schei√üe.\"\n",
    "    ]\n",
    "    \n",
    "    # Combine all data\n",
    "    texts = non_offensive_texts + mild_offensive_texts + highly_offensive_texts\n",
    "    labels = ([0] * len(non_offensive_texts) +      # non-offensive = 0\n",
    "             [1] * len(mild_offensive_texts) +      # mild offensive = 1\n",
    "             [2] * len(highly_offensive_texts))     # highly offensive = 2\n",
    "    \n",
    "    severity_names = ['non-offensive', 'mild_offensive', 'highly_offensive']\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'label': labels,\n",
    "        'severity': [severity_names[label] for label in labels]\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_dataset_bias(df):\n",
    "    \"\"\"\n",
    "    Analyze potential biases in the offensive language dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to analyze\n",
    "    \"\"\"\n",
    "    # TODO: Implement bias analysis:\n",
    "    # 1. Check class distribution\n",
    "    # 2. Analyze text length patterns\n",
    "    # 3. Look for demographic bias indicators\n",
    "    # 4. Check for vocabulary overlaps\n",
    "    \n",
    "    print(\"üìä Dataset Bias Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Class distribution\n",
    "    print(\"\\n1. Class Distribution:\")\n",
    "    print(df['severity'].value_counts())\n",
    "    print(f\"Class balance ratio: {df['severity'].value_counts().min() / df['severity'].value_counts().max():.2f}\")\n",
    "    \n",
    "    # Text length analysis\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    print(\"\\n2. Text Length Statistics by Class:\")\n",
    "    print(df.groupby('severity')['text_length'].describe())\n",
    "    \n",
    "    # Vocabulary analysis\n",
    "    print(\"\\n3. Vocabulary Analysis:\")\n",
    "    all_words = ' '.join(df['text']).lower().split()\n",
    "    from collections import Counter\n",
    "    word_freq = Counter(all_words)\n",
    "    print(f\"Total unique words: {len(word_freq)}\")\n",
    "    print(f\"Most common words: {word_freq.most_common(5)}\")\n",
    "\n",
    "def build_baseline_classifier(df):\n",
    "    \"\"\"\n",
    "    Build a baseline classifier for offensive language detection.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Training dataset\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (classifier, vectorizer, performance_metrics)\n",
    "    \"\"\"\n",
    "    # TODO: Build baseline classifier with:\n",
    "    # 1. TF-IDF vectorization with appropriate parameters\n",
    "    # 2. Multiple algorithm comparison\n",
    "    # 3. Cross-validation evaluation\n",
    "    # 4. Performance metrics calculation\n",
    "    \n",
    "    print(\"üöÄ Building Baseline Classifier\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['text'], df['label'], \n",
    "        test_size=0.3, \n",
    "        random_state=42, \n",
    "        stratify=df['label']\n",
    "    )\n",
    "    \n",
    "    # Vectorization with German-specific settings\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=5000,\n",
    "        min_df=1,\n",
    "        max_df=0.95,\n",
    "        strip_accents='unicode',\n",
    "        lowercase=True\n",
    "    )\n",
    "    \n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Train classifier\n",
    "    classifier = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'  # Handle class imbalance\n",
    "    )\n",
    "    \n",
    "    classifier.fit(X_train_vec, y_train)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    y_pred = classifier.predict(X_test_vec)\n",
    "    \n",
    "    print(\"\\nüìà Baseline Performance:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['non-offensive', 'mild', 'highly_offensive']))\n",
    "    \n",
    "    return classifier, vectorizer, {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "\n",
    "# Create comprehensive dataset\n",
    "print(\"üìÅ Creating Comprehensive German Offensive Language Dataset...\")\n",
    "df = create_comprehensive_german_dataset()\n",
    "\n",
    "print(f\"\\nDataset created with {len(df)} samples\")\n",
    "print(f\"Class distribution:\")\n",
    "print(df['severity'].value_counts())\n",
    "\n",
    "# Analyze dataset for potential biases\n",
    "analyze_dataset_bias(df)\n",
    "\n",
    "# Build and evaluate baseline classifier\n",
    "classifier, vectorizer, metrics = build_baseline_classifier(df)\n",
    "\n",
    "# Save model\n",
    "import joblib\n",
    "joblib.dump({\n",
    "    'classifier': classifier, \n",
    "    'vectorizer': vectorizer,\n",
    "    'label_mapping': {0: 'non-offensive', 1: 'mild_offensive', 2: 'highly_offensive'}\n",
    "}, MODEL_DIR / 'comprehensive_offensive_language_classifier.joblib')\n",
    "\n",
    "print(f\"\\nüíæ Model saved to: {MODEL_DIR / 'comprehensive_offensive_language_classifier.joblib'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f46bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_bert_classifier(df):\n",
    "    \"\"\"\n",
    "    Implement BERT-based offensive language classification.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset for training\n",
    "        \n",
    "    Returns:\n",
    "        dict: BERT model results and comparison\n",
    "    \"\"\"\n",
    "    # TODO: Implement BERT classifier:\n",
    "    # 1. Load German BERT model for sequence classification\n",
    "    # 2. Prepare data in BERT format\n",
    "    # 3. Fine-tune or use pre-trained model\n",
    "    # 4. Compare with baseline performance\n",
    "    \n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "        import torch\n",
    "        \n",
    "        print(\"ü§ñ Implementing BERT-based Classification\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Try German BERT models\n",
    "        german_models = [\n",
    "            'dbmdz/bert-base-german-cased',\n",
    "            'bert-base-german-cased',\n",
    "            'distilbert-base-german-cased'\n",
    "        ]\n",
    "        \n",
    "        for model_name in german_models:\n",
    "            try:\n",
    "                print(f\"\\nTrying model: {model_name}\")\n",
    "                \n",
    "                # Load tokenizer\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                \n",
    "                # For demonstration, use a simple classification approach\n",
    "                # In practice, you would fine-tune the model\n",
    "                \n",
    "                print(f\"‚úÖ Successfully loaded tokenizer for {model_name}\")\n",
    "                \n",
    "                # Tokenize sample texts\n",
    "                sample_texts = df['text'].head(3).tolist()\n",
    "                print(f\"\\nüìù Tokenization Example:\")\n",
    "                \n",
    "                for text in sample_texts:\n",
    "                    tokens = tokenizer(text, truncation=True, padding=True, return_tensors='pt')\n",
    "                    print(f\"Text: {text}\")\n",
    "                    print(f\"Tokens: {tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])[:10]}...\")\n",
    "                    print()\n",
    "                \n",
    "                return {\n",
    "                    'model_name': model_name,\n",
    "                    'tokenizer': tokenizer,\n",
    "                    'status': 'loaded_successfully'\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to load {model_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"‚ö†Ô∏è  Could not load any German BERT model\")\n",
    "        return {'status': 'failed', 'error': 'No models available'}\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå Transformers library not available\")\n",
    "        print(\"Install with: pip install transformers torch\")\n",
    "        return {'status': 'failed', 'error': 'Missing dependencies'}\n",
    "\n",
    "def evaluate_ethical_considerations():\n",
    "    \"\"\"\n",
    "    Evaluate ethical considerations in offensive language detection.\n",
    "    \"\"\"\n",
    "    # TODO: Address ethical considerations:\n",
    "    # 1. Bias detection and mitigation\n",
    "    # 2. Cultural sensitivity analysis\n",
    "    # 3. False positive impact assessment\n",
    "    # 4. Privacy and user rights considerations\n",
    "    \n",
    "    print(\"‚öñÔ∏è  Ethical Considerations in Content Moderation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    ethical_guidelines = {\n",
    "        'Bias Mitigation': [\n",
    "            'Test model performance across different demographic groups',\n",
    "            'Regularly audit model decisions for unfair bias',\n",
    "            'Include diverse perspectives in dataset creation',\n",
    "            'Implement bias detection metrics in evaluation'\n",
    "        ],\n",
    "        'Cultural Sensitivity': [\n",
    "            'Consider cultural context in language interpretation',\n",
    "            'Account for regional language variations',\n",
    "            'Respect cultural differences in expression',\n",
    "            'Involve native speakers in model validation'\n",
    "        ],\n",
    "        'False Positive Management': [\n",
    "            'Balance precision and recall to minimize censorship',\n",
    "            'Implement appeals process for disputed decisions',\n",
    "            'Provide clear explanations for moderation actions',\n",
    "            'Regular review of borderline cases'\n",
    "        ],\n",
    "        'Privacy and Rights': [\n",
    "            'Respect user privacy in content analysis',\n",
    "            'Provide transparency in moderation policies',\n",
    "            'Allow user control over their content',\n",
    "            'Comply with data protection regulations'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, guidelines in ethical_guidelines.items():\n",
    "        print(f\"\\nüî∏ {category}:\")\n",
    "        for guideline in guidelines:\n",
    "            print(f\"  ‚Ä¢ {guideline}\")\n",
    "    \n",
    "    print(f\"\\nüí° Key Principle: Build systems that protect users while respecting rights and diversity\")\n",
    "\n",
    "# Test BERT implementation\n",
    "bert_results = implement_bert_classifier(df)\n",
    "\n",
    "# Discuss ethical considerations\n",
    "evaluate_ethical_considerations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4cc0b5",
   "metadata": {},
   "source": [
    "## Next steps (simple)\n",
    "- Improve baseline by expanding training data\n",
    "- Use class weighting for imbalanced datasets\n",
    "- Optionally fine-tune a transformer if you have GPU and enough data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
