{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a03f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook placeholder for 07_speech_recognition.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbcb704",
   "metadata": {},
   "source": [
    "# Exercise 7: Speech Recognition\n",
    "\n",
    "Welcome to speech recognition! You'll learn how to convert spoken German into text using modern deep learning models.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this exercise, you will be able to:\n",
    "1. **Audio Processing**: Handle audio files, sampling rates, and preprocessing\n",
    "2. **German ASR Models**: Use pre-trained German speech recognition models\n",
    "3. **Real-time Processing**: Implement streaming speech recognition\n",
    "4. **Audio Feature Extraction**: Understand MFCCs, spectrograms, and mel-scale features\n",
    "5. **Model Comparison**: Compare different ASR approaches (Wav2Vec2, Whisper, DeepSpeech)\n",
    "6. **Post-processing**: Clean and improve transcription results\n",
    "\n",
    "## What You'll Build\n",
    "- German speech-to-text system\n",
    "- Audio file batch processor\n",
    "- Real-time speech recognition interface\n",
    "- Transcription quality evaluator\n",
    "- Multi-speaker recognition system\n",
    "\n",
    "## Applications\n",
    "- **Voice Assistants**: Convert voice commands to text\n",
    "- **Meeting Transcription**: Automatic meeting minutes generation\n",
    "- **Accessibility**: Voice-controlled interfaces for disabled users\n",
    "- **Content Creation**: Podcast and video transcription services\n",
    "\n",
    "**Ready to give voice to your applications?** üé§üîä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c57c1",
   "metadata": {},
   "source": [
    "## Exercise 1: German Speech Recognition Pipeline\n",
    "\n",
    "**Goal**: Build a complete German speech recognition system using pre-trained models.\n",
    "\n",
    "**Your Tasks**: \n",
    "1. Set up audio processing pipeline\n",
    "2. Load and test German ASR models\n",
    "3. Process different audio types and qualities\n",
    "4. Evaluate transcription accuracy\n",
    "\n",
    "**Hints**:\n",
    "- Use 16kHz sampling rate for most ASR models\n",
    "- Wav2Vec2 models often work best for German\n",
    "- Longer audio files may need chunking\n",
    "- Background noise significantly affects accuracy\n",
    "\n",
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387633f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for speech recognition\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import audio processing libraries\n",
    "try:\n",
    "    import soundfile as sf\n",
    "    import librosa\n",
    "    AUDIO_LIBS_AVAILABLE = True\n",
    "    print(\"‚úÖ Audio processing libraries (soundfile, librosa) available!\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Audio libraries not available. Install with: pip install soundfile librosa\")\n",
    "    AUDIO_LIBS_AVAILABLE = False\n",
    "\n",
    "# Try to import speech recognition libraries\n",
    "try:\n",
    "    from transformers import pipeline, AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "    import torch\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "    print(\"‚úÖ Transformers library available for ASR!\")\n",
    "    \n",
    "    # Check device availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"   Using device: {device}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Transformers not available. Install with: pip install transformers torch\")\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    device = 'cpu'\n",
    "\n",
    "# Try speech_recognition library as alternative\n",
    "try:\n",
    "    import speech_recognition as sr\n",
    "    SPEECH_RECOGNITION_AVAILABLE = True\n",
    "    print(\"‚úÖ SpeechRecognition library available!\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå SpeechRecognition not available. Install with: pip install SpeechRecognition\")\n",
    "    SPEECH_RECOGNITION_AVAILABLE = False\n",
    "\n",
    "# Setup directories\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "AUDIO_DIR = PROJECT_ROOT / 'data' / 'audio'\n",
    "AUDIO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüé§ Speech Recognition Toolkit Status:\")\n",
    "print(f\"   Audio processing: {'Available' if AUDIO_LIBS_AVAILABLE else 'Not available'}\")\n",
    "print(f\"   Transformers ASR: {'Available' if TRANSFORMERS_AVAILABLE else 'Not available'}\")\n",
    "print(f\"   SpeechRecognition: {'Available' if SPEECH_RECOGNITION_AVAILABLE else 'Not available'}\")\n",
    "print(f\"   Audio directory: {AUDIO_DIR}\")\n",
    "\n",
    "def create_sample_audio_files():\n",
    "    \"\"\"Create sample audio files for testing.\"\"\"\n",
    "    \n",
    "    if not AUDIO_LIBS_AVAILABLE:\n",
    "        print(\"Cannot create sample audio - audio libraries not available\")\n",
    "        return []\n",
    "    \n",
    "    print(\"\\nüîä Creating Sample Audio Files...\")\n",
    "    \n",
    "    sample_files = []\n",
    "    \n",
    "    # Create different types of sample audio\n",
    "    sample_configs = [\n",
    "        {'name': 'tone_440hz.wav', 'freq': 440, 'duration': 1.0, 'description': '440Hz tone (1 second)'},\n",
    "        {'name': 'tone_880hz.wav', 'freq': 880, 'duration': 0.5, 'description': '880Hz tone (0.5 seconds)'},\n",
    "        {'name': 'noise_sample.wav', 'freq': None, 'duration': 0.8, 'description': 'White noise sample'}\n",
    "    ]\n",
    "    \n",
    "    for config in sample_configs:\n",
    "        file_path = AUDIO_DIR / config['name']\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            sr = 16000  # Standard sample rate for speech\n",
    "            duration = config['duration']\n",
    "            t = np.linspace(0, duration, int(sr * duration), False)\n",
    "            \n",
    "            if config['freq'] is None:\n",
    "                # Create white noise\n",
    "                signal = 0.1 * np.random.normal(0, 1, len(t))\n",
    "            else:\n",
    "                # Create sine wave tone\n",
    "                signal = 0.1 * np.sin(2 * np.pi * config['freq'] * t)\n",
    "            \n",
    "            sf.write(str(file_path), signal, sr)\n",
    "            print(f\"   Created: {config['name']} - {config['description']}\")\n",
    "        else:\n",
    "            print(f\"   Found existing: {config['name']}\")\n",
    "        \n",
    "        sample_files.append(file_path)\n",
    "    \n",
    "    return sample_files\n",
    "\n",
    "# Create sample audio files\n",
    "sample_audio_files = create_sample_audio_files() if AUDIO_LIBS_AVAILABLE else []\n",
    "print(f\"\\nüéµ Ready with {len(sample_audio_files)} sample audio files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49456e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_german_asr_models():\n",
    "    \"\"\"\n",
    "    Load and compare different German ASR models.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of loaded ASR models\n",
    "    \"\"\"\n",
    "    # TODO: Load multiple German ASR models:\n",
    "    # 1. Wav2Vec2 models for German\n",
    "    # 2. Whisper models (if available)\n",
    "    # 3. Alternative German speech models\n",
    "    # 4. Handle model loading errors gracefully\n",
    "    \n",
    "    if not TRANSFORMERS_AVAILABLE:\n",
    "        print(\"Transformers library not available for ASR model loading\")\n",
    "        return {}\n",
    "    \n",
    "    print(\"ü§ñ Loading German ASR Models...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    german_asr_models = {\n",
    "        'wav2vec2_german': 'jonatasgrosman/wav2vec2-large-xlsr-53-german',\n",
    "        'wav2vec2_german_cv': 'facebook/wav2vec2-large-xlsr-53-german',\n",
    "        'wav2vec2_multilingual': 'facebook/wav2vec2-large-xlsr-53',\n",
    "    }\n",
    "    \n",
    "    loaded_models = {}\n",
    "    \n",
    "    for model_name, model_id in german_asr_models.items():\n",
    "        try:\n",
    "            print(f\"\\nLoading {model_name} ({model_id})...\")\n",
    "            \n",
    "            # Create ASR pipeline\n",
    "            asr_pipeline = pipeline(\n",
    "                \"automatic-speech-recognition\",\n",
    "                model=model_id,\n",
    "                device=0 if torch.cuda.is_available() else -1,\n",
    "                return_timestamps=False\n",
    "            )\n",
    "            \n",
    "            loaded_models[model_name] = {\n",
    "                'pipeline': asr_pipeline,\n",
    "                'model_id': model_id,\n",
    "                'status': 'loaded'\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Successfully loaded {model_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {model_name}: {e}\")\n",
    "            loaded_models[model_name] = {\n",
    "                'pipeline': None,\n",
    "                'model_id': model_id,\n",
    "                'status': 'failed',\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    print(f\"\\nüéØ Successfully loaded {sum(1 for m in loaded_models.values() if m['status'] == 'loaded')} out of {len(german_asr_models)} models\")\n",
    "    \n",
    "    return loaded_models\n",
    "\n",
    "def analyze_audio_file(audio_path):\n",
    "    \"\"\"\n",
    "    Analyze audio file properties and visualize waveform.\n",
    "    \n",
    "    Args:\n",
    "        audio_path (Path): Path to audio file\n",
    "    \n",
    "    Returns:\n",
    "        dict: Audio analysis results\n",
    "    \"\"\"\n",
    "    # TODO: Implement audio analysis:\n",
    "    # 1. Load audio file and extract properties\n",
    "    # 2. Calculate duration, sample rate, channels\n",
    "    # 3. Visualize waveform and spectrogram\n",
    "    # 4. Detect silence and speech segments\n",
    "    \n",
    "    if not AUDIO_LIBS_AVAILABLE:\n",
    "        print(\"Audio libraries not available for audio analysis\")\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        print(f\"üéµ Analyzing Audio: {audio_path.name}\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Load audio file\n",
    "        audio_data, sample_rate = librosa.load(str(audio_path), sr=None)\n",
    "        \n",
    "        # Calculate properties\n",
    "        duration = len(audio_data) / sample_rate\n",
    "        max_amplitude = np.max(np.abs(audio_data))\n",
    "        rms_energy = np.sqrt(np.mean(audio_data**2))\n",
    "        \n",
    "        print(f\"üìä Audio Properties:\")\n",
    "        print(f\"   Duration: {duration:.2f} seconds\")\n",
    "        print(f\"   Sample rate: {sample_rate} Hz\")\n",
    "        print(f\"   Samples: {len(audio_data)}\")\n",
    "        print(f\"   Max amplitude: {max_amplitude:.4f}\")\n",
    "        print(f\"   RMS energy: {rms_energy:.4f}\")\n",
    "        \n",
    "        # Create visualizations\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Waveform plot\n",
    "        time_axis = np.linspace(0, duration, len(audio_data))\n",
    "        axes[0].plot(time_axis, audio_data)\n",
    "        axes[0].set_title(f'Waveform - {audio_path.name}')\n",
    "        axes[0].set_xlabel('Time (seconds)')\n",
    "        axes[0].set_ylabel('Amplitude')\n",
    "        axes[0].grid(True)\n",
    "        \n",
    "        # Spectrogram\n",
    "        spectrogram = librosa.stft(audio_data)\n",
    "        spectrogram_db = librosa.amplitude_to_db(np.abs(spectrogram))\n",
    "        \n",
    "        img = axes[1].imshow(spectrogram_db, aspect='auto', origin='lower', \n",
    "                           extent=[0, duration, 0, sample_rate/2])\n",
    "        axes[1].set_title('Spectrogram')\n",
    "        axes[1].set_xlabel('Time (seconds)')\n",
    "        axes[1].set_ylabel('Frequency (Hz)')\n",
    "        plt.colorbar(img, ax=axes[1], label='Magnitude (dB)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'duration': duration,\n",
    "            'sample_rate': sample_rate,\n",
    "            'samples': len(audio_data),\n",
    "            'max_amplitude': max_amplitude,\n",
    "            'rms_energy': rms_energy,\n",
    "            'audio_data': audio_data\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Audio analysis failed: {e}\")\n",
    "        return {}\n",
    "\n",
    "def transcribe_audio_multiple_models(audio_path, asr_models):\n",
    "    \"\"\"\n",
    "    Transcribe audio using multiple ASR models and compare results.\n",
    "    \n",
    "    Args:\n",
    "        audio_path (Path): Path to audio file\n",
    "        asr_models (dict): Dictionary of loaded ASR models\n",
    "    \n",
    "    Returns:\n",
    "        dict: Transcription results from different models\n",
    "    \"\"\"\n",
    "    # TODO: Implement multi-model transcription:\n",
    "    # 1. Transcribe audio with each available model\n",
    "    # 2. Compare transcription results\n",
    "    # 3. Measure transcription confidence (if available)\n",
    "    # 4. Handle different audio formats and lengths\n",
    "    \n",
    "    print(f\"üéôÔ∏è  Multi-Model Speech Recognition\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Audio file: {audio_path.name}\")\n",
    "    print()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model_data in asr_models.items():\n",
    "        if model_data['status'] != 'loaded':\n",
    "            print(f\"‚è≠Ô∏è  Skipping {model_name}: {model_data['status']}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"üîÑ Transcribing with {model_name}...\")\n",
    "            \n",
    "            # Transcribe audio\n",
    "            pipeline = model_data['pipeline']\n",
    "            result = pipeline(str(audio_path))\n",
    "            \n",
    "            # Extract text and confidence (if available)\n",
    "            if isinstance(result, dict):\n",
    "                transcription = result.get('text', '')\n",
    "                confidence = result.get('confidence', 'N/A')\n",
    "            else:\n",
    "                transcription = str(result)\n",
    "                confidence = 'N/A'\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'transcription': transcription,\n",
    "                'confidence': confidence,\n",
    "                'model_id': model_data['model_id'],\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ {model_name}: '{transcription}'\")\n",
    "            if confidence != 'N/A':\n",
    "                print(f\"   Confidence: {confidence}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {model_name} failed: {e}\")\n",
    "            results[model_name] = {\n",
    "                'transcription': '',\n",
    "                'confidence': 'N/A',\n",
    "                'model_id': model_data['model_id'],\n",
    "                'status': 'failed',\n",
    "                'error': str(e)\n",
    "            }\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load German ASR models\n",
    "print(\"üöÄ Initializing German Speech Recognition System...\")\n",
    "asr_models = load_german_asr_models()\n",
    "\n",
    "# Analyze sample audio files\n",
    "if sample_audio_files and AUDIO_LIBS_AVAILABLE:\n",
    "    print(\"\\nüîç Analyzing Sample Audio Files...\")\n",
    "    for audio_file in sample_audio_files[:1]:  # Analyze first file\n",
    "        audio_analysis = analyze_audio_file(audio_file)\n",
    "        break\n",
    "\n",
    "# Test speech recognition on sample files\n",
    "if asr_models and sample_audio_files:\n",
    "    print(\"\\nüéØ Testing Speech Recognition...\")\n",
    "    for audio_file in sample_audio_files[:1]:  # Test first file\n",
    "        transcription_results = transcribe_audio_multiple_models(audio_file, asr_models)\n",
    "        break\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No models or audio files available for testing\")\n",
    "    print(\"This is expected for synthetic audio - real speech audio needed for meaningful transcription\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274376df",
   "metadata": {},
   "source": [
    "## Next steps (simple)\n",
    "- Replace the placeholder audio with a real recording in `data/audio/`\n",
    "- Install `transformers`, `librosa`, and `soundfile` in your venv and re-run the transcription cell"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
