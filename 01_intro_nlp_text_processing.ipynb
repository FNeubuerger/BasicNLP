{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b539e8b",
   "metadata": {},
   "source": [
    "# Exercise 1: Introduction to NLP and Text Processing\n",
    "\n",
    "Welcome to Natural Language Processing! In this notebook, you'll learn the fundamental concepts and techniques for working with text data.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this exercise, you will be able to:\n",
    "1. **Tokenization**: Split text into meaningful units (words, sentences)\n",
    "2. **Text Statistics**: Calculate basic metrics (word count, character count, etc.)\n",
    "3. **Stop Word Removal**: Filter out common, non-informative words\n",
    "4. **Normalization**: Convert text to consistent format (lowercase, etc.)\n",
    "5. **Frequency Analysis**: Find most common words and patterns\n",
    "6. **Visualization**: Create charts to understand text data\n",
    "\n",
    "## What You'll Build\n",
    "- A complete text preprocessing pipeline\n",
    "- Text analysis tools for German language\n",
    "- Visualization functions for text statistics\n",
    "- A reusable NLP toolkit for future projects\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Understanding of lists, dictionaries, and functions\n",
    "- No prior NLP knowledge required!\n",
    "\n",
    "**Ready to start your NLP journey?** Let's go! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a906f4",
   "metadata": {},
   "source": [
    "## Exercise 1: Setting Up Your NLP Toolkit\n",
    "\n",
    "**Goal**: Import necessary libraries and understand the NLP ecosystem.\n",
    "\n",
    "**Your Task**: Set up the libraries you'll need for text processing, with fallbacks for missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c67e345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK loaded successfully!\n",
      "Ready to start!\n"
     ]
    }
   ],
   "source": [
    "# Essential imports for text processing\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to import advanced NLP libraries with helpful error handling\n",
    "try:\n",
    "    import nltk\n",
    "    # Download required NLTK data\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    NLTK_AVAILABLE = True\n",
    "    print(\"‚úÖ NLTK loaded successfully!\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå NLTK not available. Install with: pip install nltk\")\n",
    "    NLTK_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    # Try to load German model\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "    SPACY_AVAILABLE = True\n",
    "    print(\"‚úÖ spaCy German model loaded!\")\n",
    "except (ImportError, OSError):\n",
    "    print(\"‚ùå spaCy German model not available.\")\n",
    "    print(\"   Install with: python -m spacy download de_core_news_sm\")\n",
    "    SPACY_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nüìö NLP Toolkit Status:\")\n",
    "print(f\"   NLTK: {'Available' if NLTK_AVAILABLE else 'Not available (using fallbacks)'}\")\n",
    "print(f\"   spaCy: {'Available' if SPACY_AVAILABLE else 'Not available (using fallbacks)'}\")\n",
    "print(\"\\nüöÄ Ready to start text processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89c1aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our text to work with:\n",
      "\n",
      "Ich liebe Pizza und Pasta. Computer sind sehr n√ºtzlich heute. \n",
      "Die Sonne scheint hell und warm. Hunde sind treue Freunde.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple German text for practice\n",
    "german_text = \"\"\"\n",
    "Ich liebe Pizza und Pasta. Computer sind sehr n√ºtzlich heute. \n",
    "Die Sonne scheint hell und warm. Hunde sind treue Freunde.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Our text to work with:\")\n",
    "print(german_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af767124",
   "metadata": {},
   "source": [
    "### Step 1: Basic Text Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443a9fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 123\n",
      "Words: 20\n",
      "Sentences: 4\n",
      "Average words per sentence: 5.0\n"
     ]
    }
   ],
   "source": [
    "def analyze_text_statistics(text):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive text statistics.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing various text statistics\n",
    "    \"\"\"\n",
    "    # TODO: Implement comprehensive text analysis:\n",
    "    # 1. Count characters, words, sentences\n",
    "    # 2. Calculate average word/sentence length\n",
    "    # 3. Find unique words and vocabulary size\n",
    "    # 4. Analyze punctuation usage\n",
    "    \n",
    "    # Clean text for analysis\n",
    "    clean_text = text.strip()\n",
    "    \n",
    "    # Basic counts\n",
    "    char_count = len(clean_text)\n",
    "    char_count_no_spaces = len(clean_text.replace(' ', ''))\n",
    "    \n",
    "    # Word analysis\n",
    "    words = clean_text.split()\n",
    "    word_count = len(words)\n",
    "    unique_words = set(word.lower().strip(string.punctuation) for word in words)\n",
    "    vocabulary_size = len(unique_words)\n",
    "    \n",
    "    # Sentence analysis (simple approach)\n",
    "    sentences = [s.strip() for s in clean_text.split('.') if s.strip()]\n",
    "    sentence_count = len(sentences)\n",
    "    \n",
    "    # Average lengths\n",
    "    avg_word_length = sum(len(word) for word in words) / word_count if word_count > 0 else 0\n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
    "    \n",
    "    # Punctuation analysis\n",
    "    punctuation_count = sum(1 for char in clean_text if char in string.punctuation)\n",
    "    \n",
    "    stats = {\n",
    "        'characters_total': char_count,\n",
    "        'characters_no_spaces': char_count_no_spaces,\n",
    "        'words_total': word_count,\n",
    "        'words_unique': vocabulary_size,\n",
    "        'sentences': sentence_count,\n",
    "        'avg_word_length': round(avg_word_length, 2),\n",
    "        'avg_sentence_length': round(avg_sentence_length, 2),\n",
    "        'punctuation_marks': punctuation_count,\n",
    "        'vocabulary_richness': round(vocabulary_size / word_count * 100, 2) if word_count > 0 else 0\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def display_text_statistics(text, title=\"Text Analysis\"):\n",
    "    \"\"\"Display formatted text statistics.\"\"\"\n",
    "    \n",
    "    print(f\"üìä {title}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    stats = analyze_text_statistics(text)\n",
    "    \n",
    "    print(f\"üìù Basic Counts:\")\n",
    "    print(f\"   Characters (total): {stats['characters_total']}\")\n",
    "    print(f\"   Characters (no spaces): {stats['characters_no_spaces']}\")\n",
    "    print(f\"   Words (total): {stats['words_total']}\")\n",
    "    print(f\"   Words (unique): {stats['words_unique']}\")\n",
    "    print(f\"   Sentences: {stats['sentences']}\")\n",
    "    \n",
    "    print(f\"\\nüìè Averages:\")\n",
    "    print(f\"   Average word length: {stats['avg_word_length']} characters\")\n",
    "    print(f\"   Average sentence length: {stats['avg_sentence_length']} words\")\n",
    "    \n",
    "    print(f\"\\nüéØ Text Quality:\")\n",
    "    print(f\"   Punctuation marks: {stats['punctuation_marks']}\")\n",
    "    print(f\"   Vocabulary richness: {stats['vocabulary_richness']}%\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Analyze our sample text\n",
    "sample_stats = display_text_statistics(german_text, \"German Sample Text Analysis\")\n",
    "\n",
    "# Let's try it!\n",
    "chars, words, sentences = count_basic_things(german_text)\n",
    "print(f\"Characters: {chars}\")\n",
    "print(f\"Words: {words}\")\n",
    "print(f\"Sentences: {sentences}\")\n",
    "print(f\"Average words per sentence: {words/sentences:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713941e3",
   "metadata": {},
   "source": [
    "### Step 2: Advanced Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_multiple_methods(text):\n",
    "    \"\"\"\n",
    "    Compare different tokenization approaches.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to tokenize\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results from different tokenization methods\n",
    "    \"\"\"\n",
    "    # TODO: Implement multiple tokenization approaches:\n",
    "    # 1. Simple whitespace splitting\n",
    "    # 2. Regular expression-based tokenization\n",
    "    # 3. NLTK tokenization (if available)\n",
    "    # 4. spaCy tokenization (if available)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Method 1: Simple whitespace splitting\n",
    "    simple_tokens = text.split()\n",
    "    results['simple_split'] = {\n",
    "        'tokens': simple_tokens,\n",
    "        'count': len(simple_tokens),\n",
    "        'method': 'Whitespace splitting'\n",
    "    }\n",
    "    \n",
    "    # Method 2: Regular expression tokenization\n",
    "    # This handles punctuation better\n",
    "    import re\n",
    "    regex_pattern = r'\\b\\w+\\b'  # Word boundaries\n",
    "    regex_tokens = re.findall(regex_pattern, text.lower())\n",
    "    results['regex'] = {\n",
    "        'tokens': regex_tokens,\n",
    "        'count': len(regex_tokens),\n",
    "        'method': 'Regex word boundaries'\n",
    "    }\n",
    "    \n",
    "    # Method 3: NLTK tokenization (if available)\n",
    "    if NLTK_AVAILABLE:\n",
    "        try:\n",
    "            nltk_tokens = word_tokenize(text.lower(), language='german')\n",
    "            results['nltk'] = {\n",
    "                'tokens': nltk_tokens,\n",
    "                'count': len(nltk_tokens),\n",
    "                'method': 'NLTK German tokenizer'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            results['nltk'] = {'error': f\"NLTK tokenization failed: {e}\"}\n",
    "    else:\n",
    "        results['nltk'] = {'error': 'NLTK not available'}\n",
    "    \n",
    "    # Method 4: spaCy tokenization (if available)\n",
    "    if SPACY_AVAILABLE:\n",
    "        try:\n",
    "            doc = nlp(text)\n",
    "            spacy_tokens = [token.text.lower() for token in doc if not token.is_space]\n",
    "            results['spacy'] = {\n",
    "                'tokens': spacy_tokens,\n",
    "                'count': len(spacy_tokens),\n",
    "                'method': 'spaCy German model'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            results['spacy'] = {'error': f\"spaCy tokenization failed: {e}\"}\n",
    "    else:\n",
    "        results['spacy'] = {'error': 'spaCy not available'}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_tokenization_methods(text):\n",
    "    \"\"\"Compare and display different tokenization results.\"\"\"\n",
    "    \n",
    "    print(\"üî§ Tokenization Method Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Input text: '{text.strip()}'\")\n",
    "    print()\n",
    "    \n",
    "    results = tokenize_text_multiple_methods(text)\n",
    "    \n",
    "    for method_name, data in results.items():\n",
    "        print(f\"üìù {method_name.upper()}:\")\n",
    "        \n",
    "        if 'error' in data:\n",
    "            print(f\"   ‚ùå {data['error']}\")\n",
    "        else:\n",
    "            print(f\"   Method: {data['method']}\")\n",
    "            print(f\"   Token count: {data['count']}\")\n",
    "            print(f\"   First 10 tokens: {data['tokens'][:10]}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def german_sentence_tokenization(text):\n",
    "    \"\"\"\n",
    "    Tokenize German text into sentences.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        list: List of sentences\n",
    "    \"\"\"\n",
    "    # TODO: Implement sentence tokenization:\n",
    "    # 1. Use simple period splitting as fallback\n",
    "    # 2. Use NLTK sentence tokenizer if available\n",
    "    # 3. Handle German-specific sentence patterns\n",
    "    \n",
    "    print(\"üìë Sentence Tokenization\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Method 1: Simple approach\n",
    "    simple_sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    print(f\"Simple method: {len(simple_sentences)} sentences\")\n",
    "    \n",
    "    # Method 2: NLTK (if available)\n",
    "    if NLTK_AVAILABLE:\n",
    "        try:\n",
    "            nltk_sentences = sent_tokenize(text, language='german')\n",
    "            print(f\"NLTK method: {len(nltk_sentences)} sentences\")\n",
    "            \n",
    "            print(\"\\nNLTK sentences:\")\n",
    "            for i, sentence in enumerate(nltk_sentences, 1):\n",
    "                print(f\"  {i}. {sentence.strip()}\")\n",
    "            \n",
    "            return nltk_sentences\n",
    "        except Exception as e:\n",
    "            print(f\"NLTK sentence tokenization failed: {e}\")\n",
    "    \n",
    "    print(\"\\nSimple sentences:\")\n",
    "    for i, sentence in enumerate(simple_sentences, 1):\n",
    "        print(f\"  {i}. {sentence}\")\n",
    "    \n",
    "    return simple_sentences\n",
    "\n",
    "# Test tokenization methods\n",
    "print(\"Testing different tokenization approaches:\")\n",
    "tokenization_results = compare_tokenization_methods(german_text)\n",
    "\n",
    "# Test sentence tokenization\n",
    "sentences = german_sentence_tokenization(german_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08477504",
   "metadata": {},
   "source": [
    "### Step 2: Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a23b182c",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/german/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Felix Neub√ºrger/nltk_data'\n    - 'c:\\\\Users\\\\Felix Neub√ºrger\\\\Documents\\\\Lehre\\\\NLP_BA2526\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\Felix Neub√ºrger\\\\Documents\\\\Lehre\\\\NLP_BA2526\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Felix Neub√ºrger\\\\Documents\\\\Lehre\\\\NLP_BA2526\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Felix Neub√ºrger\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Apply NLTK preprocessing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m nltk_result = \u001b[43mnltk_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgerman_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNLTK Preprocessing Results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of sentences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(nltk_result[\u001b[33m'\u001b[39m\u001b[33msentences\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mnltk_preprocessing\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m     22\u001b[39m result = {}\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Sentence tokenization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m result[\u001b[33m'\u001b[39m\u001b[33msentences\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgerman\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Word tokenization\u001b[39;00m\n\u001b[32m     28\u001b[39m tokens = word_tokenize(text, language=\u001b[33m'\u001b[39m\u001b[33mgerman\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Felix Neub√ºrger\\Documents\\Lehre\\NLP_BA2526\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Felix Neub√ºrger\\Documents\\Lehre\\NLP_BA2526\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Felix Neub√ºrger\\Documents\\Lehre\\NLP_BA2526\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Felix Neub√ºrger\\Documents\\Lehre\\NLP_BA2526\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Felix Neub√ºrger\\Documents\\Lehre\\NLP_BA2526\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/german/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Felix Neub√ºrger/nltk_data'\n    - 'c:\\\\Users\\\\Felix Neub√ºrger\\\\Documents\\\\Lehre\\\\NLP_BA2526\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\Felix Neub√ºrger\\\\Documents\\\\Lehre\\\\NLP_BA2526\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Felix Neub√ºrger\\\\Documents\\\\Lehre\\\\NLP_BA2526\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Felix Neub√ºrger\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def nltk_preprocessing(text, language='german'):\n",
    "    \"\"\"\n",
    "    Preprocess text using NLTK tools.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        language (str): Language for stopwords\n",
    "    \n",
    "    Returns:\n",
    "        dict: Preprocessed text components\n",
    "    \"\"\"\n",
    "    # TODO: Implement the following preprocessing steps:\n",
    "    # 1. Sentence tokenization\n",
    "    # 2. Word tokenization\n",
    "    # 3. Convert to lowercase\n",
    "    # 4. Remove punctuation and numbers\n",
    "    # 5. Remove stopwords\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    # Sentence tokenization\n",
    "    result['sentences'] = sent_tokenize(text, language='german')\n",
    "    \n",
    "    # Word tokenization\n",
    "    tokens = word_tokenize(text, language='german')\n",
    "    result['tokens'] = tokens\n",
    "    \n",
    "    # Lowercase conversion\n",
    "    tokens_lower = [token.lower() for token in tokens]\n",
    "    result['tokens_lower'] = tokens_lower\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    tokens_clean = [token for token in tokens_lower if token.isalpha()]\n",
    "    result['tokens_clean'] = tokens_clean\n",
    "    \n",
    "    # Remove stopwords\n",
    "    german_stopwords = set(stopwords.words('german'))\n",
    "    tokens_no_stop = [token for token in tokens_clean if token not in german_stopwords]\n",
    "    result['tokens_no_stopwords'] = tokens_no_stop\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply NLTK preprocessing\n",
    "nltk_result = nltk_preprocessing(german_text)\n",
    "\n",
    "print(\"NLTK Preprocessing Results:\")\n",
    "print(f\"Number of sentences: {len(nltk_result['sentences'])}\")\n",
    "print(f\"Number of tokens: {len(nltk_result['tokens'])}\")\n",
    "print(f\"Number of clean tokens: {len(nltk_result['tokens_clean'])}\")\n",
    "print(f\"Number of tokens without stopwords: {len(nltk_result['tokens_no_stopwords'])}\")\n",
    "print(\"\\nTokens without stopwords:\")\n",
    "print(nltk_result['tokens_no_stopwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847b267",
   "metadata": {},
   "source": [
    "### Step 3: Advanced Processing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_processing(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Process text using spaCy for advanced NLP features.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        nlp_model: Loaded spaCy model\n",
    "    \n",
    "    Returns:\n",
    "        dict: Advanced NLP analysis results\n",
    "    \"\"\"\n",
    "    # Process text with spaCy\n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    result = {\n",
    "        'tokens': [],\n",
    "        'lemmas': [],\n",
    "        'pos_tags': [],\n",
    "        'entities': [],\n",
    "        'noun_phrases': []\n",
    "    }\n",
    "    \n",
    "    # TODO: Extract the following information:\n",
    "    # 1. Tokens and their lemmas\n",
    "    # 2. Part-of-speech tags\n",
    "    # 3. Named entities\n",
    "    # 4. Noun phrases\n",
    "    \n",
    "    # Extract token information\n",
    "    for token in doc:\n",
    "        if not token.is_space and not token.is_punct:\n",
    "            result['tokens'].append(token.text)\n",
    "            result['lemmas'].append(token.lemma_)\n",
    "            result['pos_tags'].append((token.text, token.pos_, token.tag_))\n",
    "    \n",
    "    # Extract named entities\n",
    "    for ent in doc.ents:\n",
    "        result['entities'].append((ent.text, ent.label_, ent.start_char, ent.end_char))\n",
    "    \n",
    "    # Extract noun phrases\n",
    "    for chunk in doc.noun_chunks:\n",
    "        result['noun_phrases'].append(chunk.text)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply spaCy processing (if model is available)\n",
    "if 'nlp' in locals():\n",
    "    spacy_result = spacy_processing(german_text, nlp)\n",
    "    \n",
    "    print(\"spaCy Processing Results:\")\n",
    "    print(f\"\\nLemmas: {spacy_result['lemmas'][:10]}...\")  # Show first 10\n",
    "    print(f\"\\nPOS Tags (first 10):\")\n",
    "    for token, pos, tag in spacy_result['pos_tags'][:10]:\n",
    "        print(f\"  {token}: {pos} ({tag})\")\n",
    "    \n",
    "    print(f\"\\nNamed Entities:\")\n",
    "    for ent_text, ent_label, start, end in spacy_result['entities']:\n",
    "        print(f\"  {ent_text}: {ent_label}\")\n",
    "    \n",
    "    print(f\"\\nNoun Phrases: {spacy_result['noun_phrases']}\")\n",
    "else:\n",
    "    print(\"spaCy model not available. Please install: python -m spacy download de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e6636",
   "metadata": {},
   "source": [
    "### Step 4: Frequency Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b41e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_word_frequency(tokens, top_n=10):\n",
    "    \"\"\"\n",
    "    Analyze word frequency and create visualizations.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): List of tokens to analyze\n",
    "        top_n (int): Number of top words to display\n",
    "    \n",
    "    Returns:\n",
    "        Counter: Word frequency counter\n",
    "    \"\"\"\n",
    "    # TODO: Implement frequency analysis:\n",
    "    # 1. Count word frequencies\n",
    "    # 2. Create bar plot of most frequent words\n",
    "    # 3. Generate word cloud\n",
    "    \n",
    "    # Count frequencies\n",
    "    word_freq = Counter(tokens)\n",
    "    most_common = word_freq.most_common(top_n)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Bar plot\n",
    "    words, counts = zip(*most_common) if most_common else ([], [])\n",
    "    ax1.bar(words, counts)\n",
    "    ax1.set_title(f'Top {top_n} Most Frequent Words')\n",
    "    ax1.set_xlabel('Words')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Word cloud\n",
    "    if tokens:\n",
    "        text_for_cloud = ' '.join(tokens)\n",
    "        wordcloud = WordCloud(width=400, height=300, background_color='white').generate(text_for_cloud)\n",
    "        ax2.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax2.set_title('Word Cloud')\n",
    "        ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "# Analyze frequency of clean tokens\n",
    "if 'nltk_result' in locals():\n",
    "    word_frequencies = analyze_word_frequency(nltk_result['tokens_no_stopwords'])\n",
    "    print(\"\\nWord Frequency Analysis:\")\n",
    "    print(f\"Total unique words: {len(word_frequencies)}\")\n",
    "    print(f\"Most common words: {word_frequencies.most_common(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc857063",
   "metadata": {},
   "source": [
    "### Step 5: Text Comparison Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_preprocessing_methods(text):\n",
    "    \"\"\"\n",
    "    Compare different preprocessing approaches.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to compare\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Comparison results\n",
    "    \"\"\"\n",
    "    # TODO: Create a comparison between different preprocessing methods:\n",
    "    # 1. Raw text split\n",
    "    # 2. NLTK tokenization\n",
    "    # 3. spaCy tokenization (if available)\n",
    "    \n",
    "    methods = {}\n",
    "    \n",
    "    # Simple split\n",
    "    simple_tokens = text.split()\n",
    "    methods['Simple Split'] = {\n",
    "        'token_count': len(simple_tokens),\n",
    "        'unique_tokens': len(set(simple_tokens)),\n",
    "        'sample_tokens': simple_tokens[:5]\n",
    "    }\n",
    "    \n",
    "    # NLTK tokenization\n",
    "    nltk_tokens = word_tokenize(text, language='german')\n",
    "    methods['NLTK'] = {\n",
    "        'token_count': len(nltk_tokens),\n",
    "        'unique_tokens': len(set(nltk_tokens)),\n",
    "        'sample_tokens': nltk_tokens[:5]\n",
    "    }\n",
    "    \n",
    "    # spaCy tokenization (if available)\n",
    "    if 'nlp' in locals():\n",
    "        spacy_doc = nlp(text)\n",
    "        spacy_tokens = [token.text for token in spacy_doc if not token.is_space]\n",
    "        methods['spaCy'] = {\n",
    "            'token_count': len(spacy_tokens),\n",
    "            'unique_tokens': len(set(spacy_tokens)),\n",
    "            'sample_tokens': spacy_tokens[:5]\n",
    "        }\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame(methods).T\n",
    "    return comparison_df\n",
    "\n",
    "# Compare preprocessing methods\n",
    "comparison = compare_preprocessing_methods(german_text)\n",
    "print(\"Preprocessing Method Comparison:\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f9d57",
   "metadata": {},
   "source": [
    "## Exercise Tasks\n",
    "\n",
    "Complete the following tasks to practice your understanding:\n",
    "\n",
    "1. **Extend Text Statistics**: Add more sophisticated statistics like:\n",
    "   - Lexical diversity (unique words / total words)\n",
    "   - Average characters per word\n",
    "   - Reading difficulty scores\n",
    "\n",
    "2. **Custom Stopword List**: Create a custom stopword list for your domain and compare results.\n",
    "\n",
    "3. **Language Detection**: Use a language detection library to automatically identify text language.\n",
    "\n",
    "4. **Multi-text Analysis**: Process multiple texts and compare their characteristics.\n",
    "\n",
    "5. **Interactive Preprocessing**: Create a function that allows users to choose different preprocessing options.\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. What are the main differences between NLTK and spaCy tokenization?\n",
    "2. When would you choose lemmatization over stemming?\n",
    "3. How does German text processing differ from English?\n",
    "4. What preprocessing steps are most important for your specific use case?\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore more advanced spaCy features (dependency parsing, similarity)\n",
    "- Learn about regular expressions for custom text cleaning\n",
    "- Practice with different types of texts (social media, formal documents, etc.)\n",
    "- Study language-specific challenges in German NLP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
